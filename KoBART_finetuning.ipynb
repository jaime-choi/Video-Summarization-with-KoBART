{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **클론, 필요한 패키지 설치**"],"metadata":{"id":"iNOIUQigHuAw"}},{"cell_type":"code","source":["from google.colab import drive # 구글 드라이브 마운트 해오기\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rx-pUf5Fac6M","executionInfo":{"status":"ok","timestamp":1673154307094,"user_tz":-540,"elapsed":17563,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"outputId":"57ad383a-4b56-473c-a369-d4ea56c56a28"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/My Drive/kobart_summarization/KoBART-summarization\n","# 클론할 드라이브 경로 설정"],"metadata":{"id":"9LGo924Vbdb6","executionInfo":{"status":"ok","timestamp":1673154312672,"user_tz":-540,"elapsed":332,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1dfe2884-7064-47b6-8e18-b47693d69ec4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/kobart_summarization/KoBART-summarization\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/seujung/KoBART-summarization.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gSc1cEpCbinF","executionInfo":{"status":"ok","timestamp":1672144520227,"user_tz":-540,"elapsed":3945,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"outputId":"f088762b-60a7-4a2a-fd53-0a297c0b1e94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'KoBART-summarization'...\n","remote: Enumerating objects: 142, done.\u001b[K\n","remote: Counting objects: 100% (59/59), done.\u001b[K\n","remote: Compressing objects: 100% (22/22), done.\u001b[K\n","remote: Total 142 (delta 46), reused 37 (delta 37), pack-reused 83\u001b[K\n","Receiving objects: 100% (142/142), 37.23 MiB | 22.83 MiB/s, done.\n","Resolving deltas: 100% (76/76), done.\n","^C\n"]}]},{"cell_type":"code","source":["# 필요한 패키지 설치\n","! pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"21Ytk9SQG-x3","executionInfo":{"status":"ok","timestamp":1672732802514,"user_tz":-540,"elapsed":128767,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"outputId":"5b2af586-36b4-491c-9df8-f5e0ce1776d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.3.5)\n","Collecting torch==1.10.0\n","  Downloading torch-1.10.0-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:40tcmalloc: large alloc 1147494400 bytes == 0x39e4e000 @  0x7f6ff97a4615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n","\u001b[K     |████████████████████████████████| 881.9 MB 3.9 kB/s \n","\u001b[?25hCollecting transformers==4.8.2\n","  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 63.4 MB/s \n","\u001b[?25hCollecting pytorch-lightning==1.3.8\n","  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\n","\u001b[K     |████████████████████████████████| 813 kB 56.9 MB/s \n","\u001b[?25hCollecting streamlit==1.1.0\n","  Downloading streamlit-1.1.0-py2.py3-none-any.whl (8.3 MB)\n","\u001b[K     |████████████████████████████████| 8.3 MB 59.1 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.0->-r requirements.txt (line 2)) (4.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (3.8.2)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 53.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (4.64.1)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 61.4 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (2022.6.2)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (2022.11.0)\n","Collecting future>=0.17.1\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 55.5 MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (2.9.1)\n","Requirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (7.1.2)\n","Collecting pyDeprecate==0.3.0\n","  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n","Collecting torchmetrics>=0.2.0\n","  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n","\u001b[K     |████████████████████████████████| 512 kB 62.4 MB/s \n","\u001b[?25hCollecting pyyaml\n","  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n","\u001b[K     |████████████████████████████████| 662 kB 66.5 MB/s \n","\u001b[?25hCollecting base58\n","  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n","Collecting gitpython!=3.1.19\n","  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n","\u001b[K     |████████████████████████████████| 184 kB 73.8 MB/s \n","\u001b[?25hRequirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (7.1.2)\n","Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (4.2.0)\n","Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (3.19.6)\n","Requirement already satisfied: astor in /usr/local/lib/python3.8/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (0.8.1)\n","Collecting validators\n","  Downloading validators-0.20.0.tar.gz (30 kB)\n","Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.8/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (6.0.4)\n","Collecting blinker\n","  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: tzlocal in /usr/local/lib/python3.8/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (1.5.1)\n","Collecting pydeck>=0.1.dev5\n","  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 56.8 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (2.8.2)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (22.1.0)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.8/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (9.0.0)\n","Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.8/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (5.2.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (0.10.2)\n","Collecting watchdog\n","  Downloading watchdog-2.2.1-py3-none-manylinux2014_x86_64.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 9.6 MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 1)) (2022.6)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (4.3.3)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (0.12.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (2.11.3)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (0.4)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (3.8.3)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (6.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.8.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (2.1.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.3.3)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (4.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.3.1)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.8 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (5.10.1)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (0.19.2)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (3.11.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==4.8.2->-r requirements.txt (line 3)) (3.0.9)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (2.0.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil->streamlit==1.1.0->-r requirements.txt (line 5)) (1.15.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.0.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.3.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.6.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.51.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.38.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (57.4.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (2.15.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (3.4.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.4.6)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (5.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 3)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 3)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 3)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 3)) (2022.12.7)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (3.2.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.8.2->-r requirements.txt (line 3)) (1.2.0)\n","Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from validators->streamlit==1.1.0->-r requirements.txt (line 5)) (4.4.2)\n","Building wheels for collected packages: future, sacremoses, validators\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491071 sha256=f7706034c9ab7fcd1434eed489cfd86da11cae059b4e746fda9e4868dc9f2a66\n","  Stored in directory: /root/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=35af2570513797c084e7687452d752a9ffb6e6634eaf7eb2f7578bd7855befd1\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19581 sha256=d6fa688a68c1fe3eb11130363526aa5c0a1faabde57300049320466e1e855d07\n","  Stored in directory: /root/.cache/pip/wheels/19/09/72/3eb74d236bb48bd0f3c6c3c83e4e0c5bbfcbcad7c6c3539db8\n","Successfully built future sacremoses validators\n","Installing collected packages: smmap, torch, gitdb, watchdog, validators, torchmetrics, tokenizers, sacremoses, pyyaml, pyDeprecate, pydeck, huggingface-hub, gitpython, future, blinker, base58, transformers, streamlit, pytorch-lightning\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.0+cu116\n","    Uninstalling torch-1.13.0+cu116:\n","      Successfully uninstalled torch-1.13.0+cu116\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 6.0\n","    Uninstalling PyYAML-6.0:\n","      Successfully uninstalled PyYAML-6.0\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 1.10.0 which is incompatible.\n","torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.10.0 which is incompatible.\n","torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.10.0 which is incompatible.\u001b[0m\n","Successfully installed base58-2.1.1 blinker-1.5 future-0.18.2 gitdb-4.0.10 gitpython-3.1.30 huggingface-hub-0.0.12 pyDeprecate-0.3.0 pydeck-0.8.0 pytorch-lightning-1.3.8 pyyaml-5.4.1 sacremoses-0.0.53 smmap-5.0.0 streamlit-1.1.0 tokenizers-0.10.3 torch-1.10.0 torchmetrics-0.11.0 transformers-4.8.2 validators-0.20.0 watchdog-2.2.1\n"]}]},{"cell_type":"code","source":["! pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n","# 토치, 토치비전 버전 세팅"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yv4YM2NDHJLL","outputId":"603f666e-0b8c-4030-f5b0-54eca80e6c0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.7.1+cu101\n","  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp38-cp38-linux_x86_64.whl (735.4 MB)\n","\u001b[K     |████████████████████████████████| 735.4 MB 6.6 kB/s \n","\u001b[?25hCollecting torchvision==0.8.2+cu101\n","  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp38-cp38-linux_x86_64.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 55.1 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.7.1+cu101) (4.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.7.1+cu101) (1.21.6)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.8.2+cu101) (7.1.2)\n"]}]},{"cell_type":"code","source":["# 토치텍스트 버전 오류가 나면 다시 설치\n","! pip install torchtext==0.8.0\n","\n","# 토치매트릭스 버전 오류 나서 설치\n","! pip install torchmetrics==0.6.0\n","\n","# 파이토치 라이트닝 버전 오류 나서 수정\n","! pip install pytorch_lightning==1.5.2"],"metadata":{"id":"oR7MskHmsZpm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672670746150,"user_tz":-540,"elapsed":19954,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"outputId":"2325372d-798a-451f-a815-89c091928b53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchtext==0.8.0\n","  Downloading torchtext-0.8.0-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n","\u001b[K     |████████████████████████████████| 7.0 MB 6.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext==0.8.0) (1.21.6)\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from torchtext==0.8.0) (1.7.1+cu101)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.8.0) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.8.0) (4.64.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.8.0) (2022.12.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.8.0) (2.10)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->torchtext==0.8.0) (4.4.0)\n","Installing collected packages: torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.14.0\n","    Uninstalling torchtext-0.14.0:\n","      Successfully uninstalled torchtext-0.14.0\n","Successfully installed torchtext-0.8.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics==0.6.0\n","  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n","\u001b[K     |████████████████████████████████| 329 kB 7.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics==0.6.0) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics==0.6.0) (21.3)\n","Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics==0.6.0) (1.7.1+cu101)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.1->torchmetrics==0.6.0) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics==0.6.0) (3.0.9)\n","Installing collected packages: torchmetrics\n","  Attempting uninstall: torchmetrics\n","    Found existing installation: torchmetrics 0.11.0\n","    Uninstalling torchmetrics-0.11.0:\n","      Successfully uninstalled torchmetrics-0.11.0\n","Successfully installed torchmetrics-0.6.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_lightning==1.5.2\n","  Downloading pytorch_lightning-1.5.2-py3-none-any.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 6.8 MB/s \n","\u001b[?25hRequirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.5.2) (0.18.2)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.5.2) (1.21.6)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.5.2) (1.7.1+cu101)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.5.2) (21.3)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.5.2) (4.64.1)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.5.2) (2.9.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.5.2) (4.4.0)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.5.2) (2022.11.0)\n","Collecting pyDeprecate==0.3.1\n","  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n","Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.5.2) (0.6.0)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.5.2) (5.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.2) (2.23.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.2) (3.8.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.2) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.2) (4.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.2) (2.1.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.2) (1.8.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.2) (22.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.2) (6.0.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.2) (1.3.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch_lightning==1.5.2) (3.0.9)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.2) (3.19.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.2) (2.15.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.2) (3.4.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.2) (0.38.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.2) (1.3.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.2) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.2) (0.4.6)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.2) (1.0.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.2) (1.51.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.2) (1.8.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.2) (57.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.5.2) (4.9)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.5.2) (1.15.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.5.2) (0.2.8)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.5.2) (5.2.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.5.2) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==1.5.2) (5.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==1.5.2) (3.11.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.5.2) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.2) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.2) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.2) (2022.12.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.2) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.5.2) (3.2.2)\n","Installing collected packages: pyDeprecate, pytorch-lightning\n","  Attempting uninstall: pyDeprecate\n","    Found existing installation: pyDeprecate 0.3.0\n","    Uninstalling pyDeprecate-0.3.0:\n","      Successfully uninstalled pyDeprecate-0.3.0\n","  Attempting uninstall: pytorch-lightning\n","    Found existing installation: pytorch-lightning 1.3.8\n","    Uninstalling pytorch-lightning-1.3.8:\n","      Successfully uninstalled pytorch-lightning-1.3.8\n","Successfully installed pyDeprecate-0.3.1 pytorch-lightning-1.5.2\n"]}]},{"cell_type":"code","source":["# 클론한 파일 제공 데이터 train, test 압축 해제\n","%cd data\n","!tar -zxvf train.tar.gz\n","!tar -zxvf test.tar.gz\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3OwfeU1JHsSE","executionInfo":{"status":"ok","timestamp":1669354340280,"user_tz":-540,"elapsed":2394,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"outputId":"c37ed3f1-6299-452e-ad44-f0275a93e278"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/kobart_summarization/KoBART-summarization/data\n","train.tsv\n","test.tsv\n","add_test.tsv  add_train.tsv  test.tar.gz  test.tsv  train.tar.gz  train.tsv\n"]}]},{"cell_type":"code","source":["%cd data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7tw9D9Oy-P7a","executionInfo":{"status":"ok","timestamp":1673154319178,"user_tz":-540,"elapsed":783,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"outputId":"0c10785f-6c16-4383-d884-b6c053ce9747"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/kobart_summarization/KoBART-summarization/data\n"]}]},{"cell_type":"code","source":["# 압축 해제된 파일 확인\n","!cat train.tsv\n","!cat test.tsv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1S_q5ikZGu4xDKea5QgXnqz364d6YLwnP"},"id":"boDK39GaH-aA","executionInfo":{"status":"ok","timestamp":1669354093696,"user_tz":-540,"elapsed":45043,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"outputId":"d0872c10-0910-4935-ac6a-1fef450e06e9"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# **데이터 처리**\n"],"metadata":{"id":"KPpJ7xjXHiBk"}},{"cell_type":"code","source":["# 학습할 데이터 가져오기\n","import shutil\n","# train\n","shutil.copy(\"/content/drive/MyDrive/kobart_summarization/data/add_train.tsv\", \"/content/drive/My Drive/kobart_summarization/KoBART-summarization/data/add_train.tsv\")\n","# test\n","shutil.copy(\"/content/drive/MyDrive/kobart_summarization/data/add_test.tsv\", \"/content/drive/My Drive/kobart_summarization/KoBART-summarization/data/add_test.tsv\")"],"metadata":{"id":"HE1Z72qMJ-q4","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1669354377447,"user_tz":-540,"elapsed":4455,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"outputId":"43098697-bc20-451e-e49f-dc51611f28ca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/kobart_summarization/KoBART-summarization/data/add_test.tsv'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# 학습할 데이터 확인\n","import pandas as pd\n","df_train = pd.read_csv('add_train.tsv', sep='\\t')\n","df_test = pd.read_csv('add_test.tsv', sep='\\t')\n","\n","df_train.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"JdUKMKTpP3iD","executionInfo":{"status":"ok","timestamp":1672369231335,"user_tz":-540,"elapsed":8945,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"outputId":"b913b54d-17d9-4dbb-f819-5ec544637a3f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                news  \\\n","0   입지요건\\n 입지 특성에 따라 ‘주거상업고밀지구(역세권)’, ‘주거산업융합지구(준...   \n","1   해당 차량은 7월 23일부터 비엠더블유코리아 공식 서비스센터에서 무상으로 수리(점...   \n","2   정부는 2021 P4G 서울 녹색미래 정상회의(5.30.(일)~5.31(월))에 ...   \n","3                 \\n 건축물대장상 실제 소유자와 같은데도 잘못 작성됐다면...   \n","4                 \\n 앞으로 예술인 복지서비스를 받기 위한 예술활동증명 ...   \n","\n","                                             summary  \n","0  사업 추진 과정에서 지정권자는 사전검토기구를 구성하여 용적률 등을 사전에 검토하고 ...  \n","1  포르쉐코리아에서 수입, 판매된 타이칸이 주행 중 시동이 꺼질 가능성이 확인되어 리콜...  \n","2  정부가 개최한 녹색미래주간 개막식은 DDP에서 20분간 개막 영상, 국회의장 축사,...  \n","3  국민권익위는 건축물대장상 소유자가 개인 명의이나 사업자로 잘못 기재됐으니 정정해달라...  \n","4  국민권익위원회는 코로나19로 어려움을 겪고 있는 문화예술인을 지원하기 위해 문화체육...  "],"text/html":["\n","  <div id=\"df-34a814d8-72c8-452a-bf1e-18dd9a89d4ca\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>news</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>입지요건\\n 입지 특성에 따라 ‘주거상업고밀지구(역세권)’, ‘주거산업융합지구(준...</td>\n","      <td>사업 추진 과정에서 지정권자는 사전검토기구를 구성하여 용적률 등을 사전에 검토하고 ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>해당 차량은 7월 23일부터 비엠더블유코리아 공식 서비스센터에서 무상으로 수리(점...</td>\n","      <td>포르쉐코리아에서 수입, 판매된 타이칸이 주행 중 시동이 꺼질 가능성이 확인되어 리콜...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>정부는 2021 P4G 서울 녹색미래 정상회의(5.30.(일)~5.31(월))에 ...</td>\n","      <td>정부가 개최한 녹색미래주간 개막식은 DDP에서 20분간 개막 영상, 국회의장 축사,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\\n 건축물대장상 실제 소유자와 같은데도 잘못 작성됐다면...</td>\n","      <td>국민권익위는 건축물대장상 소유자가 개인 명의이나 사업자로 잘못 기재됐으니 정정해달라...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\\n 앞으로 예술인 복지서비스를 받기 위한 예술활동증명 ...</td>\n","      <td>국민권익위원회는 코로나19로 어려움을 겪고 있는 문화예술인을 지원하기 위해 문화체육...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34a814d8-72c8-452a-bf1e-18dd9a89d4ca')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-34a814d8-72c8-452a-bf1e-18dd9a89d4ca button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-34a814d8-72c8-452a-bf1e-18dd9a89d4ca');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# null 값 확인\n","df_train.isnull().sum(axis = 0)\n","df_test.isnull().sum(axis = 0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eXCR1s5OP3p1","executionInfo":{"status":"ok","timestamp":1669354430584,"user_tz":-540,"elapsed":922,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"outputId":"b2d8efcb-dcec-4cfe-b078-3bd543389607"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["news       0\n","summary    0\n","dtype: int64"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# 학습 데이터 대체하기\n","df_train.to_csv(\"/content/drive/My Drive/kobart_summarization/KoBART-summarization/data/train.tsv\", mode='w',\n","                header=['news','summary'], sep='\\t', index=False)\n","df_test.to_csv(\"/content/drive/My Drive/kobart_summarization/KoBART-summarization/data/test.tsv\", mode='w',\n","               header=['news','summary'], sep='\\t', index=False)\n","\n","# mode=write, tsv라서 seperator='\\t' 사용"],"metadata":{"id":"asb5kH8bP3wX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 데이터 대체가 잘 되었는지 확인\n","import pandas as pd\n","df_train1 = pd.read_csv('train.tsv', sep='\\t')\n","df_test1 = pd.read_csv('test.tsv', sep='\\t')\n","\n","df_train1.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"UUwVYiNtRzbL","executionInfo":{"status":"ok","timestamp":1673154365692,"user_tz":-540,"elapsed":5109,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"outputId":"6e8d2b87-cf21-410a-892d-b5de381f10a3"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                news  \\\n","0   입지요건\\n 입지 특성에 따라 ‘주거상업고밀지구(역세권)’, ‘주거산업융합지구(준...   \n","1   해당 차량은 7월 23일부터 비엠더블유코리아 공식 서비스센터에서 무상으로 수리(점...   \n","2   정부는 2021 P4G 서울 녹색미래 정상회의(5.30.(일)~5.31(월))에 ...   \n","3                 \\n 건축물대장상 실제 소유자와 같은데도 잘못 작성됐다면...   \n","4                 \\n 앞으로 예술인 복지서비스를 받기 위한 예술활동증명 ...   \n","\n","                                             summary  \n","0  사업 추진 과정에서 지정권자는 사전검토기구를 구성하여 용적률 등을 사전에 검토하고 ...  \n","1  포르쉐코리아에서 수입, 판매된 타이칸이 주행 중 시동이 꺼질 가능성이 확인되어 리콜...  \n","2  정부가 개최한 녹색미래주간 개막식은 DDP에서 20분간 개막 영상, 국회의장 축사,...  \n","3  국민권익위는 건축물대장상 소유자가 개인 명의이나 사업자로 잘못 기재됐으니 정정해달라...  \n","4  국민권익위원회는 코로나19로 어려움을 겪고 있는 문화예술인을 지원하기 위해 문화체육...  "],"text/html":["\n","  <div id=\"df-554d4e97-2775-4b34-af6a-d98618a23775\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>news</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>입지요건\\n 입지 특성에 따라 ‘주거상업고밀지구(역세권)’, ‘주거산업융합지구(준...</td>\n","      <td>사업 추진 과정에서 지정권자는 사전검토기구를 구성하여 용적률 등을 사전에 검토하고 ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>해당 차량은 7월 23일부터 비엠더블유코리아 공식 서비스센터에서 무상으로 수리(점...</td>\n","      <td>포르쉐코리아에서 수입, 판매된 타이칸이 주행 중 시동이 꺼질 가능성이 확인되어 리콜...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>정부는 2021 P4G 서울 녹색미래 정상회의(5.30.(일)~5.31(월))에 ...</td>\n","      <td>정부가 개최한 녹색미래주간 개막식은 DDP에서 20분간 개막 영상, 국회의장 축사,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\\n 건축물대장상 실제 소유자와 같은데도 잘못 작성됐다면...</td>\n","      <td>국민권익위는 건축물대장상 소유자가 개인 명의이나 사업자로 잘못 기재됐으니 정정해달라...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\\n 앞으로 예술인 복지서비스를 받기 위한 예술활동증명 ...</td>\n","      <td>국민권익위원회는 코로나19로 어려움을 겪고 있는 문화예술인을 지원하기 위해 문화체육...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-554d4e97-2775-4b34-af6a-d98618a23775')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-554d4e97-2775-4b34-af6a-d98618a23775 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-554d4e97-2775-4b34-af6a-d98618a23775');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["# **테스트셋, 검증셋 나누기**"],"metadata":{"id":"MjXYYWvSy08d"}},{"cell_type":"code","source":["# 기존 테스트셋에서 10%가 테스트용, 90%가 검증용\n","import numpy as np\n","np.random.seed(42) # 실행할 때마다 다른 결과를 피하고 일정한 결과를 얻기 위해 초기에 random seed 지정\n","index = np.arange(10400)\n","np.random.shuffle(index) # 주어진 배열을 무작위로 섞음"],"metadata":{"id":"PDBEJF3MxOyo","executionInfo":{"status":"ok","timestamp":1673154508000,"user_tz":-540,"elapsed":281,"user":{"displayName":"최해민","userId":"04401546372408002204"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["test = df_test1.iloc[index[:1040]]\n","test_input = pd.DataFrame(test['news'])\n","test_target = pd.DataFrame(test['summary'])\n","\n","test_input.reset_index(drop=True, inplace=True)\n","test_target.reset_index(drop=True, inplace=True)\n","\n","test_input.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"gdJiCQuUx7nD","executionInfo":{"status":"ok","timestamp":1673155264562,"user_tz":-540,"elapsed":456,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"outputId":"8901479b-be0b-4c0a-d62a-cf9604f5f935"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                news\n","0  이 부분은 우리 도로국에서 금액을 좀 말씀해 주시죠.  <답변> (김재동 한국도로공...\n","1  첫 번째 질문은 코로나19의 감염자 중에 발열이 어느 정도 발생하는지에 대한 질문이...\n","2   농촌진흥청(청장 김경규)은 수박 유전자원의 기능성 성분 대량 평가를 통해 라이코펜...\n","3   \\n 산업통상자원부(장관 성윤모)는 지역 청년일자리 창출 확대를 통해 지역균형발전...\n","4  (박영준 방대본 역학조사팀장) 먼저 유가족분들에게 위로의 말씀을 전하겠습니다. 해당..."],"text/html":["\n","  <div id=\"df-21d3ef0f-6a5e-49f5-9526-4cb373160a83\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>news</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>이 부분은 우리 도로국에서 금액을 좀 말씀해 주시죠.  &lt;답변&gt; (김재동 한국도로공...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>첫 번째 질문은 코로나19의 감염자 중에 발열이 어느 정도 발생하는지에 대한 질문이...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>농촌진흥청(청장 김경규)은 수박 유전자원의 기능성 성분 대량 평가를 통해 라이코펜...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\\n 산업통상자원부(장관 성윤모)는 지역 청년일자리 창출 확대를 통해 지역균형발전...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(박영준 방대본 역학조사팀장) 먼저 유가족분들에게 위로의 말씀을 전하겠습니다. 해당...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21d3ef0f-6a5e-49f5-9526-4cb373160a83')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-21d3ef0f-6a5e-49f5-9526-4cb373160a83 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-21d3ef0f-6a5e-49f5-9526-4cb373160a83');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["val = df_test1.iloc[index[1040:]]\n","val_input = pd.DataFrame(val['news'])\n","val_target = pd.DataFrame(val['summary'])\n","\n","val_input.reset_index(drop=True, inplace=True)\n","val_target.reset_index(drop=True, inplace=True)\n","\n","val_input.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"CdhGeg79zGpA","executionInfo":{"status":"ok","timestamp":1673155295298,"user_tz":-540,"elapsed":309,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"outputId":"b61c43b9-3d83-446e-c4ca-675bbdd310a8"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                news\n","0   문체부는 이러한 환경 변화와 현장의 요구에 더욱 적극적으로 대응하기 위해 ‘4대 ...\n","1  조정식 위원] \"이게 일주일 됐는데 일주일밖에 안 되긴 했지만 수출입은행이 거기에 ...\n","2  강은희 위원] \"강은희 위원입니다.  교육부장관님 고교 정상화 기여 대학 지원 사업...\n","3  급수취약 지역은 제가 아까 말씀드린 대로 해수담수화 당연히 포함되고요. 그다음에 지...\n","4     배민의 OD모델도 `15년 서비스를 개시하였으나 아직 서울․광역시 중심으로 운..."],"text/html":["\n","  <div id=\"df-69182bda-695e-44b4-ad50-79597fa58a22\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>news</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>문체부는 이러한 환경 변화와 현장의 요구에 더욱 적극적으로 대응하기 위해 ‘4대 ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>조정식 위원] \"이게 일주일 됐는데 일주일밖에 안 되긴 했지만 수출입은행이 거기에 ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>강은희 위원] \"강은희 위원입니다.  교육부장관님 고교 정상화 기여 대학 지원 사업...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>급수취약 지역은 제가 아까 말씀드린 대로 해수담수화 당연히 포함되고요. 그다음에 지...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>배민의 OD모델도 `15년 서비스를 개시하였으나 아직 서울․광역시 중심으로 운...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69182bda-695e-44b4-ad50-79597fa58a22')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-69182bda-695e-44b4-ad50-79597fa58a22 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-69182bda-695e-44b4-ad50-79597fa58a22');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["test_input.to_csv(\"test_input.tsv\", sep='\\t', index=False)\n","test_target.to_csv(\"test_target.tsv\", sep='\\t', index=False)\n","\n","val_input.to_csv(\"val_input.tsv\", sep='\\t', index=False)\n","val_target.to_csv(\"val_target.tsv\", sep='\\t', index=False)"],"metadata":{"id":"TE19ipol1L19","executionInfo":{"status":"ok","timestamp":1673155482760,"user_tz":-540,"elapsed":729,"user":{"displayName":"최해민","userId":"04401546372408002204"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# **모델 학습**"],"metadata":{"id":"JA_4jvTHIZi-"}},{"cell_type":"code","source":["# gpu 사용 체크\n","! nvidia-smi"],"metadata":{"id":"8YzDuVNrIcH1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672732994179,"user_tz":-540,"elapsed":1309,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"outputId":"a76bc928-70c4-4493-a4c7-73c27f517746"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Jan  3 08:03:13 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0    26W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["# 학습 진행\n","! python train.py  --gradient_clip_val 1.0  \\\n","                 --max_epochs 1 \\\n","                 --default_root_dir logs \\\n","                 --gpus 1 \\\n","                 --batch_size 4 \\\n","                 --num_workers 4\n","\n","#!sh run_train.sh"],"metadata":{"id":"jT3mi2rNIhmE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"93a776d0-f580-49ae-a5a2-127a8450ecfb","executionInfo":{"status":"ok","timestamp":1672747127865,"user_tz":-540,"elapsed":13917530,"user":{"displayName":"최해민","userId":"04401546372408002204"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:root:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=4, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=None, checkpoint_path=None, default_root_dir='logs', detect_anomaly=False, deterministic=False, devices=None, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, fast_dev_run=False, flush_logs_every_n_steps=None, gpus=1, gradient_clip_algorithm=None, gradient_clip_val=1.0, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=1, max_len=512, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, model_path=None, move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=4, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=None, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, stochastic_weight_avg=False, strategy=None, sync_batchnorm=False, terminate_on_nan=None, test_file='data/test.tsv', tpu_cores=None, track_grad_norm=-1, train_file='data/train.tsv', val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:root:number of workers 4, data length 20893\n","INFO:root:num_train_steps : 1305\n","INFO:root:num_warmup_steps : 130\n","\n","  | Name  | Type                         | Params\n","-------------------------------------------------------\n","0 | model | BartForConditionalGeneration | 123 M \n","-------------------------------------------------------\n","123 M     Trainable params\n","0         Non-trainable params\n","123 M     Total params\n","495.440   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /content/drive/MyDrive/kobart_summarization/KoBART-summarization/logs exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n","Epoch 0:  89% 20893/23493 [3:40:36<27:27,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/2600 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  89% 20895/23493 [3:40:36<27:25,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   0% 2/2600 [00:00<11:47,  3.67it/s]\u001b[A\n","Epoch 0:  89% 20897/23493 [3:40:37<27:24,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   0% 4/2600 [00:01<10:13,  4.23it/s]\u001b[A\n","Epoch 0:  89% 20899/23493 [3:40:37<27:23,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   0% 6/2600 [00:01<10:32,  4.10it/s]\u001b[A\n","Epoch 0:  89% 20901/23493 [3:40:38<27:21,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   0% 8/2600 [00:01<09:59,  4.32it/s]\u001b[A\n","Epoch 0:  89% 20903/23493 [3:40:38<27:20,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   0% 10/2600 [00:02<10:06,  4.27it/s]\u001b[A\n","Epoch 0:  89% 20905/23493 [3:40:39<27:18,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   0% 12/2600 [00:02<09:55,  4.34it/s]\u001b[A\n","Epoch 0:  89% 20907/23493 [3:40:39<27:17,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   1% 14/2600 [00:03<09:59,  4.31it/s]\u001b[A\n","Epoch 0:  89% 20909/23493 [3:40:39<27:16,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   1% 16/2600 [00:03<09:50,  4.37it/s]\u001b[A\n","Epoch 0:  89% 20911/23493 [3:40:40<27:14,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   1% 18/2600 [00:04<09:52,  4.36it/s]\u001b[A\n","Epoch 0:  89% 20913/23493 [3:40:40<27:13,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   1% 20/2600 [00:04<09:48,  4.38it/s]\u001b[A\n","Epoch 0:  89% 20915/23493 [3:40:41<27:12,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   1% 22/2600 [00:05<09:45,  4.40it/s]\u001b[A\n","Epoch 0:  89% 20917/23493 [3:40:41<27:10,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   1% 24/2600 [00:05<09:53,  4.34it/s]\u001b[A\n","Epoch 0:  89% 20919/23493 [3:40:42<27:09,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   1% 26/2600 [00:06<09:45,  4.39it/s]\u001b[A\n","Epoch 0:  89% 20921/23493 [3:40:42<27:08,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   1% 28/2600 [00:06<09:51,  4.35it/s]\u001b[A\n","Epoch 0:  89% 20923/23493 [3:40:43<27:06,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   1% 30/2600 [00:06<09:43,  4.41it/s]\u001b[A\n","Epoch 0:  89% 20925/23493 [3:40:43<27:05,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   1% 32/2600 [00:07<09:52,  4.34it/s]\u001b[A\n","Epoch 0:  89% 20927/23493 [3:40:44<27:03,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   1% 34/2600 [00:07<09:55,  4.31it/s]\u001b[A\n","Epoch 0:  89% 20929/23493 [3:40:44<27:02,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   1% 36/2600 [00:08<09:47,  4.36it/s]\u001b[A\n","Epoch 0:  89% 20931/23493 [3:40:44<27:01,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   1% 38/2600 [00:08<09:48,  4.36it/s]\u001b[A\n","Epoch 0:  89% 20933/23493 [3:40:45<26:59,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   2% 40/2600 [00:09<09:50,  4.34it/s]\u001b[A\n","Epoch 0:  89% 20935/23493 [3:40:45<26:58,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   2% 42/2600 [00:09<09:52,  4.32it/s]\u001b[A\n","Epoch 0:  89% 20937/23493 [3:40:46<26:57,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   2% 44/2600 [00:10<09:50,  4.33it/s]\u001b[A\n","Epoch 0:  89% 20939/23493 [3:40:46<26:55,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   2% 46/2600 [00:10<09:47,  4.35it/s]\u001b[A\n","Epoch 0:  89% 20941/23493 [3:40:47<26:54,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   2% 48/2600 [00:11<09:43,  4.37it/s]\u001b[A\n","Epoch 0:  89% 20943/23493 [3:40:47<26:53,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   2% 50/2600 [00:11<09:46,  4.35it/s]\u001b[A\n","Epoch 0:  89% 20945/23493 [3:40:48<26:51,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   2% 52/2600 [00:12<09:46,  4.35it/s]\u001b[A\n","Epoch 0:  89% 20947/23493 [3:40:48<26:50,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   2% 54/2600 [00:12<09:49,  4.32it/s]\u001b[A\n","Epoch 0:  89% 20949/23493 [3:40:49<26:48,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   2% 56/2600 [00:12<09:44,  4.35it/s]\u001b[A\n","Epoch 0:  89% 20951/23493 [3:40:49<26:47,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   2% 58/2600 [00:13<09:44,  4.35it/s]\u001b[A\n","Epoch 0:  89% 20953/23493 [3:40:50<26:46,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   2% 60/2600 [00:13<09:45,  4.34it/s]\u001b[A\n","Epoch 0:  89% 20955/23493 [3:40:50<26:44,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   2% 62/2600 [00:14<09:42,  4.36it/s]\u001b[A\n","Epoch 0:  89% 20957/23493 [3:40:50<26:43,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   2% 64/2600 [00:14<09:45,  4.33it/s]\u001b[A\n","Epoch 0:  89% 20959/23493 [3:40:51<26:42,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   3% 66/2600 [00:15<09:39,  4.37it/s]\u001b[A\n","Epoch 0:  89% 20961/23493 [3:40:51<26:40,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   3% 68/2600 [00:15<09:44,  4.34it/s]\u001b[A\n","Epoch 0:  89% 20963/23493 [3:40:52<26:39,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   3% 70/2600 [00:16<09:37,  4.38it/s]\u001b[A\n","Epoch 0:  89% 20965/23493 [3:40:52<26:38,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   3% 72/2600 [00:16<09:38,  4.37it/s]\u001b[A\n","Epoch 0:  89% 20967/23493 [3:40:53<26:36,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   3% 74/2600 [00:17<09:42,  4.34it/s]\u001b[A\n","Epoch 0:  89% 20969/23493 [3:40:53<26:35,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   3% 76/2600 [00:17<09:44,  4.32it/s]\u001b[A\n","Epoch 0:  89% 20971/23493 [3:40:54<26:33,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   3% 78/2600 [00:18<09:41,  4.33it/s]\u001b[A\n","Epoch 0:  89% 20973/23493 [3:40:54<26:32,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   3% 80/2600 [00:18<09:39,  4.35it/s]\u001b[A\n","Epoch 0:  89% 20975/23493 [3:40:55<26:31,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   3% 82/2600 [00:18<09:38,  4.35it/s]\u001b[A\n","Epoch 0:  89% 20977/23493 [3:40:55<26:29,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   3% 84/2600 [00:19<09:38,  4.35it/s]\u001b[A\n","Epoch 0:  89% 20979/23493 [3:40:56<26:28,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   3% 86/2600 [00:19<09:38,  4.35it/s]\u001b[A\n","Epoch 0:  89% 20981/23493 [3:40:56<26:27,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   3% 88/2600 [00:20<09:44,  4.30it/s]\u001b[A\n","Epoch 0:  89% 20983/23493 [3:40:56<26:25,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   3% 90/2600 [00:20<09:35,  4.36it/s]\u001b[A\n","Epoch 0:  89% 20985/23493 [3:40:57<26:24,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   4% 92/2600 [00:21<09:36,  4.35it/s]\u001b[A\n","Epoch 0:  89% 20987/23493 [3:40:57<26:23,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   4% 94/2600 [00:21<09:35,  4.35it/s]\u001b[A\n","Epoch 0:  89% 20989/23493 [3:40:58<26:21,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   4% 96/2600 [00:22<09:38,  4.33it/s]\u001b[A\n","Epoch 0:  89% 20991/23493 [3:40:58<26:20,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   4% 98/2600 [00:22<09:34,  4.36it/s]\u001b[A\n","Epoch 0:  89% 20993/23493 [3:40:59<26:19,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   4% 100/2600 [00:23<09:30,  4.38it/s]\u001b[A\n","Epoch 0:  89% 20995/23493 [3:40:59<26:17,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   4% 102/2600 [00:23<09:34,  4.35it/s]\u001b[A\n","Epoch 0:  89% 20997/23493 [3:41:00<26:16,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   4% 104/2600 [00:24<09:35,  4.34it/s]\u001b[A\n","Epoch 0:  89% 20999/23493 [3:41:00<26:14,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   4% 106/2600 [00:24<09:33,  4.35it/s]\u001b[A\n","Epoch 0:  89% 21001/23493 [3:41:01<26:13,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   4% 108/2600 [00:24<09:34,  4.34it/s]\u001b[A\n","Epoch 0:  89% 21003/23493 [3:41:01<26:12,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   4% 110/2600 [00:25<09:35,  4.33it/s]\u001b[A\n","Epoch 0:  89% 21005/23493 [3:41:02<26:10,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   4% 112/2600 [00:25<09:30,  4.36it/s]\u001b[A\n","Epoch 0:  89% 21007/23493 [3:41:02<26:09,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   4% 114/2600 [00:26<09:34,  4.32it/s]\u001b[A\n","Epoch 0:  89% 21009/23493 [3:41:02<26:08,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   4% 116/2600 [00:26<09:32,  4.34it/s]\u001b[A\n","Epoch 0:  89% 21011/23493 [3:41:03<26:06,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   5% 118/2600 [00:27<09:25,  4.39it/s]\u001b[A\n","Epoch 0:  89% 21013/23493 [3:41:03<26:05,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   5% 120/2600 [00:27<09:28,  4.36it/s]\u001b[A\n","Epoch 0:  89% 21015/23493 [3:41:04<26:04,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   5% 122/2600 [00:28<09:35,  4.31it/s]\u001b[A\n","Epoch 0:  89% 21017/23493 [3:41:04<26:02,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   5% 124/2600 [00:28<09:22,  4.40it/s]\u001b[A\n","Epoch 0:  89% 21019/23493 [3:41:05<26:01,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   5% 126/2600 [00:29<09:26,  4.37it/s]\u001b[A\n","Epoch 0:  89% 21021/23493 [3:41:05<25:59,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   5% 128/2600 [00:29<09:24,  4.38it/s]\u001b[A\n","Epoch 0:  89% 21023/23493 [3:41:06<25:58,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   5% 130/2600 [00:29<09:28,  4.35it/s]\u001b[A\n","Epoch 0:  89% 21025/23493 [3:41:06<25:57,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   5% 132/2600 [00:30<09:30,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21027/23493 [3:41:07<25:55,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   5% 134/2600 [00:30<09:27,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21029/23493 [3:41:07<25:54,  1.58it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   5% 136/2600 [00:31<09:26,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21031/23493 [3:41:07<25:53,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   5% 138/2600 [00:31<09:26,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21033/23493 [3:41:08<25:51,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   5% 140/2600 [00:32<09:27,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21035/23493 [3:41:08<25:50,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   5% 142/2600 [00:32<09:22,  4.37it/s]\u001b[A\n","Epoch 0:  90% 21037/23493 [3:41:09<25:49,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   6% 144/2600 [00:33<09:23,  4.36it/s]\u001b[A\n","Epoch 0:  90% 21039/23493 [3:41:09<25:47,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   6% 146/2600 [00:33<09:22,  4.36it/s]\u001b[A\n","Epoch 0:  90% 21041/23493 [3:41:10<25:46,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   6% 148/2600 [00:34<09:24,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21043/23493 [3:41:10<25:45,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   6% 150/2600 [00:34<09:25,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21045/23493 [3:41:11<25:43,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   6% 152/2600 [00:35<09:20,  4.37it/s]\u001b[A\n","Epoch 0:  90% 21047/23493 [3:41:11<25:42,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   6% 154/2600 [00:35<09:23,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21049/23493 [3:41:12<25:41,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   6% 156/2600 [00:35<09:16,  4.39it/s]\u001b[A\n","Epoch 0:  90% 21051/23493 [3:41:12<25:39,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   6% 158/2600 [00:36<09:24,  4.32it/s]\u001b[A\n","Epoch 0:  90% 21053/23493 [3:41:13<25:38,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   6% 160/2600 [00:36<09:15,  4.39it/s]\u001b[A\n","Epoch 0:  90% 21055/23493 [3:41:13<25:36,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   6% 162/2600 [00:37<09:18,  4.36it/s]\u001b[A\n","Epoch 0:  90% 21057/23493 [3:41:13<25:35,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   6% 164/2600 [00:37<09:21,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21059/23493 [3:41:14<25:34,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   6% 166/2600 [00:38<09:14,  4.39it/s]\u001b[A\n","Epoch 0:  90% 21061/23493 [3:41:14<25:32,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   6% 168/2600 [00:38<09:18,  4.36it/s]\u001b[A\n","Epoch 0:  90% 21063/23493 [3:41:15<25:31,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   7% 170/2600 [00:39<09:16,  4.36it/s]\u001b[A\n","Epoch 0:  90% 21065/23493 [3:41:15<25:30,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   7% 172/2600 [00:39<09:20,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21067/23493 [3:41:16<25:28,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   7% 174/2600 [00:40<09:17,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21069/23493 [3:41:16<25:27,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   7% 176/2600 [00:40<09:14,  4.37it/s]\u001b[A\n","Epoch 0:  90% 21071/23493 [3:41:17<25:26,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   7% 178/2600 [00:41<09:16,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21073/23493 [3:41:17<25:24,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   7% 180/2600 [00:41<09:11,  4.39it/s]\u001b[A\n","Epoch 0:  90% 21075/23493 [3:41:18<25:23,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   7% 182/2600 [00:41<09:16,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21077/23493 [3:41:18<25:22,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   7% 184/2600 [00:42<09:14,  4.36it/s]\u001b[A\n","Epoch 0:  90% 21079/23493 [3:41:18<25:20,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   7% 186/2600 [00:42<09:16,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21081/23493 [3:41:19<25:19,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   7% 188/2600 [00:43<09:14,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21083/23493 [3:41:19<25:18,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   7% 190/2600 [00:43<09:15,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21085/23493 [3:41:20<25:16,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   7% 192/2600 [00:44<09:12,  4.36it/s]\u001b[A\n","Epoch 0:  90% 21087/23493 [3:41:20<25:15,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   7% 194/2600 [00:44<09:12,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21089/23493 [3:41:21<25:13,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   8% 196/2600 [00:45<09:16,  4.32it/s]\u001b[A\n","Epoch 0:  90% 21091/23493 [3:41:21<25:12,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   8% 198/2600 [00:45<09:14,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21093/23493 [3:41:22<25:11,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   8% 200/2600 [00:46<09:11,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21095/23493 [3:41:22<25:09,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   8% 202/2600 [00:46<09:12,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21097/23493 [3:41:23<25:08,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   8% 204/2600 [00:47<09:12,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21099/23493 [3:41:23<25:07,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   8% 206/2600 [00:47<09:11,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21101/23493 [3:41:24<25:05,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   8% 208/2600 [00:47<09:10,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21103/23493 [3:41:24<25:04,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   8% 210/2600 [00:48<09:10,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21105/23493 [3:41:24<25:03,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   8% 212/2600 [00:48<09:12,  4.32it/s]\u001b[A\n","Epoch 0:  90% 21107/23493 [3:41:25<25:01,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   8% 214/2600 [00:49<09:10,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21109/23493 [3:41:25<25:00,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   8% 216/2600 [00:49<09:08,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21111/23493 [3:41:26<24:59,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   8% 218/2600 [00:50<09:10,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21113/23493 [3:41:26<24:57,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   8% 220/2600 [00:50<09:07,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21115/23493 [3:41:27<24:56,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   9% 222/2600 [00:51<09:05,  4.36it/s]\u001b[A\n","Epoch 0:  90% 21117/23493 [3:41:27<24:55,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   9% 224/2600 [00:51<09:03,  4.37it/s]\u001b[A\n","Epoch 0:  90% 21119/23493 [3:41:28<24:53,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   9% 226/2600 [00:52<09:04,  4.36it/s]\u001b[A\n","Epoch 0:  90% 21121/23493 [3:41:28<24:52,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   9% 228/2600 [00:52<09:04,  4.36it/s]\u001b[A\n","Epoch 0:  90% 21123/23493 [3:41:29<24:51,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   9% 230/2600 [00:52<09:04,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21125/23493 [3:41:29<24:49,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   9% 232/2600 [00:53<08:58,  4.40it/s]\u001b[A\n","Epoch 0:  90% 21127/23493 [3:41:30<24:48,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   9% 234/2600 [00:53<09:06,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21129/23493 [3:41:30<24:46,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   9% 236/2600 [00:54<09:03,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21131/23493 [3:41:30<24:45,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   9% 238/2600 [00:54<09:01,  4.36it/s]\u001b[A\n","Epoch 0:  90% 21133/23493 [3:41:31<24:44,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   9% 240/2600 [00:55<09:00,  4.37it/s]\u001b[A\n","Epoch 0:  90% 21135/23493 [3:41:31<24:42,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   9% 242/2600 [00:55<09:03,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21137/23493 [3:41:32<24:41,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   9% 244/2600 [00:56<09:03,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21139/23493 [3:41:32<24:40,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:   9% 246/2600 [00:56<09:00,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21141/23493 [3:41:33<24:38,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  10% 248/2600 [00:57<09:03,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21143/23493 [3:41:33<24:37,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  10% 250/2600 [00:57<09:05,  4.31it/s]\u001b[A\n","Epoch 0:  90% 21145/23493 [3:41:34<24:36,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  10% 252/2600 [00:58<09:02,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21147/23493 [3:41:34<24:34,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  10% 254/2600 [00:58<09:02,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21149/23493 [3:41:35<24:33,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  10% 256/2600 [00:58<09:02,  4.32it/s]\u001b[A\n","Epoch 0:  90% 21151/23493 [3:41:35<24:32,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  10% 258/2600 [00:59<09:01,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21153/23493 [3:41:36<24:30,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  10% 260/2600 [00:59<09:06,  4.28it/s]\u001b[A\n","Epoch 0:  90% 21155/23493 [3:41:36<24:29,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  10% 262/2600 [01:00<08:57,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21157/23493 [3:41:36<24:28,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  10% 264/2600 [01:00<08:59,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21159/23493 [3:41:37<24:26,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  10% 266/2600 [01:01<08:57,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21161/23493 [3:41:37<24:25,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  10% 268/2600 [01:01<08:53,  4.37it/s]\u001b[A\n","Epoch 0:  90% 21163/23493 [3:41:38<24:24,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  10% 270/2600 [01:02<08:57,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21165/23493 [3:41:38<24:22,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  10% 272/2600 [01:02<08:59,  4.31it/s]\u001b[A\n","Epoch 0:  90% 21167/23493 [3:41:39<24:21,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  11% 274/2600 [01:03<08:54,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21169/23493 [3:41:39<24:20,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  11% 276/2600 [01:03<08:55,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21171/23493 [3:41:40<24:18,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  11% 278/2600 [01:04<08:56,  4.32it/s]\u001b[A\n","Epoch 0:  90% 21173/23493 [3:41:40<24:17,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  11% 280/2600 [01:04<08:56,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21175/23493 [3:41:41<24:16,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  11% 282/2600 [01:04<08:58,  4.31it/s]\u001b[A\n","Epoch 0:  90% 21177/23493 [3:41:41<24:14,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  11% 284/2600 [01:05<08:54,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21179/23493 [3:41:42<24:13,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  11% 286/2600 [01:05<08:55,  4.32it/s]\u001b[A\n","Epoch 0:  90% 21181/23493 [3:41:42<24:12,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  11% 288/2600 [01:06<08:52,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21183/23493 [3:41:42<24:10,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  11% 290/2600 [01:06<08:51,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21185/23493 [3:41:43<24:09,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  11% 292/2600 [01:07<08:51,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21187/23493 [3:41:43<24:07,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  11% 294/2600 [01:07<08:48,  4.37it/s]\u001b[A\n","Epoch 0:  90% 21189/23493 [3:41:44<24:06,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  11% 296/2600 [01:08<08:49,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21191/23493 [3:41:44<24:05,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  11% 298/2600 [01:08<08:49,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21193/23493 [3:41:45<24:03,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  12% 300/2600 [01:09<08:49,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21195/23493 [3:41:45<24:02,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  12% 302/2600 [01:09<08:52,  4.32it/s]\u001b[A\n","Epoch 0:  90% 21197/23493 [3:41:46<24:01,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  12% 304/2600 [01:10<08:51,  4.32it/s]\u001b[A\n","Epoch 0:  90% 21199/23493 [3:41:46<23:59,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  12% 306/2600 [01:10<08:47,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21201/23493 [3:41:47<23:58,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  12% 308/2600 [01:10<08:52,  4.31it/s]\u001b[A\n","Epoch 0:  90% 21203/23493 [3:41:47<23:57,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  12% 310/2600 [01:11<08:47,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21205/23493 [3:41:48<23:55,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  12% 312/2600 [01:11<08:47,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21207/23493 [3:41:48<23:54,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  12% 314/2600 [01:12<08:42,  4.37it/s]\u001b[A\n","Epoch 0:  90% 21209/23493 [3:41:48<23:53,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  12% 316/2600 [01:12<08:44,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21211/23493 [3:41:49<23:51,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  12% 318/2600 [01:13<08:42,  4.37it/s]\u001b[A\n","Epoch 0:  90% 21213/23493 [3:41:49<23:50,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  12% 320/2600 [01:13<08:43,  4.36it/s]\u001b[A\n","Epoch 0:  90% 21215/23493 [3:41:50<23:49,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  12% 322/2600 [01:14<08:45,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21217/23493 [3:41:50<23:47,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  12% 324/2600 [01:14<08:39,  4.38it/s]\u001b[A\n","Epoch 0:  90% 21219/23493 [3:41:51<23:46,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  13% 326/2600 [01:15<08:44,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21221/23493 [3:41:51<23:45,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  13% 328/2600 [01:15<08:41,  4.36it/s]\u001b[A\n","Epoch 0:  90% 21223/23493 [3:41:52<23:43,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  13% 330/2600 [01:16<08:43,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21225/23493 [3:41:52<23:42,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  13% 332/2600 [01:16<08:43,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21227/23493 [3:41:53<23:41,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  13% 334/2600 [01:16<08:40,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21229/23493 [3:41:53<23:39,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  13% 336/2600 [01:17<08:41,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21231/23493 [3:41:54<23:38,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  13% 338/2600 [01:17<08:39,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21233/23493 [3:41:54<23:37,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  13% 340/2600 [01:18<08:38,  4.36it/s]\u001b[A\n","Epoch 0:  90% 21235/23493 [3:41:54<23:35,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  13% 342/2600 [01:18<08:44,  4.31it/s]\u001b[A\n","Epoch 0:  90% 21237/23493 [3:41:55<23:34,  1.59it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  13% 344/2600 [01:19<08:41,  4.32it/s]\u001b[A\n","Epoch 0:  90% 21239/23493 [3:41:55<23:33,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  13% 346/2600 [01:19<08:35,  4.37it/s]\u001b[A\n","Epoch 0:  90% 21241/23493 [3:41:56<23:31,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  13% 348/2600 [01:20<08:40,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21243/23493 [3:41:56<23:30,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  13% 350/2600 [01:20<08:41,  4.32it/s]\u001b[A\n","Epoch 0:  90% 21245/23493 [3:41:57<23:29,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  14% 352/2600 [01:21<08:40,  4.32it/s]\u001b[A\n","Epoch 0:  90% 21247/23493 [3:41:57<23:27,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  14% 354/2600 [01:21<08:32,  4.38it/s]\u001b[A\n","Epoch 0:  90% 21249/23493 [3:41:58<23:26,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  14% 356/2600 [01:22<08:35,  4.35it/s]\u001b[A\n","Epoch 0:  90% 21251/23493 [3:41:58<23:25,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  14% 358/2600 [01:22<08:37,  4.33it/s]\u001b[A\n","Epoch 0:  90% 21253/23493 [3:41:59<23:23,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  14% 360/2600 [01:22<08:34,  4.36it/s]\u001b[A\n","Epoch 0:  90% 21255/23493 [3:41:59<23:22,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  14% 362/2600 [01:23<08:35,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21257/23493 [3:42:00<23:21,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  14% 364/2600 [01:23<08:35,  4.34it/s]\u001b[A\n","Epoch 0:  90% 21259/23493 [3:42:00<23:19,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  14% 366/2600 [01:24<08:38,  4.31it/s]\u001b[A\n","Epoch 0:  90% 21261/23493 [3:42:00<23:18,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  14% 368/2600 [01:24<08:35,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21263/23493 [3:42:01<23:17,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  14% 370/2600 [01:25<08:33,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21265/23493 [3:42:01<23:15,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  14% 372/2600 [01:25<08:36,  4.32it/s]\u001b[A\n","Epoch 0:  91% 21267/23493 [3:42:02<23:14,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  14% 374/2600 [01:26<08:30,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21269/23493 [3:42:02<23:13,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  14% 376/2600 [01:26<08:33,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21271/23493 [3:42:03<23:11,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  15% 378/2600 [01:27<08:33,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21273/23493 [3:42:03<23:10,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  15% 380/2600 [01:27<08:30,  4.35it/s]\u001b[A\n","Epoch 0:  91% 21275/23493 [3:42:04<23:09,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  15% 382/2600 [01:28<08:30,  4.35it/s]\u001b[A\n","Epoch 0:  91% 21277/23493 [3:42:04<23:07,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  15% 384/2600 [01:28<08:30,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21279/23493 [3:42:05<23:06,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  15% 386/2600 [01:28<08:26,  4.37it/s]\u001b[A\n","Epoch 0:  91% 21281/23493 [3:42:05<23:05,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  15% 388/2600 [01:29<08:28,  4.35it/s]\u001b[A\n","Epoch 0:  91% 21283/23493 [3:42:06<23:03,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  15% 390/2600 [01:29<08:28,  4.35it/s]\u001b[A\n","Epoch 0:  91% 21285/23493 [3:42:06<23:02,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  15% 392/2600 [01:30<08:25,  4.37it/s]\u001b[A\n","Epoch 0:  91% 21287/23493 [3:42:06<23:01,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  15% 394/2600 [01:30<08:30,  4.32it/s]\u001b[A\n","Epoch 0:  91% 21289/23493 [3:42:07<22:59,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  15% 396/2600 [01:31<08:23,  4.38it/s]\u001b[A\n","Epoch 0:  91% 21291/23493 [3:42:07<22:58,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  15% 398/2600 [01:31<08:27,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21293/23493 [3:42:08<22:57,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  15% 400/2600 [01:32<08:32,  4.29it/s]\u001b[A\n","Epoch 0:  91% 21295/23493 [3:42:08<22:55,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  15% 402/2600 [01:32<08:26,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21297/23493 [3:42:09<22:54,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  16% 404/2600 [01:33<08:31,  4.30it/s]\u001b[A\n","Epoch 0:  91% 21299/23493 [3:42:09<22:53,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  16% 406/2600 [01:33<08:27,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21301/23493 [3:42:10<22:51,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  16% 408/2600 [01:34<08:25,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21303/23493 [3:42:10<22:50,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  16% 410/2600 [01:34<08:24,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21305/23493 [3:42:11<22:49,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  16% 412/2600 [01:34<08:28,  4.30it/s]\u001b[A\n","Epoch 0:  91% 21307/23493 [3:42:11<22:47,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  16% 414/2600 [01:35<08:27,  4.31it/s]\u001b[A\n","Epoch 0:  91% 21309/23493 [3:42:12<22:46,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  16% 416/2600 [01:35<08:21,  4.35it/s]\u001b[A\n","Epoch 0:  91% 21311/23493 [3:42:12<22:45,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  16% 418/2600 [01:36<08:22,  4.35it/s]\u001b[A\n","Epoch 0:  91% 21313/23493 [3:42:12<22:43,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  16% 420/2600 [01:36<08:20,  4.35it/s]\u001b[A\n","Epoch 0:  91% 21315/23493 [3:42:13<22:42,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  16% 422/2600 [01:37<08:18,  4.37it/s]\u001b[A\n","Epoch 0:  91% 21317/23493 [3:42:13<22:41,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  16% 424/2600 [01:37<08:17,  4.37it/s]\u001b[A\n","Epoch 0:  91% 21319/23493 [3:42:14<22:39,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  16% 426/2600 [01:38<08:17,  4.37it/s]\u001b[A\n","Epoch 0:  91% 21321/23493 [3:42:14<22:38,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  16% 428/2600 [01:38<08:22,  4.32it/s]\u001b[A\n","Epoch 0:  91% 21323/23493 [3:42:15<22:37,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  17% 430/2600 [01:39<08:15,  4.38it/s]\u001b[A\n","Epoch 0:  91% 21325/23493 [3:42:15<22:35,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  17% 432/2600 [01:39<08:18,  4.35it/s]\u001b[A\n","Epoch 0:  91% 21327/23493 [3:42:16<22:34,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  17% 434/2600 [01:40<08:21,  4.32it/s]\u001b[A\n","Epoch 0:  91% 21329/23493 [3:42:16<22:33,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  17% 436/2600 [01:40<08:18,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21331/23493 [3:42:17<22:31,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  17% 438/2600 [01:40<08:15,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21333/23493 [3:42:17<22:30,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  17% 440/2600 [01:41<08:19,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21335/23493 [3:42:17<22:29,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  17% 442/2600 [01:41<08:17,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21337/23493 [3:42:18<22:27,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  17% 444/2600 [01:42<08:18,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21339/23493 [3:42:18<22:26,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  17% 446/2600 [01:42<08:17,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21341/23493 [3:42:19<22:25,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  17% 448/2600 [01:43<08:11,  4.38it/s]\u001b[A\n","Epoch 0:  91% 21343/23493 [3:42:19<22:23,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  17% 450/2600 [01:43<08:12,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21345/23493 [3:42:20<22:22,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  17% 452/2600 [01:44<08:16,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21347/23493 [3:42:20<22:21,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  17% 454/2600 [01:44<08:12,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21349/23493 [3:42:21<22:19,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  18% 456/2600 [01:45<08:15,  4.32it/s]\u001b[A\n","Epoch 0:  91% 21351/23493 [3:42:21<22:18,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  18% 458/2600 [01:45<08:15,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21353/23493 [3:42:22<22:17,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  18% 460/2600 [01:45<08:16,  4.31it/s]\u001b[A\n","Epoch 0:  91% 21355/23493 [3:42:22<22:15,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  18% 462/2600 [01:46<08:16,  4.31it/s]\u001b[A\n","Epoch 0:  91% 21357/23493 [3:42:23<22:14,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  18% 464/2600 [01:46<08:10,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21359/23493 [3:42:23<22:13,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  18% 466/2600 [01:47<08:14,  4.31it/s]\u001b[A\n","Epoch 0:  91% 21361/23493 [3:42:23<22:11,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  18% 468/2600 [01:47<08:20,  4.26it/s]\u001b[A\n","Epoch 0:  91% 21363/23493 [3:42:24<22:10,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  18% 470/2600 [01:48<08:13,  4.32it/s]\u001b[A\n","Epoch 0:  91% 21365/23493 [3:42:24<22:09,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  18% 472/2600 [01:48<08:13,  4.31it/s]\u001b[A\n","Epoch 0:  91% 21367/23493 [3:42:25<22:07,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  18% 474/2600 [01:49<08:14,  4.30it/s]\u001b[A\n","Epoch 0:  91% 21369/23493 [3:42:25<22:06,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  18% 476/2600 [01:49<08:08,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21371/23493 [3:42:26<22:05,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  18% 478/2600 [01:50<08:08,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21373/23493 [3:42:26<22:03,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  18% 480/2600 [01:50<08:07,  4.35it/s]\u001b[A\n","Epoch 0:  91% 21375/23493 [3:42:27<22:02,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  19% 482/2600 [01:51<08:05,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21377/23493 [3:42:27<22:01,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  19% 484/2600 [01:51<08:11,  4.31it/s]\u001b[A\n","Epoch 0:  91% 21379/23493 [3:42:28<21:59,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  19% 486/2600 [01:51<08:07,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21381/23493 [3:42:28<21:58,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  19% 488/2600 [01:52<08:05,  4.35it/s]\u001b[A\n","Epoch 0:  91% 21383/23493 [3:42:29<21:57,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  19% 490/2600 [01:52<08:05,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21385/23493 [3:42:29<21:55,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  19% 492/2600 [01:53<08:05,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21387/23493 [3:42:29<21:54,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  19% 494/2600 [01:53<08:01,  4.37it/s]\u001b[A\n","Epoch 0:  91% 21389/23493 [3:42:30<21:53,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  19% 496/2600 [01:54<08:04,  4.35it/s]\u001b[A\n","Epoch 0:  91% 21391/23493 [3:42:30<21:51,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  19% 498/2600 [01:54<08:06,  4.32it/s]\u001b[A\n","Epoch 0:  91% 21393/23493 [3:42:31<21:50,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  19% 500/2600 [01:55<07:57,  4.39it/s]\u001b[A\n","Epoch 0:  91% 21395/23493 [3:42:31<21:49,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  19% 502/2600 [01:55<08:03,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21397/23493 [3:42:32<21:47,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  19% 504/2600 [01:56<07:58,  4.38it/s]\u001b[A\n","Epoch 0:  91% 21399/23493 [3:42:32<21:46,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  19% 506/2600 [01:56<08:01,  4.35it/s]\u001b[A\n","Epoch 0:  91% 21401/23493 [3:42:33<21:45,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  20% 508/2600 [01:57<08:01,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21403/23493 [3:42:33<21:43,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  20% 510/2600 [01:57<08:09,  4.27it/s]\u001b[A\n","Epoch 0:  91% 21405/23493 [3:42:34<21:42,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  20% 512/2600 [01:57<08:02,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21407/23493 [3:42:34<21:41,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  20% 514/2600 [01:58<07:58,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21409/23493 [3:42:35<21:40,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  20% 516/2600 [01:58<08:01,  4.32it/s]\u001b[A\n","Epoch 0:  91% 21411/23493 [3:42:35<21:38,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  20% 518/2600 [01:59<08:01,  4.32it/s]\u001b[A\n","Epoch 0:  91% 21413/23493 [3:42:35<21:37,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  20% 520/2600 [01:59<07:56,  4.37it/s]\u001b[A\n","Epoch 0:  91% 21415/23493 [3:42:36<21:36,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  20% 522/2600 [02:00<07:58,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21417/23493 [3:42:36<21:34,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  20% 524/2600 [02:00<07:58,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21419/23493 [3:42:37<21:33,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  20% 526/2600 [02:01<07:59,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21421/23493 [3:42:37<21:32,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  20% 528/2600 [02:01<07:55,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21423/23493 [3:42:38<21:30,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  20% 530/2600 [02:02<07:58,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21425/23493 [3:42:38<21:29,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  20% 532/2600 [02:02<07:56,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21427/23493 [3:42:39<21:28,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  21% 534/2600 [02:03<07:54,  4.35it/s]\u001b[A\n","Epoch 0:  91% 21429/23493 [3:42:39<21:26,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  21% 536/2600 [02:03<07:56,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21431/23493 [3:42:40<21:25,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  21% 538/2600 [02:03<07:56,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21433/23493 [3:42:40<21:24,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  21% 540/2600 [02:04<07:53,  4.35it/s]\u001b[A\n","Epoch 0:  91% 21435/23493 [3:42:41<21:22,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  21% 542/2600 [02:04<07:51,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21437/23493 [3:42:41<21:21,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  21% 544/2600 [02:05<07:48,  4.39it/s]\u001b[A\n","Epoch 0:  91% 21439/23493 [3:42:41<21:20,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  21% 546/2600 [02:05<07:53,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21441/23493 [3:42:42<21:18,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  21% 548/2600 [02:06<07:50,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21443/23493 [3:42:42<21:17,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  21% 550/2600 [02:06<07:49,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21445/23493 [3:42:43<21:16,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  21% 552/2600 [02:07<07:54,  4.32it/s]\u001b[A\n","Epoch 0:  91% 21447/23493 [3:42:43<21:14,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  21% 554/2600 [02:07<07:53,  4.32it/s]\u001b[A\n","Epoch 0:  91% 21449/23493 [3:42:44<21:13,  1.60it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  21% 556/2600 [02:08<07:48,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21451/23493 [3:42:44<21:12,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  21% 558/2600 [02:08<07:50,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21453/23493 [3:42:45<21:10,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  22% 560/2600 [02:09<07:48,  4.35it/s]\u001b[A\n","Epoch 0:  91% 21455/23493 [3:42:45<21:09,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  22% 562/2600 [02:09<07:50,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21457/23493 [3:42:46<21:08,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  22% 564/2600 [02:09<07:51,  4.32it/s]\u001b[A\n","Epoch 0:  91% 21459/23493 [3:42:46<21:06,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  22% 566/2600 [02:10<07:50,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21461/23493 [3:42:47<21:05,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  22% 568/2600 [02:10<07:44,  4.37it/s]\u001b[A\n","Epoch 0:  91% 21463/23493 [3:42:47<21:04,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  22% 570/2600 [02:11<07:45,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21465/23493 [3:42:47<21:02,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  22% 572/2600 [02:11<07:44,  4.37it/s]\u001b[A\n","Epoch 0:  91% 21467/23493 [3:42:48<21:01,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  22% 574/2600 [02:12<07:47,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21469/23493 [3:42:48<21:00,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  22% 576/2600 [02:12<07:44,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21471/23493 [3:42:49<20:59,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  22% 578/2600 [02:13<07:46,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21473/23493 [3:42:49<20:57,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  22% 580/2600 [02:13<07:39,  4.39it/s]\u001b[A\n","Epoch 0:  91% 21475/23493 [3:42:50<20:56,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  22% 582/2600 [02:14<07:41,  4.37it/s]\u001b[A\n","Epoch 0:  91% 21477/23493 [3:42:50<20:55,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  22% 584/2600 [02:14<07:45,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21479/23493 [3:42:51<20:53,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  23% 586/2600 [02:15<07:40,  4.38it/s]\u001b[A\n","Epoch 0:  91% 21481/23493 [3:42:51<20:52,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  23% 588/2600 [02:15<07:41,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21483/23493 [3:42:52<20:51,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  23% 590/2600 [02:15<07:45,  4.32it/s]\u001b[A\n","Epoch 0:  91% 21485/23493 [3:42:52<20:49,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  23% 592/2600 [02:16<07:49,  4.28it/s]\u001b[A\n","Epoch 0:  91% 21487/23493 [3:42:52<20:48,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  23% 594/2600 [02:16<07:40,  4.36it/s]\u001b[A\n","Epoch 0:  91% 21489/23493 [3:42:53<20:47,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  23% 596/2600 [02:17<07:37,  4.38it/s]\u001b[A\n","Epoch 0:  91% 21491/23493 [3:42:53<20:45,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  23% 598/2600 [02:17<07:41,  4.34it/s]\u001b[A\n","Epoch 0:  91% 21493/23493 [3:42:54<20:44,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  23% 600/2600 [02:18<07:42,  4.33it/s]\u001b[A\n","Epoch 0:  91% 21495/23493 [3:42:54<20:43,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  23% 602/2600 [02:18<07:43,  4.31it/s]\u001b[A\n","Epoch 0:  92% 21497/23493 [3:42:55<20:41,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  23% 604/2600 [02:19<07:40,  4.33it/s]\u001b[A\n","Epoch 0:  92% 21499/23493 [3:42:55<20:40,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  23% 606/2600 [02:19<07:40,  4.33it/s]\u001b[A\n","Epoch 0:  92% 21501/23493 [3:42:56<20:39,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  23% 608/2600 [02:20<07:37,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21503/23493 [3:42:56<20:37,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  23% 610/2600 [02:20<07:36,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21505/23493 [3:42:57<20:36,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  24% 612/2600 [02:21<07:40,  4.32it/s]\u001b[A\n","Epoch 0:  92% 21507/23493 [3:42:57<20:35,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  24% 614/2600 [02:21<07:37,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21509/23493 [3:42:58<20:33,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  24% 616/2600 [02:21<07:36,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21511/23493 [3:42:58<20:32,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  24% 618/2600 [02:22<07:31,  4.39it/s]\u001b[A\n","Epoch 0:  92% 21513/23493 [3:42:58<20:31,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  24% 620/2600 [02:22<07:34,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21515/23493 [3:42:59<20:30,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  24% 622/2600 [02:23<07:36,  4.34it/s]\u001b[A\n","Epoch 0:  92% 21517/23493 [3:42:59<20:28,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  24% 624/2600 [02:23<07:30,  4.38it/s]\u001b[A\n","Epoch 0:  92% 21519/23493 [3:43:00<20:27,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  24% 626/2600 [02:24<07:33,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21521/23493 [3:43:00<20:26,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  24% 628/2600 [02:24<07:34,  4.33it/s]\u001b[A\n","Epoch 0:  92% 21523/23493 [3:43:01<20:24,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  24% 630/2600 [02:25<07:34,  4.33it/s]\u001b[A\n","Epoch 0:  92% 21525/23493 [3:43:01<20:23,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  24% 632/2600 [02:25<07:32,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21527/23493 [3:43:02<20:22,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  24% 634/2600 [02:26<07:33,  4.34it/s]\u001b[A\n","Epoch 0:  92% 21529/23493 [3:43:02<20:20,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  24% 636/2600 [02:26<07:30,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21531/23493 [3:43:03<20:19,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  25% 638/2600 [02:26<07:31,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21533/23493 [3:43:03<20:18,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  25% 640/2600 [02:27<07:33,  4.32it/s]\u001b[A\n","Epoch 0:  92% 21535/23493 [3:43:04<20:16,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  25% 642/2600 [02:27<07:29,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21537/23493 [3:43:04<20:15,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  25% 644/2600 [02:28<07:31,  4.33it/s]\u001b[A\n","Epoch 0:  92% 21539/23493 [3:43:04<20:14,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  25% 646/2600 [02:28<07:28,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21541/23493 [3:43:05<20:12,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  25% 648/2600 [02:29<07:27,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21543/23493 [3:43:05<20:11,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  25% 650/2600 [02:29<07:28,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21545/23493 [3:43:06<20:10,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  25% 652/2600 [02:30<07:28,  4.34it/s]\u001b[A\n","Epoch 0:  92% 21547/23493 [3:43:06<20:09,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  25% 654/2600 [02:30<07:23,  4.39it/s]\u001b[A\n","Epoch 0:  92% 21549/23493 [3:43:07<20:07,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  25% 656/2600 [02:31<07:23,  4.39it/s]\u001b[A\n","Epoch 0:  92% 21551/23493 [3:43:07<20:06,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  25% 658/2600 [02:31<07:26,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21553/23493 [3:43:08<20:05,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  25% 660/2600 [02:32<07:26,  4.34it/s]\u001b[A\n","Epoch 0:  92% 21555/23493 [3:43:08<20:03,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  25% 662/2600 [02:32<07:22,  4.38it/s]\u001b[A\n","Epoch 0:  92% 21557/23493 [3:43:09<20:02,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  26% 664/2600 [02:32<07:27,  4.32it/s]\u001b[A\n","Epoch 0:  92% 21559/23493 [3:43:09<20:01,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  26% 666/2600 [02:33<07:23,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21561/23493 [3:43:10<19:59,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  26% 668/2600 [02:33<07:23,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21563/23493 [3:43:10<19:58,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  26% 670/2600 [02:34<07:24,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21565/23493 [3:43:10<19:57,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  26% 672/2600 [02:34<07:20,  4.38it/s]\u001b[A\n","Epoch 0:  92% 21567/23493 [3:43:11<19:55,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  26% 674/2600 [02:35<07:22,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21569/23493 [3:43:11<19:54,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  26% 676/2600 [02:35<07:20,  4.37it/s]\u001b[A\n","Epoch 0:  92% 21571/23493 [3:43:12<19:53,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  26% 678/2600 [02:36<07:20,  4.37it/s]\u001b[A\n","Epoch 0:  92% 21573/23493 [3:43:12<19:51,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  26% 680/2600 [02:36<07:20,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21575/23493 [3:43:13<19:50,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  26% 682/2600 [02:37<07:24,  4.31it/s]\u001b[A\n","Epoch 0:  92% 21577/23493 [3:43:13<19:49,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  26% 684/2600 [02:37<07:20,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21579/23493 [3:43:14<19:48,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  26% 686/2600 [02:38<07:18,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21581/23493 [3:43:14<19:46,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  26% 688/2600 [02:38<07:20,  4.34it/s]\u001b[A\n","Epoch 0:  92% 21583/23493 [3:43:15<19:45,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  27% 690/2600 [02:38<07:15,  4.38it/s]\u001b[A\n","Epoch 0:  92% 21585/23493 [3:43:15<19:44,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  27% 692/2600 [02:39<07:18,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21587/23493 [3:43:15<19:42,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  27% 694/2600 [02:39<07:16,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21589/23493 [3:43:16<19:41,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  27% 696/2600 [02:40<07:16,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21591/23493 [3:43:16<19:40,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  27% 698/2600 [02:40<07:18,  4.33it/s]\u001b[A\n","Epoch 0:  92% 21593/23493 [3:43:17<19:38,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  27% 700/2600 [02:41<07:13,  4.38it/s]\u001b[A\n","Epoch 0:  92% 21595/23493 [3:43:17<19:37,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  27% 702/2600 [02:41<07:17,  4.34it/s]\u001b[A\n","Epoch 0:  92% 21597/23493 [3:43:18<19:36,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  27% 704/2600 [02:42<07:12,  4.38it/s]\u001b[A\n","Epoch 0:  92% 21599/23493 [3:43:18<19:34,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  27% 706/2600 [02:42<07:13,  4.37it/s]\u001b[A\n","Epoch 0:  92% 21601/23493 [3:43:19<19:33,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  27% 708/2600 [02:43<07:09,  4.41it/s]\u001b[A\n","Epoch 0:  92% 21603/23493 [3:43:19<19:32,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  27% 710/2600 [02:43<07:15,  4.34it/s]\u001b[A\n","Epoch 0:  92% 21605/23493 [3:43:20<19:30,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  27% 712/2600 [02:43<07:14,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21607/23493 [3:43:20<19:29,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  27% 714/2600 [02:44<07:11,  4.37it/s]\u001b[A\n","Epoch 0:  92% 21609/23493 [3:43:21<19:28,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  28% 716/2600 [02:44<07:14,  4.34it/s]\u001b[A\n","Epoch 0:  92% 21611/23493 [3:43:21<19:27,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  28% 718/2600 [02:45<07:15,  4.32it/s]\u001b[A\n","Epoch 0:  92% 21613/23493 [3:43:21<19:25,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  28% 720/2600 [02:45<07:16,  4.31it/s]\u001b[A\n","Epoch 0:  92% 21615/23493 [3:43:22<19:24,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  28% 722/2600 [02:46<07:10,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21617/23493 [3:43:22<19:23,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  28% 724/2600 [02:46<07:07,  4.39it/s]\u001b[A\n","Epoch 0:  92% 21619/23493 [3:43:23<19:21,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  28% 726/2600 [02:47<07:08,  4.37it/s]\u001b[A\n","Epoch 0:  92% 21621/23493 [3:43:23<19:20,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  28% 728/2600 [02:47<07:04,  4.41it/s]\u001b[A\n","Epoch 0:  92% 21623/23493 [3:43:24<19:19,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  28% 730/2600 [02:48<07:07,  4.37it/s]\u001b[A\n","Epoch 0:  92% 21625/23493 [3:43:24<19:17,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  28% 732/2600 [02:48<07:11,  4.33it/s]\u001b[A\n","Epoch 0:  92% 21627/23493 [3:43:25<19:16,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  28% 734/2600 [02:49<07:04,  4.40it/s]\u001b[A\n","Epoch 0:  92% 21629/23493 [3:43:25<19:15,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  28% 736/2600 [02:49<07:02,  4.41it/s]\u001b[A\n","Epoch 0:  92% 21631/23493 [3:43:26<19:13,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  28% 738/2600 [02:49<07:05,  4.38it/s]\u001b[A\n","Epoch 0:  92% 21633/23493 [3:43:26<19:12,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  28% 740/2600 [02:50<07:09,  4.33it/s]\u001b[A\n","Epoch 0:  92% 21635/23493 [3:43:26<19:11,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  29% 742/2600 [02:50<07:05,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21637/23493 [3:43:27<19:10,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  29% 744/2600 [02:51<07:07,  4.34it/s]\u001b[A\n","Epoch 0:  92% 21639/23493 [3:43:27<19:08,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  29% 746/2600 [02:51<07:09,  4.32it/s]\u001b[A\n","Epoch 0:  92% 21641/23493 [3:43:28<19:07,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  29% 748/2600 [02:52<07:06,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21643/23493 [3:43:28<19:06,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  29% 750/2600 [02:52<07:04,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21645/23493 [3:43:29<19:04,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  29% 752/2600 [02:53<07:02,  4.38it/s]\u001b[A\n","Epoch 0:  92% 21647/23493 [3:43:29<19:03,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  29% 754/2600 [02:53<07:00,  4.39it/s]\u001b[A\n","Epoch 0:  92% 21649/23493 [3:43:30<19:02,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  29% 756/2600 [02:54<06:56,  4.42it/s]\u001b[A\n","Epoch 0:  92% 21651/23493 [3:43:30<19:00,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  29% 758/2600 [02:54<07:00,  4.38it/s]\u001b[A\n","Epoch 0:  92% 21653/23493 [3:43:31<18:59,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  29% 760/2600 [02:54<07:00,  4.37it/s]\u001b[A\n","Epoch 0:  92% 21655/23493 [3:43:31<18:58,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  29% 762/2600 [02:55<07:01,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21657/23493 [3:43:32<18:57,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  29% 764/2600 [02:55<07:02,  4.34it/s]\u001b[A\n","Epoch 0:  92% 21659/23493 [3:43:32<18:55,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  29% 766/2600 [02:56<06:58,  4.38it/s]\u001b[A\n","Epoch 0:  92% 21661/23493 [3:43:32<18:54,  1.61it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  30% 768/2600 [02:56<06:59,  4.37it/s]\u001b[A\n","Epoch 0:  92% 21663/23493 [3:43:33<18:53,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  30% 770/2600 [02:57<06:56,  4.40it/s]\u001b[A\n","Epoch 0:  92% 21665/23493 [3:43:33<18:51,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  30% 772/2600 [02:57<06:58,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21667/23493 [3:43:34<18:50,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  30% 774/2600 [02:58<07:00,  4.34it/s]\u001b[A\n","Epoch 0:  92% 21669/23493 [3:43:34<18:49,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  30% 776/2600 [02:58<06:57,  4.37it/s]\u001b[A\n","Epoch 0:  92% 21671/23493 [3:43:35<18:47,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  30% 778/2600 [02:59<06:58,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21673/23493 [3:43:35<18:46,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  30% 780/2600 [02:59<06:55,  4.38it/s]\u001b[A\n","Epoch 0:  92% 21675/23493 [3:43:36<18:45,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  30% 782/2600 [03:00<06:56,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21677/23493 [3:43:36<18:43,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  30% 784/2600 [03:00<06:59,  4.33it/s]\u001b[A\n","Epoch 0:  92% 21679/23493 [3:43:37<18:42,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  30% 786/2600 [03:00<06:55,  4.37it/s]\u001b[A\n","Epoch 0:  92% 21681/23493 [3:43:37<18:41,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  30% 788/2600 [03:01<06:58,  4.33it/s]\u001b[A\n","Epoch 0:  92% 21683/23493 [3:43:37<18:40,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  30% 790/2600 [03:01<06:52,  4.39it/s]\u001b[A\n","Epoch 0:  92% 21685/23493 [3:43:38<18:38,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  30% 792/2600 [03:02<06:55,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21687/23493 [3:43:38<18:37,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  31% 794/2600 [03:02<06:58,  4.31it/s]\u001b[A\n","Epoch 0:  92% 21689/23493 [3:43:39<18:36,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  31% 796/2600 [03:03<06:52,  4.38it/s]\u001b[A\n","Epoch 0:  92% 21691/23493 [3:43:39<18:34,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  31% 798/2600 [03:03<06:55,  4.34it/s]\u001b[A\n","Epoch 0:  92% 21693/23493 [3:43:40<18:33,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  31% 800/2600 [03:04<06:49,  4.40it/s]\u001b[A\n","Epoch 0:  92% 21695/23493 [3:43:40<18:32,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  31% 802/2600 [03:04<06:52,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21697/23493 [3:43:41<18:30,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  31% 804/2600 [03:05<06:50,  4.38it/s]\u001b[A\n","Epoch 0:  92% 21699/23493 [3:43:41<18:29,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  31% 806/2600 [03:05<06:54,  4.33it/s]\u001b[A\n","Epoch 0:  92% 21701/23493 [3:43:42<18:28,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  31% 808/2600 [03:05<06:54,  4.33it/s]\u001b[A\n","Epoch 0:  92% 21703/23493 [3:43:42<18:27,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  31% 810/2600 [03:06<06:49,  4.37it/s]\u001b[A\n","Epoch 0:  92% 21705/23493 [3:43:43<18:25,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  31% 812/2600 [03:06<06:51,  4.34it/s]\u001b[A\n","Epoch 0:  92% 21707/23493 [3:43:43<18:24,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  31% 814/2600 [03:07<06:45,  4.40it/s]\u001b[A\n","Epoch 0:  92% 21709/23493 [3:43:43<18:23,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  31% 816/2600 [03:07<06:49,  4.36it/s]\u001b[A\n","Epoch 0:  92% 21711/23493 [3:43:44<18:21,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  31% 818/2600 [03:08<06:45,  4.40it/s]\u001b[A\n","Epoch 0:  92% 21713/23493 [3:43:44<18:20,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  32% 820/2600 [03:08<06:43,  4.41it/s]\u001b[A\n","Epoch 0:  92% 21715/23493 [3:43:45<18:19,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  32% 822/2600 [03:09<06:44,  4.39it/s]\u001b[A\n","Epoch 0:  92% 21717/23493 [3:43:45<18:17,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  32% 824/2600 [03:09<06:43,  4.40it/s]\u001b[A\n","Epoch 0:  92% 21719/23493 [3:43:46<18:16,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  32% 826/2600 [03:10<06:45,  4.37it/s]\u001b[A\n","Epoch 0:  92% 21721/23493 [3:43:46<18:15,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  32% 828/2600 [03:10<06:42,  4.40it/s]\u001b[A\n","Epoch 0:  92% 21723/23493 [3:43:47<18:14,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  32% 830/2600 [03:11<06:46,  4.35it/s]\u001b[A\n","Epoch 0:  92% 21725/23493 [3:43:47<18:12,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  32% 832/2600 [03:11<06:44,  4.37it/s]\u001b[A\n","Epoch 0:  92% 21727/23493 [3:43:48<18:11,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  32% 834/2600 [03:11<06:50,  4.31it/s]\u001b[A\n","Epoch 0:  92% 21729/23493 [3:43:48<18:10,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  32% 836/2600 [03:12<06:41,  4.39it/s]\u001b[A\n","Epoch 0:  92% 21731/23493 [3:43:48<18:08,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  32% 838/2600 [03:12<06:43,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21733/23493 [3:43:49<18:07,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  32% 840/2600 [03:13<06:43,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21735/23493 [3:43:49<18:06,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  32% 842/2600 [03:13<06:41,  4.38it/s]\u001b[A\n","Epoch 0:  93% 21737/23493 [3:43:50<18:04,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  32% 844/2600 [03:14<06:43,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21739/23493 [3:43:50<18:03,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  33% 846/2600 [03:14<06:45,  4.33it/s]\u001b[A\n","Epoch 0:  93% 21741/23493 [3:43:51<18:02,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  33% 848/2600 [03:15<06:41,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21743/23493 [3:43:51<18:01,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  33% 850/2600 [03:15<06:38,  4.40it/s]\u001b[A\n","Epoch 0:  93% 21745/23493 [3:43:52<17:59,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  33% 852/2600 [03:16<06:40,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21747/23493 [3:43:52<17:58,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  33% 854/2600 [03:16<06:36,  4.40it/s]\u001b[A\n","Epoch 0:  93% 21749/23493 [3:43:53<17:57,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  33% 856/2600 [03:16<06:41,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21751/23493 [3:43:53<17:55,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  33% 858/2600 [03:17<06:38,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21753/23493 [3:43:54<17:54,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  33% 860/2600 [03:17<06:43,  4.31it/s]\u001b[A\n","Epoch 0:  93% 21755/23493 [3:43:54<17:53,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  33% 862/2600 [03:18<06:39,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21757/23493 [3:43:54<17:51,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  33% 864/2600 [03:18<06:40,  4.33it/s]\u001b[A\n","Epoch 0:  93% 21759/23493 [3:43:55<17:50,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  33% 866/2600 [03:19<06:37,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21761/23493 [3:43:55<17:49,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  33% 868/2600 [03:19<06:34,  4.39it/s]\u001b[A\n","Epoch 0:  93% 21763/23493 [3:43:56<17:48,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  33% 870/2600 [03:20<06:37,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21765/23493 [3:43:56<17:46,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  34% 872/2600 [03:20<06:38,  4.34it/s]\u001b[A\n","Epoch 0:  93% 21767/23493 [3:43:57<17:45,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  34% 874/2600 [03:21<06:37,  4.34it/s]\u001b[A\n","Epoch 0:  93% 21769/23493 [3:43:57<17:44,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  34% 876/2600 [03:21<06:34,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21771/23493 [3:43:58<17:42,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  34% 878/2600 [03:22<06:33,  4.38it/s]\u001b[A\n","Epoch 0:  93% 21773/23493 [3:43:58<17:41,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  34% 880/2600 [03:22<06:37,  4.33it/s]\u001b[A\n","Epoch 0:  93% 21775/23493 [3:43:59<17:40,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  34% 882/2600 [03:22<06:33,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21777/23493 [3:43:59<17:39,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  34% 884/2600 [03:23<06:34,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21779/23493 [3:43:59<17:37,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  34% 886/2600 [03:23<06:34,  4.34it/s]\u001b[A\n","Epoch 0:  93% 21781/23493 [3:44:00<17:36,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  34% 888/2600 [03:24<06:32,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21783/23493 [3:44:00<17:35,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  34% 890/2600 [03:24<06:31,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21785/23493 [3:44:01<17:33,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  34% 892/2600 [03:25<06:32,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21787/23493 [3:44:01<17:32,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  34% 894/2600 [03:25<06:27,  4.40it/s]\u001b[A\n","Epoch 0:  93% 21789/23493 [3:44:02<17:31,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  34% 896/2600 [03:26<06:29,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21791/23493 [3:44:02<17:29,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  35% 898/2600 [03:26<06:31,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21793/23493 [3:44:03<17:28,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  35% 900/2600 [03:27<06:29,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21795/23493 [3:44:03<17:27,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  35% 902/2600 [03:27<06:25,  4.40it/s]\u001b[A\n","Epoch 0:  93% 21797/23493 [3:44:04<17:26,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  35% 904/2600 [03:27<06:29,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21799/23493 [3:44:04<17:24,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  35% 906/2600 [03:28<06:25,  4.39it/s]\u001b[A\n","Epoch 0:  93% 21801/23493 [3:44:05<17:23,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  35% 908/2600 [03:28<06:28,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21803/23493 [3:44:05<17:22,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  35% 910/2600 [03:29<06:28,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21805/23493 [3:44:05<17:20,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  35% 912/2600 [03:29<06:27,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21807/23493 [3:44:06<17:19,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  35% 914/2600 [03:30<06:28,  4.34it/s]\u001b[A\n","Epoch 0:  93% 21809/23493 [3:44:06<17:18,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  35% 916/2600 [03:30<06:26,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21811/23493 [3:44:07<17:17,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  35% 918/2600 [03:31<06:26,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21813/23493 [3:44:07<17:15,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  35% 920/2600 [03:31<06:23,  4.39it/s]\u001b[A\n","Epoch 0:  93% 21815/23493 [3:44:08<17:14,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  35% 922/2600 [03:32<06:24,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21817/23493 [3:44:08<17:13,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  36% 924/2600 [03:32<06:23,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21819/23493 [3:44:09<17:11,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  36% 926/2600 [03:33<06:24,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21821/23493 [3:44:09<17:10,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  36% 928/2600 [03:33<06:25,  4.34it/s]\u001b[A\n","Epoch 0:  93% 21823/23493 [3:44:10<17:09,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  36% 930/2600 [03:33<06:23,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21825/23493 [3:44:10<17:07,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  36% 932/2600 [03:34<06:22,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21827/23493 [3:44:10<17:06,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  36% 934/2600 [03:34<06:20,  4.38it/s]\u001b[A\n","Epoch 0:  93% 21829/23493 [3:44:11<17:05,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  36% 936/2600 [03:35<06:21,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21831/23493 [3:44:11<17:04,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  36% 938/2600 [03:35<06:17,  4.41it/s]\u001b[A\n","Epoch 0:  93% 21833/23493 [3:44:12<17:02,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  36% 940/2600 [03:36<06:19,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21835/23493 [3:44:12<17:01,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  36% 942/2600 [03:36<06:17,  4.39it/s]\u001b[A\n","Epoch 0:  93% 21837/23493 [3:44:13<17:00,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  36% 944/2600 [03:37<06:22,  4.33it/s]\u001b[A\n","Epoch 0:  93% 21839/23493 [3:44:13<16:58,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  36% 946/2600 [03:37<06:21,  4.34it/s]\u001b[A\n","Epoch 0:  93% 21841/23493 [3:44:14<16:57,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  36% 948/2600 [03:38<06:19,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21843/23493 [3:44:14<16:56,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  37% 950/2600 [03:38<06:18,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21845/23493 [3:44:15<16:55,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  37% 952/2600 [03:38<06:14,  4.40it/s]\u001b[A\n","Epoch 0:  93% 21847/23493 [3:44:15<16:53,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  37% 954/2600 [03:39<06:16,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21849/23493 [3:44:16<16:52,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  37% 956/2600 [03:39<06:13,  4.40it/s]\u001b[A\n","Epoch 0:  93% 21851/23493 [3:44:16<16:51,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  37% 958/2600 [03:40<06:16,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21853/23493 [3:44:16<16:49,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  37% 960/2600 [03:40<06:14,  4.38it/s]\u001b[A\n","Epoch 0:  93% 21855/23493 [3:44:17<16:48,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  37% 962/2600 [03:41<06:15,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21857/23493 [3:44:17<16:47,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  37% 964/2600 [03:41<06:12,  4.39it/s]\u001b[A\n","Epoch 0:  93% 21859/23493 [3:44:18<16:46,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  37% 966/2600 [03:42<06:14,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21861/23493 [3:44:18<16:44,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  37% 968/2600 [03:42<06:15,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21863/23493 [3:44:19<16:43,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  37% 970/2600 [03:43<06:14,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21865/23493 [3:44:19<16:42,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  37% 972/2600 [03:43<06:14,  4.34it/s]\u001b[A\n","Epoch 0:  93% 21867/23493 [3:44:20<16:40,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  37% 974/2600 [03:44<06:18,  4.30it/s]\u001b[A\n","Epoch 0:  93% 21869/23493 [3:44:20<16:39,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  38% 976/2600 [03:44<06:18,  4.29it/s]\u001b[A\n","Epoch 0:  93% 21871/23493 [3:44:21<16:38,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  38% 978/2600 [03:44<06:12,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21873/23493 [3:44:21<16:37,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  38% 980/2600 [03:45<06:13,  4.34it/s]\u001b[A\n","Epoch 0:  93% 21875/23493 [3:44:22<16:35,  1.62it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  38% 982/2600 [03:45<06:12,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21877/23493 [3:44:22<16:34,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  38% 984/2600 [03:46<06:11,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21879/23493 [3:44:22<16:33,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  38% 986/2600 [03:46<06:11,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21881/23493 [3:44:23<16:31,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  38% 988/2600 [03:47<06:14,  4.30it/s]\u001b[A\n","Epoch 0:  93% 21883/23493 [3:44:23<16:30,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  38% 990/2600 [03:47<06:06,  4.40it/s]\u001b[A\n","Epoch 0:  93% 21885/23493 [3:44:24<16:29,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  38% 992/2600 [03:48<06:07,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21887/23493 [3:44:24<16:28,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  38% 994/2600 [03:48<06:04,  4.41it/s]\u001b[A\n","Epoch 0:  93% 21889/23493 [3:44:25<16:26,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  38% 996/2600 [03:49<06:06,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21891/23493 [3:44:25<16:25,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  38% 998/2600 [03:49<06:04,  4.40it/s]\u001b[A\n","Epoch 0:  93% 21893/23493 [3:44:26<16:24,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  38% 1000/2600 [03:49<06:06,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21895/23493 [3:44:26<16:22,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  39% 1002/2600 [03:50<06:07,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21897/23493 [3:44:27<16:21,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  39% 1004/2600 [03:50<06:06,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21899/23493 [3:44:27<16:20,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  39% 1006/2600 [03:51<06:08,  4.32it/s]\u001b[A\n","Epoch 0:  93% 21901/23493 [3:44:27<16:18,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  39% 1008/2600 [03:51<06:03,  4.38it/s]\u001b[A\n","Epoch 0:  93% 21903/23493 [3:44:28<16:17,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  39% 1010/2600 [03:52<06:04,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21905/23493 [3:44:28<16:16,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  39% 1012/2600 [03:52<06:06,  4.33it/s]\u001b[A\n","Epoch 0:  93% 21907/23493 [3:44:29<16:15,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  39% 1014/2600 [03:53<06:03,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21909/23493 [3:44:29<16:13,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  39% 1016/2600 [03:53<06:05,  4.33it/s]\u001b[A\n","Epoch 0:  93% 21911/23493 [3:44:30<16:12,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  39% 1018/2600 [03:54<06:03,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21913/23493 [3:44:30<16:11,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  39% 1020/2600 [03:54<06:06,  4.31it/s]\u001b[A\n","Epoch 0:  93% 21915/23493 [3:44:31<16:09,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  39% 1022/2600 [03:55<06:01,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21917/23493 [3:44:31<16:08,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  39% 1024/2600 [03:55<06:01,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21919/23493 [3:44:32<16:07,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  39% 1026/2600 [03:55<05:58,  4.39it/s]\u001b[A\n","Epoch 0:  93% 21921/23493 [3:44:32<16:06,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  40% 1028/2600 [03:56<06:00,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21923/23493 [3:44:33<16:04,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  40% 1030/2600 [03:56<05:58,  4.38it/s]\u001b[A\n","Epoch 0:  93% 21925/23493 [3:44:33<16:03,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  40% 1032/2600 [03:57<05:58,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21927/23493 [3:44:33<16:02,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  40% 1034/2600 [03:57<05:55,  4.40it/s]\u001b[A\n","Epoch 0:  93% 21929/23493 [3:44:34<16:01,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  40% 1036/2600 [03:58<05:58,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21931/23493 [3:44:34<15:59,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  40% 1038/2600 [03:58<05:56,  4.39it/s]\u001b[A\n","Epoch 0:  93% 21933/23493 [3:44:35<15:58,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  40% 1040/2600 [03:59<05:52,  4.42it/s]\u001b[A\n","Epoch 0:  93% 21935/23493 [3:44:35<15:57,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  40% 1042/2600 [03:59<05:55,  4.38it/s]\u001b[A\n","Epoch 0:  93% 21937/23493 [3:44:36<15:55,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  40% 1044/2600 [04:00<05:57,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21939/23493 [3:44:36<15:54,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  40% 1046/2600 [04:00<05:55,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21941/23493 [3:44:37<15:53,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  40% 1048/2600 [04:00<05:56,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21943/23493 [3:44:37<15:52,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  40% 1050/2600 [04:01<05:54,  4.38it/s]\u001b[A\n","Epoch 0:  93% 21945/23493 [3:44:38<15:50,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  40% 1052/2600 [04:01<05:54,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21947/23493 [3:44:38<15:49,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  41% 1054/2600 [04:02<05:55,  4.35it/s]\u001b[A\n","Epoch 0:  93% 21949/23493 [3:44:38<15:48,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  41% 1056/2600 [04:02<05:55,  4.34it/s]\u001b[A\n","Epoch 0:  93% 21951/23493 [3:44:39<15:46,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  41% 1058/2600 [04:03<05:52,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21953/23493 [3:44:39<15:45,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  41% 1060/2600 [04:03<05:50,  4.39it/s]\u001b[A\n","Epoch 0:  93% 21955/23493 [3:44:40<15:44,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  41% 1062/2600 [04:04<05:51,  4.37it/s]\u001b[A\n","Epoch 0:  93% 21957/23493 [3:44:40<15:43,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  41% 1064/2600 [04:04<05:52,  4.36it/s]\u001b[A\n","Epoch 0:  93% 21959/23493 [3:44:41<15:41,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  41% 1066/2600 [04:05<05:50,  4.38it/s]\u001b[A\n","Epoch 0:  93% 21961/23493 [3:44:41<15:40,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  41% 1068/2600 [04:05<05:47,  4.41it/s]\u001b[A\n","Epoch 0:  93% 21963/23493 [3:44:42<15:39,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  41% 1070/2600 [04:06<05:48,  4.39it/s]\u001b[A\n","Epoch 0:  93% 21965/23493 [3:44:42<15:37,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  41% 1072/2600 [04:06<05:46,  4.42it/s]\u001b[A\n","Epoch 0:  94% 21967/23493 [3:44:43<15:36,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  41% 1074/2600 [04:06<05:48,  4.37it/s]\u001b[A\n","Epoch 0:  94% 21969/23493 [3:44:43<15:35,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  41% 1076/2600 [04:07<05:47,  4.39it/s]\u001b[A\n","Epoch 0:  94% 21971/23493 [3:44:43<15:34,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  41% 1078/2600 [04:07<05:44,  4.42it/s]\u001b[A\n","Epoch 0:  94% 21973/23493 [3:44:44<15:32,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  42% 1080/2600 [04:08<05:46,  4.39it/s]\u001b[A\n","Epoch 0:  94% 21975/23493 [3:44:44<15:31,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  42% 1082/2600 [04:08<05:46,  4.39it/s]\u001b[A\n","Epoch 0:  94% 21977/23493 [3:44:45<15:30,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  42% 1084/2600 [04:09<05:47,  4.36it/s]\u001b[A\n","Epoch 0:  94% 21979/23493 [3:44:45<15:28,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  42% 1086/2600 [04:09<05:48,  4.35it/s]\u001b[A\n","Epoch 0:  94% 21981/23493 [3:44:46<15:27,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  42% 1088/2600 [04:10<05:50,  4.31it/s]\u001b[A\n","Epoch 0:  94% 21983/23493 [3:44:46<15:26,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  42% 1090/2600 [04:10<05:45,  4.37it/s]\u001b[A\n","Epoch 0:  94% 21985/23493 [3:44:47<15:25,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  42% 1092/2600 [04:11<05:44,  4.38it/s]\u001b[A\n","Epoch 0:  94% 21987/23493 [3:44:47<15:23,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  42% 1094/2600 [04:11<05:45,  4.36it/s]\u001b[A\n","Epoch 0:  94% 21989/23493 [3:44:48<15:22,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  42% 1096/2600 [04:11<05:48,  4.32it/s]\u001b[A\n","Epoch 0:  94% 21991/23493 [3:44:48<15:21,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  42% 1098/2600 [04:12<05:45,  4.34it/s]\u001b[A\n","Epoch 0:  94% 21993/23493 [3:44:49<15:20,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  42% 1100/2600 [04:12<05:42,  4.38it/s]\u001b[A\n","Epoch 0:  94% 21995/23493 [3:44:49<15:18,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  42% 1102/2600 [04:13<05:43,  4.36it/s]\u001b[A\n","Epoch 0:  94% 21997/23493 [3:44:49<15:17,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  42% 1104/2600 [04:13<05:45,  4.33it/s]\u001b[A\n","Epoch 0:  94% 21999/23493 [3:44:50<15:16,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  43% 1106/2600 [04:14<05:42,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22001/23493 [3:44:50<15:14,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  43% 1108/2600 [04:14<05:38,  4.41it/s]\u001b[A\n","Epoch 0:  94% 22003/23493 [3:44:51<15:13,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  43% 1110/2600 [04:15<05:40,  4.38it/s]\u001b[A\n","Epoch 0:  94% 22005/23493 [3:44:51<15:12,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  43% 1112/2600 [04:15<05:38,  4.39it/s]\u001b[A\n","Epoch 0:  94% 22007/23493 [3:44:52<15:11,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  43% 1114/2600 [04:16<05:41,  4.35it/s]\u001b[A\n","Epoch 0:  94% 22009/23493 [3:44:52<15:09,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  43% 1116/2600 [04:16<05:35,  4.42it/s]\u001b[A\n","Epoch 0:  94% 22011/23493 [3:44:53<15:08,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  43% 1118/2600 [04:17<05:39,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22013/23493 [3:44:53<15:07,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  43% 1120/2600 [04:17<05:36,  4.40it/s]\u001b[A\n","Epoch 0:  94% 22015/23493 [3:44:54<15:05,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  43% 1122/2600 [04:17<05:38,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22017/23493 [3:44:54<15:04,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  43% 1124/2600 [04:18<05:40,  4.34it/s]\u001b[A\n","Epoch 0:  94% 22019/23493 [3:44:54<15:03,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  43% 1126/2600 [04:18<05:37,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22021/23493 [3:44:55<15:02,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  43% 1128/2600 [04:19<05:37,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22023/23493 [3:44:55<15:00,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  43% 1130/2600 [04:19<05:34,  4.40it/s]\u001b[A\n","Epoch 0:  94% 22025/23493 [3:44:56<14:59,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  44% 1132/2600 [04:20<05:37,  4.35it/s]\u001b[A\n","Epoch 0:  94% 22027/23493 [3:44:56<14:58,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  44% 1134/2600 [04:20<05:33,  4.40it/s]\u001b[A\n","Epoch 0:  94% 22029/23493 [3:44:57<14:56,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  44% 1136/2600 [04:21<05:35,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22031/23493 [3:44:57<14:55,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  44% 1138/2600 [04:21<05:32,  4.39it/s]\u001b[A\n","Epoch 0:  94% 22033/23493 [3:44:58<14:54,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  44% 1140/2600 [04:22<05:33,  4.38it/s]\u001b[A\n","Epoch 0:  94% 22035/23493 [3:44:58<14:53,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  44% 1142/2600 [04:22<05:33,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22037/23493 [3:44:59<14:51,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  44% 1144/2600 [04:22<05:36,  4.33it/s]\u001b[A\n","Epoch 0:  94% 22039/23493 [3:44:59<14:50,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  44% 1146/2600 [04:23<05:37,  4.31it/s]\u001b[A\n","Epoch 0:  94% 22041/23493 [3:45:00<14:49,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  44% 1148/2600 [04:23<05:35,  4.33it/s]\u001b[A\n","Epoch 0:  94% 22043/23493 [3:45:00<14:48,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  44% 1150/2600 [04:24<05:33,  4.35it/s]\u001b[A\n","Epoch 0:  94% 22045/23493 [3:45:00<14:46,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  44% 1152/2600 [04:24<05:33,  4.34it/s]\u001b[A\n","Epoch 0:  94% 22047/23493 [3:45:01<14:45,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  44% 1154/2600 [04:25<05:32,  4.35it/s]\u001b[A\n","Epoch 0:  94% 22049/23493 [3:45:01<14:44,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  44% 1156/2600 [04:25<05:31,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22051/23493 [3:45:02<14:42,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  45% 1158/2600 [04:26<05:32,  4.34it/s]\u001b[A\n","Epoch 0:  94% 22053/23493 [3:45:02<14:41,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  45% 1160/2600 [04:26<05:33,  4.32it/s]\u001b[A\n","Epoch 0:  94% 22055/23493 [3:45:03<14:40,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  45% 1162/2600 [04:27<05:31,  4.34it/s]\u001b[A\n","Epoch 0:  94% 22057/23493 [3:45:03<14:39,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  45% 1164/2600 [04:27<05:30,  4.35it/s]\u001b[A\n","Epoch 0:  94% 22059/23493 [3:45:04<14:37,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  45% 1166/2600 [04:28<05:27,  4.38it/s]\u001b[A\n","Epoch 0:  94% 22061/23493 [3:45:04<14:36,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  45% 1168/2600 [04:28<05:26,  4.39it/s]\u001b[A\n","Epoch 0:  94% 22063/23493 [3:45:05<14:35,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  45% 1170/2600 [04:28<05:30,  4.32it/s]\u001b[A\n","Epoch 0:  94% 22065/23493 [3:45:05<14:34,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  45% 1172/2600 [04:29<05:27,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22067/23493 [3:45:06<14:32,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  45% 1174/2600 [04:29<05:27,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22069/23493 [3:45:06<14:31,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  45% 1176/2600 [04:30<05:26,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22071/23493 [3:45:06<14:30,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  45% 1178/2600 [04:30<05:24,  4.38it/s]\u001b[A\n","Epoch 0:  94% 22073/23493 [3:45:07<14:28,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  45% 1180/2600 [04:31<05:24,  4.38it/s]\u001b[A\n","Epoch 0:  94% 22075/23493 [3:45:07<14:27,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  45% 1182/2600 [04:31<05:24,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22077/23493 [3:45:08<14:26,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  46% 1184/2600 [04:32<05:26,  4.33it/s]\u001b[A\n","Epoch 0:  94% 22079/23493 [3:45:08<14:25,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  46% 1186/2600 [04:32<05:23,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22081/23493 [3:45:09<14:23,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  46% 1188/2600 [04:33<05:23,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22083/23493 [3:45:09<14:22,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  46% 1190/2600 [04:33<05:23,  4.35it/s]\u001b[A\n","Epoch 0:  94% 22085/23493 [3:45:10<14:21,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  46% 1192/2600 [04:33<05:21,  4.38it/s]\u001b[A\n","Epoch 0:  94% 22087/23493 [3:45:10<14:20,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  46% 1194/2600 [04:34<05:22,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22089/23493 [3:45:11<14:18,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  46% 1196/2600 [04:34<05:18,  4.40it/s]\u001b[A\n","Epoch 0:  94% 22091/23493 [3:45:11<14:17,  1.63it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  46% 1198/2600 [04:35<05:18,  4.40it/s]\u001b[A\n","Epoch 0:  94% 22093/23493 [3:45:11<14:16,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  46% 1200/2600 [04:35<05:18,  4.40it/s]\u001b[A\n","Epoch 0:  94% 22095/23493 [3:45:12<14:14,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  46% 1202/2600 [04:36<05:17,  4.40it/s]\u001b[A\n","Epoch 0:  94% 22097/23493 [3:45:12<14:13,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  46% 1204/2600 [04:36<05:16,  4.40it/s]\u001b[A\n","Epoch 0:  94% 22099/23493 [3:45:13<14:12,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  46% 1206/2600 [04:37<05:17,  4.39it/s]\u001b[A\n","Epoch 0:  94% 22101/23493 [3:45:13<14:11,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  46% 1208/2600 [04:37<05:15,  4.42it/s]\u001b[A\n","Epoch 0:  94% 22103/23493 [3:45:14<14:09,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  47% 1210/2600 [04:38<05:17,  4.38it/s]\u001b[A\n","Epoch 0:  94% 22105/23493 [3:45:14<14:08,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  47% 1212/2600 [04:38<05:15,  4.40it/s]\u001b[A\n","Epoch 0:  94% 22107/23493 [3:45:15<14:07,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  47% 1214/2600 [04:39<05:17,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22109/23493 [3:45:15<14:06,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  47% 1216/2600 [04:39<05:15,  4.39it/s]\u001b[A\n","Epoch 0:  94% 22111/23493 [3:45:16<14:04,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  47% 1218/2600 [04:39<05:15,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22113/23493 [3:45:16<14:03,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  47% 1220/2600 [04:40<05:15,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22115/23493 [3:45:16<14:02,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  47% 1222/2600 [04:40<05:12,  4.41it/s]\u001b[A\n","Epoch 0:  94% 22117/23493 [3:45:17<14:00,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  47% 1224/2600 [04:41<05:14,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22119/23493 [3:45:17<13:59,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  47% 1226/2600 [04:41<05:14,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22121/23493 [3:45:18<13:58,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  47% 1228/2600 [04:42<05:15,  4.34it/s]\u001b[A\n","Epoch 0:  94% 22123/23493 [3:45:18<13:57,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  47% 1230/2600 [04:42<05:10,  4.41it/s]\u001b[A\n","Epoch 0:  94% 22125/23493 [3:45:19<13:55,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  47% 1232/2600 [04:43<05:13,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22127/23493 [3:45:19<13:54,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  47% 1234/2600 [04:43<05:12,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22129/23493 [3:45:20<13:53,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  48% 1236/2600 [04:44<05:12,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22131/23493 [3:45:20<13:52,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  48% 1238/2600 [04:44<05:09,  4.40it/s]\u001b[A\n","Epoch 0:  94% 22133/23493 [3:45:21<13:50,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  48% 1240/2600 [04:44<05:11,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22135/23493 [3:45:21<13:49,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  48% 1242/2600 [04:45<05:11,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22137/23493 [3:45:22<13:48,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  48% 1244/2600 [04:45<05:12,  4.34it/s]\u001b[A\n","Epoch 0:  94% 22139/23493 [3:45:22<13:47,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  48% 1246/2600 [04:46<05:12,  4.34it/s]\u001b[A\n","Epoch 0:  94% 22141/23493 [3:45:22<13:45,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  48% 1248/2600 [04:46<05:11,  4.34it/s]\u001b[A\n","Epoch 0:  94% 22143/23493 [3:45:23<13:44,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  48% 1250/2600 [04:47<05:09,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22145/23493 [3:45:23<13:43,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  48% 1252/2600 [04:47<05:08,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22147/23493 [3:45:24<13:41,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  48% 1254/2600 [04:48<05:07,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22149/23493 [3:45:24<13:40,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  48% 1256/2600 [04:48<05:07,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22151/23493 [3:45:25<13:39,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  48% 1258/2600 [04:49<05:03,  4.42it/s]\u001b[A\n","Epoch 0:  94% 22153/23493 [3:45:25<13:38,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  48% 1260/2600 [04:49<05:05,  4.39it/s]\u001b[A\n","Epoch 0:  94% 22155/23493 [3:45:26<13:36,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  49% 1262/2600 [04:49<05:03,  4.41it/s]\u001b[A\n","Epoch 0:  94% 22157/23493 [3:45:26<13:35,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  49% 1264/2600 [04:50<05:08,  4.33it/s]\u001b[A\n","Epoch 0:  94% 22159/23493 [3:45:27<13:34,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  49% 1266/2600 [04:50<05:04,  4.39it/s]\u001b[A\n","Epoch 0:  94% 22161/23493 [3:45:27<13:33,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  49% 1268/2600 [04:51<05:05,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22163/23493 [3:45:27<13:31,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  49% 1270/2600 [04:51<05:01,  4.41it/s]\u001b[A\n","Epoch 0:  94% 22165/23493 [3:45:28<13:30,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  49% 1272/2600 [04:52<05:02,  4.39it/s]\u001b[A\n","Epoch 0:  94% 22167/23493 [3:45:28<13:29,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  49% 1274/2600 [04:52<05:02,  4.38it/s]\u001b[A\n","Epoch 0:  94% 22169/23493 [3:45:29<13:28,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  49% 1276/2600 [04:53<05:02,  4.38it/s]\u001b[A\n","Epoch 0:  94% 22171/23493 [3:45:29<13:26,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  49% 1278/2600 [04:53<05:02,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22173/23493 [3:45:30<13:25,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  49% 1280/2600 [04:54<05:02,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22175/23493 [3:45:30<13:24,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  49% 1282/2600 [04:54<04:59,  4.41it/s]\u001b[A\n","Epoch 0:  94% 22177/23493 [3:45:31<13:22,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  49% 1284/2600 [04:55<04:57,  4.42it/s]\u001b[A\n","Epoch 0:  94% 22179/23493 [3:45:31<13:21,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  49% 1286/2600 [04:55<05:00,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22181/23493 [3:45:32<13:20,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  50% 1288/2600 [04:55<04:58,  4.40it/s]\u001b[A\n","Epoch 0:  94% 22183/23493 [3:45:32<13:19,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  50% 1290/2600 [04:56<05:00,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22185/23493 [3:45:32<13:17,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  50% 1292/2600 [04:56<05:00,  4.35it/s]\u001b[A\n","Epoch 0:  94% 22187/23493 [3:45:33<13:16,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  50% 1294/2600 [04:57<04:57,  4.38it/s]\u001b[A\n","Epoch 0:  94% 22189/23493 [3:45:33<13:15,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  50% 1296/2600 [04:57<04:59,  4.35it/s]\u001b[A\n","Epoch 0:  94% 22191/23493 [3:45:34<13:14,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  50% 1298/2600 [04:58<04:57,  4.38it/s]\u001b[A\n","Epoch 0:  94% 22193/23493 [3:45:34<13:12,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  50% 1300/2600 [04:58<04:57,  4.37it/s]\u001b[A\n","Epoch 0:  94% 22195/23493 [3:45:35<13:11,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  50% 1302/2600 [04:59<04:55,  4.39it/s]\u001b[A\n","Epoch 0:  94% 22197/23493 [3:45:35<13:10,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  50% 1304/2600 [04:59<04:57,  4.36it/s]\u001b[A\n","Epoch 0:  94% 22199/23493 [3:45:36<13:09,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  50% 1306/2600 [05:00<04:53,  4.41it/s]\u001b[A\n","Epoch 0:  95% 22201/23493 [3:45:36<13:07,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  50% 1308/2600 [05:00<04:55,  4.38it/s]\u001b[A\n","Epoch 0:  95% 22203/23493 [3:45:37<13:06,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  50% 1310/2600 [05:00<04:53,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22205/23493 [3:45:37<13:05,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  50% 1312/2600 [05:01<04:55,  4.35it/s]\u001b[A\n","Epoch 0:  95% 22207/23493 [3:45:38<13:03,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  51% 1314/2600 [05:01<04:52,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22209/23493 [3:45:38<13:02,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  51% 1316/2600 [05:02<04:54,  4.36it/s]\u001b[A\n","Epoch 0:  95% 22211/23493 [3:45:38<13:01,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  51% 1318/2600 [05:02<04:52,  4.38it/s]\u001b[A\n","Epoch 0:  95% 22213/23493 [3:45:39<13:00,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  51% 1320/2600 [05:03<04:48,  4.44it/s]\u001b[A\n","Epoch 0:  95% 22215/23493 [3:45:39<12:58,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  51% 1322/2600 [05:03<04:52,  4.38it/s]\u001b[A\n","Epoch 0:  95% 22217/23493 [3:45:40<12:57,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  51% 1324/2600 [05:04<04:48,  4.42it/s]\u001b[A\n","Epoch 0:  95% 22219/23493 [3:45:40<12:56,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  51% 1326/2600 [05:04<04:50,  4.38it/s]\u001b[A\n","Epoch 0:  95% 22221/23493 [3:45:41<12:55,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  51% 1328/2600 [05:05<04:51,  4.37it/s]\u001b[A\n","Epoch 0:  95% 22223/23493 [3:45:41<12:53,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  51% 1330/2600 [05:05<04:53,  4.33it/s]\u001b[A\n","Epoch 0:  95% 22225/23493 [3:45:42<12:52,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  51% 1332/2600 [05:05<04:49,  4.37it/s]\u001b[A\n","Epoch 0:  95% 22227/23493 [3:45:42<12:51,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  51% 1334/2600 [05:06<04:49,  4.37it/s]\u001b[A\n","Epoch 0:  95% 22229/23493 [3:45:43<12:50,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  51% 1336/2600 [05:06<04:48,  4.38it/s]\u001b[A\n","Epoch 0:  95% 22231/23493 [3:45:43<12:48,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  51% 1338/2600 [05:07<04:47,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22233/23493 [3:45:43<12:47,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  52% 1340/2600 [05:07<04:46,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22235/23493 [3:45:44<12:46,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  52% 1342/2600 [05:08<04:43,  4.44it/s]\u001b[A\n","Epoch 0:  95% 22237/23493 [3:45:44<12:45,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  52% 1344/2600 [05:08<04:47,  4.37it/s]\u001b[A\n","Epoch 0:  95% 22239/23493 [3:45:45<12:43,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  52% 1346/2600 [05:09<04:44,  4.41it/s]\u001b[A\n","Epoch 0:  95% 22241/23493 [3:45:45<12:42,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  52% 1348/2600 [05:09<04:46,  4.37it/s]\u001b[A\n","Epoch 0:  95% 22243/23493 [3:45:46<12:41,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  52% 1350/2600 [05:10<04:44,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22245/23493 [3:45:46<12:40,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  52% 1352/2600 [05:10<04:47,  4.35it/s]\u001b[A\n","Epoch 0:  95% 22247/23493 [3:45:47<12:38,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  52% 1354/2600 [05:10<04:42,  4.42it/s]\u001b[A\n","Epoch 0:  95% 22249/23493 [3:45:47<12:37,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  52% 1356/2600 [05:11<04:45,  4.36it/s]\u001b[A\n","Epoch 0:  95% 22251/23493 [3:45:48<12:36,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  52% 1358/2600 [05:11<04:42,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22253/23493 [3:45:48<12:34,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  52% 1360/2600 [05:12<04:40,  4.42it/s]\u001b[A\n","Epoch 0:  95% 22255/23493 [3:45:48<12:33,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  52% 1362/2600 [05:12<04:43,  4.37it/s]\u001b[A\n","Epoch 0:  95% 22257/23493 [3:45:49<12:32,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  52% 1364/2600 [05:13<04:44,  4.35it/s]\u001b[A\n","Epoch 0:  95% 22259/23493 [3:45:49<12:31,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  53% 1366/2600 [05:13<04:42,  4.37it/s]\u001b[A\n","Epoch 0:  95% 22261/23493 [3:45:50<12:29,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  53% 1368/2600 [05:14<04:42,  4.36it/s]\u001b[A\n","Epoch 0:  95% 22263/23493 [3:45:50<12:28,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  53% 1370/2600 [05:14<04:43,  4.34it/s]\u001b[A\n","Epoch 0:  95% 22265/23493 [3:45:51<12:27,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  53% 1372/2600 [05:15<04:41,  4.36it/s]\u001b[A\n","Epoch 0:  95% 22267/23493 [3:45:51<12:26,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  53% 1374/2600 [05:15<04:41,  4.35it/s]\u001b[A\n","Epoch 0:  95% 22269/23493 [3:45:52<12:24,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  53% 1376/2600 [05:16<04:39,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22271/23493 [3:45:52<12:23,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  53% 1378/2600 [05:16<04:37,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22273/23493 [3:45:53<12:22,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  53% 1380/2600 [05:16<04:39,  4.37it/s]\u001b[A\n","Epoch 0:  95% 22275/23493 [3:45:53<12:21,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  53% 1382/2600 [05:17<04:36,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22277/23493 [3:45:53<12:19,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  53% 1384/2600 [05:17<04:37,  4.38it/s]\u001b[A\n","Epoch 0:  95% 22279/23493 [3:45:54<12:18,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  53% 1386/2600 [05:18<04:36,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22281/23493 [3:45:54<12:17,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  53% 1388/2600 [05:18<04:38,  4.35it/s]\u001b[A\n","Epoch 0:  95% 22283/23493 [3:45:55<12:16,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  53% 1390/2600 [05:19<04:35,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22285/23493 [3:45:55<12:14,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  54% 1392/2600 [05:19<04:34,  4.41it/s]\u001b[A\n","Epoch 0:  95% 22287/23493 [3:45:56<12:13,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  54% 1394/2600 [05:20<04:37,  4.34it/s]\u001b[A\n","Epoch 0:  95% 22289/23493 [3:45:56<12:12,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  54% 1396/2600 [05:20<04:31,  4.43it/s]\u001b[A\n","Epoch 0:  95% 22291/23493 [3:45:57<12:11,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  54% 1398/2600 [05:21<04:34,  4.38it/s]\u001b[A\n","Epoch 0:  95% 22293/23493 [3:45:57<12:09,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  54% 1400/2600 [05:21<04:33,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22295/23493 [3:45:58<12:08,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  54% 1402/2600 [05:21<04:31,  4.42it/s]\u001b[A\n","Epoch 0:  95% 22297/23493 [3:45:58<12:07,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  54% 1404/2600 [05:22<04:31,  4.41it/s]\u001b[A\n","Epoch 0:  95% 22299/23493 [3:45:58<12:06,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  54% 1406/2600 [05:22<04:31,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22301/23493 [3:45:59<12:04,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  54% 1408/2600 [05:23<04:34,  4.35it/s]\u001b[A\n","Epoch 0:  95% 22303/23493 [3:45:59<12:03,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  54% 1410/2600 [05:23<04:33,  4.35it/s]\u001b[A\n","Epoch 0:  95% 22305/23493 [3:46:00<12:02,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  54% 1412/2600 [05:24<04:32,  4.36it/s]\u001b[A\n","Epoch 0:  95% 22307/23493 [3:46:00<12:00,  1.64it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  54% 1414/2600 [05:24<04:30,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22309/23493 [3:46:01<11:59,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  54% 1416/2600 [05:25<04:29,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22311/23493 [3:46:01<11:58,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  55% 1418/2600 [05:25<04:27,  4.42it/s]\u001b[A\n","Epoch 0:  95% 22313/23493 [3:46:02<11:57,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  55% 1420/2600 [05:26<04:28,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22315/23493 [3:46:02<11:55,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  55% 1422/2600 [05:26<04:26,  4.41it/s]\u001b[A\n","Epoch 0:  95% 22317/23493 [3:46:03<11:54,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  55% 1424/2600 [05:26<04:25,  4.43it/s]\u001b[A\n","Epoch 0:  95% 22319/23493 [3:46:03<11:53,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  55% 1426/2600 [05:27<04:27,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22321/23493 [3:46:04<11:52,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  55% 1428/2600 [05:27<04:26,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22323/23493 [3:46:04<11:50,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  55% 1430/2600 [05:28<04:25,  4.41it/s]\u001b[A\n","Epoch 0:  95% 22325/23493 [3:46:04<11:49,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  55% 1432/2600 [05:28<04:26,  4.38it/s]\u001b[A\n","Epoch 0:  95% 22327/23493 [3:46:05<11:48,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  55% 1434/2600 [05:29<04:25,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22329/23493 [3:46:05<11:47,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  55% 1436/2600 [05:29<04:26,  4.37it/s]\u001b[A\n","Epoch 0:  95% 22331/23493 [3:46:06<11:45,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  55% 1438/2600 [05:30<04:21,  4.45it/s]\u001b[A\n","Epoch 0:  95% 22333/23493 [3:46:06<11:44,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  55% 1440/2600 [05:30<04:22,  4.43it/s]\u001b[A\n","Epoch 0:  95% 22335/23493 [3:46:07<11:43,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  55% 1442/2600 [05:31<04:22,  4.41it/s]\u001b[A\n","Epoch 0:  95% 22337/23493 [3:46:07<11:42,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  56% 1444/2600 [05:31<04:25,  4.35it/s]\u001b[A\n","Epoch 0:  95% 22339/23493 [3:46:08<11:40,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  56% 1446/2600 [05:31<04:22,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22341/23493 [3:46:08<11:39,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  56% 1448/2600 [05:32<04:22,  4.38it/s]\u001b[A\n","Epoch 0:  95% 22343/23493 [3:46:09<11:38,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  56% 1450/2600 [05:32<04:21,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22345/23493 [3:46:09<11:37,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  56% 1452/2600 [05:33<04:21,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22347/23493 [3:46:09<11:35,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  56% 1454/2600 [05:33<04:23,  4.36it/s]\u001b[A\n","Epoch 0:  95% 22349/23493 [3:46:10<11:34,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  56% 1456/2600 [05:34<04:24,  4.33it/s]\u001b[A\n","Epoch 0:  95% 22351/23493 [3:46:10<11:33,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  56% 1458/2600 [05:34<04:20,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22353/23493 [3:46:11<11:32,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  56% 1460/2600 [05:35<04:17,  4.42it/s]\u001b[A\n","Epoch 0:  95% 22355/23493 [3:46:11<11:30,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  56% 1462/2600 [05:35<04:19,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22357/23493 [3:46:12<11:29,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  56% 1464/2600 [05:36<04:20,  4.35it/s]\u001b[A\n","Epoch 0:  95% 22359/23493 [3:46:12<11:28,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  56% 1466/2600 [05:36<04:19,  4.37it/s]\u001b[A\n","Epoch 0:  95% 22361/23493 [3:46:13<11:27,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  56% 1468/2600 [05:36<04:17,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22363/23493 [3:46:13<11:25,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  57% 1470/2600 [05:37<04:14,  4.43it/s]\u001b[A\n","Epoch 0:  95% 22365/23493 [3:46:14<11:24,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  57% 1472/2600 [05:37<04:17,  4.38it/s]\u001b[A\n","Epoch 0:  95% 22367/23493 [3:46:14<11:23,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  57% 1474/2600 [05:38<04:16,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22369/23493 [3:46:14<11:22,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  57% 1476/2600 [05:38<04:14,  4.42it/s]\u001b[A\n","Epoch 0:  95% 22371/23493 [3:46:15<11:20,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  57% 1478/2600 [05:39<04:17,  4.36it/s]\u001b[A\n","Epoch 0:  95% 22373/23493 [3:46:15<11:19,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  57% 1480/2600 [05:39<04:14,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22375/23493 [3:46:16<11:18,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  57% 1482/2600 [05:40<04:15,  4.37it/s]\u001b[A\n","Epoch 0:  95% 22377/23493 [3:46:16<11:17,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  57% 1484/2600 [05:40<04:13,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22379/23493 [3:46:17<11:15,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  57% 1486/2600 [05:41<04:15,  4.36it/s]\u001b[A\n","Epoch 0:  95% 22381/23493 [3:46:17<11:14,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  57% 1488/2600 [05:41<04:12,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22383/23493 [3:46:18<11:13,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  57% 1490/2600 [05:41<04:10,  4.43it/s]\u001b[A\n","Epoch 0:  95% 22385/23493 [3:46:18<11:12,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  57% 1492/2600 [05:42<04:09,  4.45it/s]\u001b[A\n","Epoch 0:  95% 22387/23493 [3:46:19<11:10,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  57% 1494/2600 [05:42<04:09,  4.43it/s]\u001b[A\n","Epoch 0:  95% 22389/23493 [3:46:19<11:09,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  58% 1496/2600 [05:43<04:11,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22391/23493 [3:46:19<11:08,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  58% 1498/2600 [05:43<04:11,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22393/23493 [3:46:20<11:07,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  58% 1500/2600 [05:44<04:10,  4.38it/s]\u001b[A\n","Epoch 0:  95% 22395/23493 [3:46:20<11:05,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  58% 1502/2600 [05:44<04:09,  4.41it/s]\u001b[A\n","Epoch 0:  95% 22397/23493 [3:46:21<11:04,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  58% 1504/2600 [05:45<04:11,  4.36it/s]\u001b[A\n","Epoch 0:  95% 22399/23493 [3:46:21<11:03,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  58% 1506/2600 [05:45<04:10,  4.37it/s]\u001b[A\n","Epoch 0:  95% 22401/23493 [3:46:22<11:02,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  58% 1508/2600 [05:46<04:06,  4.42it/s]\u001b[A\n","Epoch 0:  95% 22403/23493 [3:46:22<11:00,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  58% 1510/2600 [05:46<04:09,  4.37it/s]\u001b[A\n","Epoch 0:  95% 22405/23493 [3:46:23<10:59,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  58% 1512/2600 [05:47<04:10,  4.34it/s]\u001b[A\n","Epoch 0:  95% 22407/23493 [3:46:23<10:58,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  58% 1514/2600 [05:47<04:09,  4.36it/s]\u001b[A\n","Epoch 0:  95% 22409/23493 [3:46:24<10:57,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  58% 1516/2600 [05:47<04:06,  4.39it/s]\u001b[A\n","Epoch 0:  95% 22411/23493 [3:46:24<10:55,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  58% 1518/2600 [05:48<04:07,  4.37it/s]\u001b[A\n","Epoch 0:  95% 22413/23493 [3:46:24<10:54,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  58% 1520/2600 [05:48<04:07,  4.36it/s]\u001b[A\n","Epoch 0:  95% 22415/23493 [3:46:25<10:53,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  59% 1522/2600 [05:49<04:07,  4.35it/s]\u001b[A\n","Epoch 0:  95% 22417/23493 [3:46:25<10:52,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  59% 1524/2600 [05:49<04:06,  4.36it/s]\u001b[A\n","Epoch 0:  95% 22419/23493 [3:46:26<10:50,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  59% 1526/2600 [05:50<04:08,  4.32it/s]\u001b[A\n","Epoch 0:  95% 22421/23493 [3:46:26<10:49,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  59% 1528/2600 [05:50<04:03,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22423/23493 [3:46:27<10:48,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  59% 1530/2600 [05:51<04:05,  4.35it/s]\u001b[A\n","Epoch 0:  95% 22425/23493 [3:46:27<10:47,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  59% 1532/2600 [05:51<04:05,  4.36it/s]\u001b[A\n","Epoch 0:  95% 22427/23493 [3:46:28<10:45,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  59% 1534/2600 [05:52<04:04,  4.37it/s]\u001b[A\n","Epoch 0:  95% 22429/23493 [3:46:28<10:44,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  59% 1536/2600 [05:52<04:01,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22431/23493 [3:46:29<10:43,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  59% 1538/2600 [05:52<04:03,  4.36it/s]\u001b[A\n","Epoch 0:  95% 22433/23493 [3:46:29<10:42,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  59% 1540/2600 [05:53<04:00,  4.40it/s]\u001b[A\n","Epoch 0:  95% 22435/23493 [3:46:29<10:40,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  59% 1542/2600 [05:53<03:58,  4.44it/s]\u001b[A\n","Epoch 0:  96% 22437/23493 [3:46:30<10:39,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  59% 1544/2600 [05:54<04:00,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22439/23493 [3:46:30<10:38,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  59% 1546/2600 [05:54<04:00,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22441/23493 [3:46:31<10:37,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  60% 1548/2600 [05:55<04:01,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22443/23493 [3:46:31<10:35,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  60% 1550/2600 [05:55<03:58,  4.40it/s]\u001b[A\n","Epoch 0:  96% 22445/23493 [3:46:32<10:34,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  60% 1552/2600 [05:56<04:01,  4.34it/s]\u001b[A\n","Epoch 0:  96% 22447/23493 [3:46:32<10:33,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  60% 1554/2600 [05:56<03:59,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22449/23493 [3:46:33<10:32,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  60% 1556/2600 [05:57<03:59,  4.35it/s]\u001b[A\n","Epoch 0:  96% 22451/23493 [3:46:33<10:30,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  60% 1558/2600 [05:57<03:59,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22453/23493 [3:46:34<10:29,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  60% 1560/2600 [05:57<03:57,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22455/23493 [3:46:34<10:28,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  60% 1562/2600 [05:58<03:59,  4.34it/s]\u001b[A\n","Epoch 0:  96% 22457/23493 [3:46:35<10:27,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  60% 1564/2600 [05:58<03:58,  4.35it/s]\u001b[A\n","Epoch 0:  96% 22459/23493 [3:46:35<10:25,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  60% 1566/2600 [05:59<03:57,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22461/23493 [3:46:35<10:24,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  60% 1568/2600 [05:59<03:56,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22463/23493 [3:46:36<10:23,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  60% 1570/2600 [06:00<03:53,  4.40it/s]\u001b[A\n","Epoch 0:  96% 22465/23493 [3:46:36<10:22,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  60% 1572/2600 [06:00<03:53,  4.41it/s]\u001b[A\n","Epoch 0:  96% 22467/23493 [3:46:37<10:20,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  61% 1574/2600 [06:01<03:54,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22469/23493 [3:46:37<10:19,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  61% 1576/2600 [06:01<03:54,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22471/23493 [3:46:38<10:18,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  61% 1578/2600 [06:02<03:55,  4.35it/s]\u001b[A\n","Epoch 0:  96% 22473/23493 [3:46:38<10:17,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  61% 1580/2600 [06:02<03:53,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22475/23493 [3:46:39<10:15,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  61% 1582/2600 [06:03<03:51,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22477/23493 [3:46:39<10:14,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  61% 1584/2600 [06:03<03:50,  4.41it/s]\u001b[A\n","Epoch 0:  96% 22479/23493 [3:46:40<10:13,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  61% 1586/2600 [06:03<03:51,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22481/23493 [3:46:40<10:12,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  61% 1588/2600 [06:04<03:54,  4.32it/s]\u001b[A\n","Epoch 0:  96% 22483/23493 [3:46:40<10:10,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  61% 1590/2600 [06:04<03:50,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22485/23493 [3:46:41<10:09,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  61% 1592/2600 [06:05<03:51,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22487/23493 [3:46:41<10:08,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  61% 1594/2600 [06:05<03:49,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22489/23493 [3:46:42<10:07,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  61% 1596/2600 [06:06<03:50,  4.35it/s]\u001b[A\n","Epoch 0:  96% 22491/23493 [3:46:42<10:06,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  61% 1598/2600 [06:06<03:50,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22493/23493 [3:46:43<10:04,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  62% 1600/2600 [06:07<03:48,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22495/23493 [3:46:43<10:03,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  62% 1602/2600 [06:07<03:47,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22497/23493 [3:46:44<10:02,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  62% 1604/2600 [06:08<03:46,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22499/23493 [3:46:44<10:01,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  62% 1606/2600 [06:08<03:46,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22501/23493 [3:46:45<09:59,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  62% 1608/2600 [06:08<03:48,  4.34it/s]\u001b[A\n","Epoch 0:  96% 22503/23493 [3:46:45<09:58,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  62% 1610/2600 [06:09<03:47,  4.35it/s]\u001b[A\n","Epoch 0:  96% 22505/23493 [3:46:46<09:57,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  62% 1612/2600 [06:09<03:44,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22507/23493 [3:46:46<09:56,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  62% 1614/2600 [06:10<03:45,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22509/23493 [3:46:46<09:54,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  62% 1616/2600 [06:10<03:44,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22511/23493 [3:46:47<09:53,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  62% 1618/2600 [06:11<03:46,  4.34it/s]\u001b[A\n","Epoch 0:  96% 22513/23493 [3:46:47<09:52,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  62% 1620/2600 [06:11<03:44,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22515/23493 [3:46:48<09:51,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  62% 1622/2600 [06:12<03:43,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22517/23493 [3:46:48<09:49,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  62% 1624/2600 [06:12<03:44,  4.35it/s]\u001b[A\n","Epoch 0:  96% 22519/23493 [3:46:49<09:48,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  63% 1626/2600 [06:13<03:41,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22521/23493 [3:46:49<09:47,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  63% 1628/2600 [06:13<03:43,  4.34it/s]\u001b[A\n","Epoch 0:  96% 22523/23493 [3:46:50<09:46,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  63% 1630/2600 [06:13<03:41,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22525/23493 [3:46:50<09:44,  1.65it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  63% 1632/2600 [06:14<03:43,  4.33it/s]\u001b[A\n","Epoch 0:  96% 22527/23493 [3:46:51<09:43,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  63% 1634/2600 [06:14<03:39,  4.41it/s]\u001b[A\n","Epoch 0:  96% 22529/23493 [3:46:51<09:42,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  63% 1636/2600 [06:15<03:39,  4.40it/s]\u001b[A\n","Epoch 0:  96% 22531/23493 [3:46:51<09:41,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  63% 1638/2600 [06:15<03:39,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22533/23493 [3:46:52<09:39,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  63% 1640/2600 [06:16<03:37,  4.41it/s]\u001b[A\n","Epoch 0:  96% 22535/23493 [3:46:52<09:38,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  63% 1642/2600 [06:16<03:38,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22537/23493 [3:46:53<09:37,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  63% 1644/2600 [06:17<03:39,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22539/23493 [3:46:53<09:36,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  63% 1646/2600 [06:17<03:37,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22541/23493 [3:46:54<09:34,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  63% 1648/2600 [06:18<03:37,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22543/23493 [3:46:54<09:33,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  63% 1650/2600 [06:18<03:36,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22545/23493 [3:46:55<09:32,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  64% 1652/2600 [06:19<03:36,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22547/23493 [3:46:55<09:31,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  64% 1654/2600 [06:19<03:35,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22549/23493 [3:46:56<09:30,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  64% 1656/2600 [06:19<03:37,  4.35it/s]\u001b[A\n","Epoch 0:  96% 22551/23493 [3:46:56<09:28,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  64% 1658/2600 [06:20<03:34,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22553/23493 [3:46:56<09:27,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  64% 1660/2600 [06:20<03:34,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22555/23493 [3:46:57<09:26,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  64% 1662/2600 [06:21<03:33,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22557/23493 [3:46:57<09:25,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  64% 1664/2600 [06:21<03:34,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22559/23493 [3:46:58<09:23,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  64% 1666/2600 [06:22<03:34,  4.35it/s]\u001b[A\n","Epoch 0:  96% 22561/23493 [3:46:58<09:22,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  64% 1668/2600 [06:22<03:33,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22563/23493 [3:46:59<09:21,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  64% 1670/2600 [06:23<03:31,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22565/23493 [3:46:59<09:20,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  64% 1672/2600 [06:23<03:30,  4.40it/s]\u001b[A\n","Epoch 0:  96% 22567/23493 [3:47:00<09:18,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  64% 1674/2600 [06:24<03:30,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22569/23493 [3:47:00<09:17,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  64% 1676/2600 [06:24<03:31,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22571/23493 [3:47:01<09:16,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  65% 1678/2600 [06:24<03:29,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22573/23493 [3:47:01<09:15,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  65% 1680/2600 [06:25<03:29,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22575/23493 [3:47:02<09:13,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  65% 1682/2600 [06:25<03:30,  4.35it/s]\u001b[A\n","Epoch 0:  96% 22577/23493 [3:47:02<09:12,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  65% 1684/2600 [06:26<03:29,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22579/23493 [3:47:02<09:11,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  65% 1686/2600 [06:26<03:28,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22581/23493 [3:47:03<09:10,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  65% 1688/2600 [06:27<03:28,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22583/23493 [3:47:03<09:08,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  65% 1690/2600 [06:27<03:28,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22585/23493 [3:47:04<09:07,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  65% 1692/2600 [06:28<03:27,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22587/23493 [3:47:04<09:06,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  65% 1694/2600 [06:28<03:25,  4.40it/s]\u001b[A\n","Epoch 0:  96% 22589/23493 [3:47:05<09:05,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  65% 1696/2600 [06:29<03:27,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22591/23493 [3:47:05<09:04,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  65% 1698/2600 [06:29<03:26,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22593/23493 [3:47:06<09:02,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  65% 1700/2600 [06:29<03:25,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22595/23493 [3:47:06<09:01,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  65% 1702/2600 [06:30<03:25,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22597/23493 [3:47:07<09:00,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  66% 1704/2600 [06:30<03:24,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22599/23493 [3:47:07<08:59,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  66% 1706/2600 [06:31<03:25,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22601/23493 [3:47:07<08:57,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  66% 1708/2600 [06:31<03:23,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22603/23493 [3:47:08<08:56,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  66% 1710/2600 [06:32<03:21,  4.42it/s]\u001b[A\n","Epoch 0:  96% 22605/23493 [3:47:08<08:55,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  66% 1712/2600 [06:32<03:22,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22607/23493 [3:47:09<08:54,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  66% 1714/2600 [06:33<03:21,  4.40it/s]\u001b[A\n","Epoch 0:  96% 22609/23493 [3:47:09<08:52,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  66% 1716/2600 [06:33<03:21,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22611/23493 [3:47:10<08:51,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  66% 1718/2600 [06:34<03:20,  4.40it/s]\u001b[A\n","Epoch 0:  96% 22613/23493 [3:47:10<08:50,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  66% 1720/2600 [06:34<03:18,  4.42it/s]\u001b[A\n","Epoch 0:  96% 22615/23493 [3:47:11<08:49,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  66% 1722/2600 [06:35<03:19,  4.40it/s]\u001b[A\n","Epoch 0:  96% 22617/23493 [3:47:11<08:47,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  66% 1724/2600 [06:35<03:20,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22619/23493 [3:47:12<08:46,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  66% 1726/2600 [06:35<03:19,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22621/23493 [3:47:12<08:45,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  66% 1728/2600 [06:36<03:18,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22623/23493 [3:47:12<08:44,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  67% 1730/2600 [06:36<03:17,  4.41it/s]\u001b[A\n","Epoch 0:  96% 22625/23493 [3:47:13<08:43,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  67% 1732/2600 [06:37<03:17,  4.40it/s]\u001b[A\n","Epoch 0:  96% 22627/23493 [3:47:13<08:41,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  67% 1734/2600 [06:37<03:17,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22629/23493 [3:47:14<08:40,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  67% 1736/2600 [06:38<03:17,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22631/23493 [3:47:14<08:39,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  67% 1738/2600 [06:38<03:15,  4.40it/s]\u001b[A\n","Epoch 0:  96% 22633/23493 [3:47:15<08:38,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  67% 1740/2600 [06:39<03:16,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22635/23493 [3:47:15<08:36,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  67% 1742/2600 [06:39<03:15,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22637/23493 [3:47:16<08:35,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  67% 1744/2600 [06:40<03:13,  4.43it/s]\u001b[A\n","Epoch 0:  96% 22639/23493 [3:47:16<08:34,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  67% 1746/2600 [06:40<03:14,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22641/23493 [3:47:17<08:33,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  67% 1748/2600 [06:40<03:14,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22643/23493 [3:47:17<08:31,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  67% 1750/2600 [06:41<03:16,  4.32it/s]\u001b[A\n","Epoch 0:  96% 22645/23493 [3:47:17<08:30,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  67% 1752/2600 [06:41<03:15,  4.35it/s]\u001b[A\n","Epoch 0:  96% 22647/23493 [3:47:18<08:29,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  67% 1754/2600 [06:42<03:13,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22649/23493 [3:47:18<08:28,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  68% 1756/2600 [06:42<03:13,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22651/23493 [3:47:19<08:27,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  68% 1758/2600 [06:43<03:12,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22653/23493 [3:47:19<08:25,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  68% 1760/2600 [06:43<03:12,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22655/23493 [3:47:20<08:24,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  68% 1762/2600 [06:44<03:12,  4.36it/s]\u001b[A\n","Epoch 0:  96% 22657/23493 [3:47:20<08:23,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  68% 1764/2600 [06:44<03:12,  4.35it/s]\u001b[A\n","Epoch 0:  96% 22659/23493 [3:47:21<08:22,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  68% 1766/2600 [06:45<03:09,  4.39it/s]\u001b[A\n","Epoch 0:  96% 22661/23493 [3:47:21<08:20,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  68% 1768/2600 [06:45<03:10,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22663/23493 [3:47:22<08:19,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  68% 1770/2600 [06:45<03:09,  4.37it/s]\u001b[A\n","Epoch 0:  96% 22665/23493 [3:47:22<08:18,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  68% 1772/2600 [06:46<03:08,  4.38it/s]\u001b[A\n","Epoch 0:  96% 22667/23493 [3:47:23<08:17,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  68% 1774/2600 [06:46<03:10,  4.35it/s]\u001b[A\n","Epoch 0:  96% 22669/23493 [3:47:23<08:15,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  68% 1776/2600 [06:47<03:06,  4.41it/s]\u001b[A\n","Epoch 0:  97% 22671/23493 [3:47:23<08:14,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  68% 1778/2600 [06:47<03:07,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22673/23493 [3:47:24<08:13,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  68% 1780/2600 [06:48<03:05,  4.41it/s]\u001b[A\n","Epoch 0:  97% 22675/23493 [3:47:24<08:12,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  69% 1782/2600 [06:48<03:05,  4.40it/s]\u001b[A\n","Epoch 0:  97% 22677/23493 [3:47:25<08:11,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  69% 1784/2600 [06:49<03:06,  4.37it/s]\u001b[A\n","Epoch 0:  97% 22679/23493 [3:47:25<08:09,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  69% 1786/2600 [06:49<03:07,  4.34it/s]\u001b[A\n","Epoch 0:  97% 22681/23493 [3:47:26<08:08,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  69% 1788/2600 [06:50<03:06,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22683/23493 [3:47:26<08:07,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  69% 1790/2600 [06:50<03:06,  4.35it/s]\u001b[A\n","Epoch 0:  97% 22685/23493 [3:47:27<08:06,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  69% 1792/2600 [06:50<03:05,  4.35it/s]\u001b[A\n","Epoch 0:  97% 22687/23493 [3:47:27<08:04,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  69% 1794/2600 [06:51<03:04,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22689/23493 [3:47:28<08:03,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  69% 1796/2600 [06:51<03:05,  4.35it/s]\u001b[A\n","Epoch 0:  97% 22691/23493 [3:47:28<08:02,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  69% 1798/2600 [06:52<03:04,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22693/23493 [3:47:28<08:01,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  69% 1800/2600 [06:52<03:02,  4.39it/s]\u001b[A\n","Epoch 0:  97% 22695/23493 [3:47:29<07:59,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  69% 1802/2600 [06:53<03:02,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22697/23493 [3:47:29<07:58,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  69% 1804/2600 [06:53<03:00,  4.40it/s]\u001b[A\n","Epoch 0:  97% 22699/23493 [3:47:30<07:57,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  69% 1806/2600 [06:54<03:00,  4.40it/s]\u001b[A\n","Epoch 0:  97% 22701/23493 [3:47:30<07:56,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  70% 1808/2600 [06:54<03:01,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22703/23493 [3:47:31<07:55,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  70% 1810/2600 [06:55<03:01,  4.35it/s]\u001b[A\n","Epoch 0:  97% 22705/23493 [3:47:31<07:53,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  70% 1812/2600 [06:55<02:58,  4.40it/s]\u001b[A\n","Epoch 0:  97% 22707/23493 [3:47:32<07:52,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  70% 1814/2600 [06:56<02:58,  4.42it/s]\u001b[A\n","Epoch 0:  97% 22709/23493 [3:47:32<07:51,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  70% 1816/2600 [06:56<02:59,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22711/23493 [3:47:33<07:50,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  70% 1818/2600 [06:56<02:57,  4.40it/s]\u001b[A\n","Epoch 0:  97% 22713/23493 [3:47:33<07:48,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  70% 1820/2600 [06:57<02:59,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22715/23493 [3:47:33<07:47,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  70% 1822/2600 [06:57<02:57,  4.39it/s]\u001b[A\n","Epoch 0:  97% 22717/23493 [3:47:34<07:46,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  70% 1824/2600 [06:58<02:57,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22719/23493 [3:47:34<07:45,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  70% 1826/2600 [06:58<02:58,  4.33it/s]\u001b[A\n","Epoch 0:  97% 22721/23493 [3:47:35<07:43,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  70% 1828/2600 [06:59<02:56,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22723/23493 [3:47:35<07:42,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  70% 1830/2600 [06:59<02:55,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22725/23493 [3:47:36<07:41,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  70% 1832/2600 [07:00<02:55,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22727/23493 [3:47:36<07:40,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  71% 1834/2600 [07:00<02:54,  4.39it/s]\u001b[A\n","Epoch 0:  97% 22729/23493 [3:47:37<07:39,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  71% 1836/2600 [07:01<02:54,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22731/23493 [3:47:37<07:37,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  71% 1838/2600 [07:01<02:54,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22733/23493 [3:47:38<07:36,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  71% 1840/2600 [07:01<02:53,  4.37it/s]\u001b[A\n","Epoch 0:  97% 22735/23493 [3:47:38<07:35,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  71% 1842/2600 [07:02<02:51,  4.41it/s]\u001b[A\n","Epoch 0:  97% 22737/23493 [3:47:39<07:34,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  71% 1844/2600 [07:02<02:53,  4.37it/s]\u001b[A\n","Epoch 0:  97% 22739/23493 [3:47:39<07:32,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  71% 1846/2600 [07:03<02:52,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22741/23493 [3:47:39<07:31,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  71% 1848/2600 [07:03<02:52,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22743/23493 [3:47:40<07:30,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  71% 1850/2600 [07:04<02:50,  4.40it/s]\u001b[A\n","Epoch 0:  97% 22745/23493 [3:47:40<07:29,  1.66it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  71% 1852/2600 [07:04<02:50,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22747/23493 [3:47:41<07:28,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  71% 1854/2600 [07:05<02:49,  4.39it/s]\u001b[A\n","Epoch 0:  97% 22749/23493 [3:47:41<07:26,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  71% 1856/2600 [07:05<02:50,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22751/23493 [3:47:42<07:25,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  71% 1858/2600 [07:06<02:48,  4.41it/s]\u001b[A\n","Epoch 0:  97% 22753/23493 [3:47:42<07:24,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  72% 1860/2600 [07:06<02:48,  4.39it/s]\u001b[A\n","Epoch 0:  97% 22755/23493 [3:47:43<07:23,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  72% 1862/2600 [07:06<02:47,  4.40it/s]\u001b[A\n","Epoch 0:  97% 22757/23493 [3:47:43<07:21,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  72% 1864/2600 [07:07<02:46,  4.42it/s]\u001b[A\n","Epoch 0:  97% 22759/23493 [3:47:44<07:20,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  72% 1866/2600 [07:07<02:47,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22761/23493 [3:47:44<07:19,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  72% 1868/2600 [07:08<02:46,  4.39it/s]\u001b[A\n","Epoch 0:  97% 22763/23493 [3:47:44<07:18,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  72% 1870/2600 [07:08<02:48,  4.34it/s]\u001b[A\n","Epoch 0:  97% 22765/23493 [3:47:45<07:17,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  72% 1872/2600 [07:09<02:46,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22767/23493 [3:47:45<07:15,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  72% 1874/2600 [07:09<02:46,  4.35it/s]\u001b[A\n","Epoch 0:  97% 22769/23493 [3:47:46<07:14,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  72% 1876/2600 [07:10<02:45,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22771/23493 [3:47:46<07:13,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  72% 1878/2600 [07:10<02:45,  4.37it/s]\u001b[A\n","Epoch 0:  97% 22773/23493 [3:47:47<07:12,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  72% 1880/2600 [07:11<02:43,  4.40it/s]\u001b[A\n","Epoch 0:  97% 22775/23493 [3:47:47<07:10,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  72% 1882/2600 [07:11<02:44,  4.37it/s]\u001b[A\n","Epoch 0:  97% 22777/23493 [3:47:48<07:09,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  72% 1884/2600 [07:12<02:43,  4.37it/s]\u001b[A\n","Epoch 0:  97% 22779/23493 [3:47:48<07:08,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  73% 1886/2600 [07:12<02:43,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22781/23493 [3:47:49<07:07,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  73% 1888/2600 [07:12<02:44,  4.32it/s]\u001b[A\n","Epoch 0:  97% 22783/23493 [3:47:49<07:05,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  73% 1890/2600 [07:13<02:41,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22785/23493 [3:47:49<07:04,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  73% 1892/2600 [07:13<02:42,  4.35it/s]\u001b[A\n","Epoch 0:  97% 22787/23493 [3:47:50<07:03,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  73% 1894/2600 [07:14<02:42,  4.34it/s]\u001b[A\n","Epoch 0:  97% 22789/23493 [3:47:50<07:02,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  73% 1896/2600 [07:14<02:41,  4.37it/s]\u001b[A\n","Epoch 0:  97% 22791/23493 [3:47:51<07:01,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  73% 1898/2600 [07:15<02:40,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22793/23493 [3:47:51<06:59,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  73% 1900/2600 [07:15<02:39,  4.39it/s]\u001b[A\n","Epoch 0:  97% 22795/23493 [3:47:52<06:58,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  73% 1902/2600 [07:16<02:38,  4.42it/s]\u001b[A\n","Epoch 0:  97% 22797/23493 [3:47:52<06:57,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  73% 1904/2600 [07:16<02:39,  4.37it/s]\u001b[A\n","Epoch 0:  97% 22799/23493 [3:47:53<06:56,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  73% 1906/2600 [07:17<02:39,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22801/23493 [3:47:53<06:54,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  73% 1908/2600 [07:17<02:37,  4.39it/s]\u001b[A\n","Epoch 0:  97% 22803/23493 [3:47:54<06:53,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  73% 1910/2600 [07:17<02:37,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22805/23493 [3:47:54<06:52,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  74% 1912/2600 [07:18<02:36,  4.41it/s]\u001b[A\n","Epoch 0:  97% 22807/23493 [3:47:55<06:51,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  74% 1914/2600 [07:18<02:37,  4.37it/s]\u001b[A\n","Epoch 0:  97% 22809/23493 [3:47:55<06:50,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  74% 1916/2600 [07:19<02:36,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22811/23493 [3:47:55<06:48,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  74% 1918/2600 [07:19<02:34,  4.40it/s]\u001b[A\n","Epoch 0:  97% 22813/23493 [3:47:56<06:47,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  74% 1920/2600 [07:20<02:33,  4.44it/s]\u001b[A\n","Epoch 0:  97% 22815/23493 [3:47:56<06:46,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  74% 1922/2600 [07:20<02:35,  4.37it/s]\u001b[A\n","Epoch 0:  97% 22817/23493 [3:47:57<06:45,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  74% 1924/2600 [07:21<02:33,  4.40it/s]\u001b[A\n","Epoch 0:  97% 22819/23493 [3:47:57<06:43,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  74% 1926/2600 [07:21<02:34,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22821/23493 [3:47:58<06:42,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  74% 1928/2600 [07:22<02:34,  4.34it/s]\u001b[A\n","Epoch 0:  97% 22823/23493 [3:47:58<06:41,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  74% 1930/2600 [07:22<02:32,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22825/23493 [3:47:59<06:40,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  74% 1932/2600 [07:22<02:33,  4.35it/s]\u001b[A\n","Epoch 0:  97% 22827/23493 [3:47:59<06:39,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  74% 1934/2600 [07:23<02:32,  4.37it/s]\u001b[A\n","Epoch 0:  97% 22829/23493 [3:48:00<06:37,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  74% 1936/2600 [07:23<02:32,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22831/23493 [3:48:00<06:36,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  75% 1938/2600 [07:24<02:30,  4.40it/s]\u001b[A\n","Epoch 0:  97% 22833/23493 [3:48:00<06:35,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  75% 1940/2600 [07:24<02:30,  4.37it/s]\u001b[A\n","Epoch 0:  97% 22835/23493 [3:48:01<06:34,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  75% 1942/2600 [07:25<02:29,  4.41it/s]\u001b[A\n","Epoch 0:  97% 22837/23493 [3:48:01<06:33,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  75% 1944/2600 [07:25<02:31,  4.34it/s]\u001b[A\n","Epoch 0:  97% 22839/23493 [3:48:02<06:31,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  75% 1946/2600 [07:26<02:29,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22841/23493 [3:48:02<06:30,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  75% 1948/2600 [07:26<02:28,  4.40it/s]\u001b[A\n","Epoch 0:  97% 22843/23493 [3:48:03<06:29,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  75% 1950/2600 [07:27<02:28,  4.37it/s]\u001b[A\n","Epoch 0:  97% 22845/23493 [3:48:03<06:28,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  75% 1952/2600 [07:27<02:27,  4.39it/s]\u001b[A\n","Epoch 0:  97% 22847/23493 [3:48:04<06:26,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  75% 1954/2600 [07:27<02:27,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22849/23493 [3:48:04<06:25,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  75% 1956/2600 [07:28<02:25,  4.42it/s]\u001b[A\n","Epoch 0:  97% 22851/23493 [3:48:05<06:24,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  75% 1958/2600 [07:28<02:26,  4.39it/s]\u001b[A\n","Epoch 0:  97% 22853/23493 [3:48:05<06:23,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  75% 1960/2600 [07:29<02:25,  4.40it/s]\u001b[A\n","Epoch 0:  97% 22855/23493 [3:48:05<06:22,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  75% 1962/2600 [07:29<02:25,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22857/23493 [3:48:06<06:20,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  76% 1964/2600 [07:30<02:24,  4.39it/s]\u001b[A\n","Epoch 0:  97% 22859/23493 [3:48:06<06:19,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  76% 1966/2600 [07:30<02:25,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22861/23493 [3:48:07<06:18,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  76% 1968/2600 [07:31<02:23,  4.40it/s]\u001b[A\n","Epoch 0:  97% 22863/23493 [3:48:07<06:17,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  76% 1970/2600 [07:31<02:22,  4.43it/s]\u001b[A\n","Epoch 0:  97% 22865/23493 [3:48:08<06:15,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  76% 1972/2600 [07:32<02:23,  4.37it/s]\u001b[A\n","Epoch 0:  97% 22867/23493 [3:48:08<06:14,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  76% 1974/2600 [07:32<02:21,  4.42it/s]\u001b[A\n","Epoch 0:  97% 22869/23493 [3:48:09<06:13,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  76% 1976/2600 [07:33<02:22,  4.39it/s]\u001b[A\n","Epoch 0:  97% 22871/23493 [3:48:09<06:12,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  76% 1978/2600 [07:33<02:22,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22873/23493 [3:48:10<06:11,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  76% 1980/2600 [07:33<02:20,  4.42it/s]\u001b[A\n","Epoch 0:  97% 22875/23493 [3:48:10<06:09,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  76% 1982/2600 [07:34<02:20,  4.39it/s]\u001b[A\n","Epoch 0:  97% 22877/23493 [3:48:10<06:08,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  76% 1984/2600 [07:34<02:21,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22879/23493 [3:48:11<06:07,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  76% 1986/2600 [07:35<02:20,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22881/23493 [3:48:11<06:06,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  76% 1988/2600 [07:35<02:20,  4.35it/s]\u001b[A\n","Epoch 0:  97% 22883/23493 [3:48:12<06:05,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  77% 1990/2600 [07:36<02:20,  4.33it/s]\u001b[A\n","Epoch 0:  97% 22885/23493 [3:48:12<06:03,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  77% 1992/2600 [07:36<02:19,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22887/23493 [3:48:13<06:02,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  77% 1994/2600 [07:37<02:18,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22889/23493 [3:48:13<06:01,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  77% 1996/2600 [07:37<02:18,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22891/23493 [3:48:14<06:00,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  77% 1998/2600 [07:38<02:18,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22893/23493 [3:48:14<05:58,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  77% 2000/2600 [07:38<02:17,  4.35it/s]\u001b[A\n","Epoch 0:  97% 22895/23493 [3:48:15<05:57,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  77% 2002/2600 [07:38<02:17,  4.35it/s]\u001b[A\n","Epoch 0:  97% 22897/23493 [3:48:15<05:56,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  77% 2004/2600 [07:39<02:18,  4.30it/s]\u001b[A\n","Epoch 0:  97% 22899/23493 [3:48:16<05:55,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  77% 2006/2600 [07:39<02:15,  4.39it/s]\u001b[A\n","Epoch 0:  97% 22901/23493 [3:48:16<05:54,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  77% 2008/2600 [07:40<02:15,  4.36it/s]\u001b[A\n","Epoch 0:  97% 22903/23493 [3:48:16<05:52,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  77% 2010/2600 [07:40<02:14,  4.38it/s]\u001b[A\n","Epoch 0:  97% 22905/23493 [3:48:17<05:51,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  77% 2012/2600 [07:41<02:13,  4.40it/s]\u001b[A\n","Epoch 0:  98% 22907/23493 [3:48:17<05:50,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  77% 2014/2600 [07:41<02:13,  4.39it/s]\u001b[A\n","Epoch 0:  98% 22909/23493 [3:48:18<05:49,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  78% 2016/2600 [07:42<02:14,  4.35it/s]\u001b[A\n","Epoch 0:  98% 22911/23493 [3:48:18<05:47,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  78% 2018/2600 [07:42<02:14,  4.32it/s]\u001b[A\n","Epoch 0:  98% 22913/23493 [3:48:19<05:46,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  78% 2020/2600 [07:43<02:12,  4.37it/s]\u001b[A\n","Epoch 0:  98% 22915/23493 [3:48:19<05:45,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  78% 2022/2600 [07:43<02:12,  4.36it/s]\u001b[A\n","Epoch 0:  98% 22917/23493 [3:48:20<05:44,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  78% 2024/2600 [07:43<02:12,  4.35it/s]\u001b[A\n","Epoch 0:  98% 22919/23493 [3:48:20<05:43,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  78% 2026/2600 [07:44<02:12,  4.34it/s]\u001b[A\n","Epoch 0:  98% 22921/23493 [3:48:21<05:41,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  78% 2028/2600 [07:44<02:12,  4.31it/s]\u001b[A\n","Epoch 0:  98% 22923/23493 [3:48:21<05:40,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  78% 2030/2600 [07:45<02:10,  4.38it/s]\u001b[A\n","Epoch 0:  98% 22925/23493 [3:48:21<05:39,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  78% 2032/2600 [07:45<02:10,  4.34it/s]\u001b[A\n","Epoch 0:  98% 22927/23493 [3:48:22<05:38,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  78% 2034/2600 [07:46<02:09,  4.39it/s]\u001b[A\n","Epoch 0:  98% 22929/23493 [3:48:22<05:37,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  78% 2036/2600 [07:46<02:09,  4.36it/s]\u001b[A\n","Epoch 0:  98% 22931/23493 [3:48:23<05:35,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  78% 2038/2600 [07:47<02:07,  4.40it/s]\u001b[A\n","Epoch 0:  98% 22933/23493 [3:48:23<05:34,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  78% 2040/2600 [07:47<02:09,  4.33it/s]\u001b[A\n","Epoch 0:  98% 22935/23493 [3:48:24<05:33,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  79% 2042/2600 [07:48<02:06,  4.40it/s]\u001b[A\n","Epoch 0:  98% 22937/23493 [3:48:24<05:32,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  79% 2044/2600 [07:48<02:07,  4.36it/s]\u001b[A\n","Epoch 0:  98% 22939/23493 [3:48:25<05:30,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  79% 2046/2600 [07:49<02:05,  4.40it/s]\u001b[A\n","Epoch 0:  98% 22941/23493 [3:48:25<05:29,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  79% 2048/2600 [07:49<02:04,  4.44it/s]\u001b[A\n","Epoch 0:  98% 22943/23493 [3:48:26<05:28,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  79% 2050/2600 [07:49<02:05,  4.37it/s]\u001b[A\n","Epoch 0:  98% 22945/23493 [3:48:26<05:27,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  79% 2052/2600 [07:50<02:04,  4.39it/s]\u001b[A\n","Epoch 0:  98% 22947/23493 [3:48:27<05:26,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  79% 2054/2600 [07:50<02:05,  4.36it/s]\u001b[A\n","Epoch 0:  98% 22949/23493 [3:48:27<05:24,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  79% 2056/2600 [07:51<02:05,  4.35it/s]\u001b[A\n","Epoch 0:  98% 22951/23493 [3:48:27<05:23,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  79% 2058/2600 [07:51<02:05,  4.33it/s]\u001b[A\n","Epoch 0:  98% 22953/23493 [3:48:28<05:22,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  79% 2060/2600 [07:52<02:03,  4.36it/s]\u001b[A\n","Epoch 0:  98% 22955/23493 [3:48:28<05:21,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  79% 2062/2600 [07:52<02:03,  4.35it/s]\u001b[A\n","Epoch 0:  98% 22957/23493 [3:48:29<05:20,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  79% 2064/2600 [07:53<02:03,  4.36it/s]\u001b[A\n","Epoch 0:  98% 22959/23493 [3:48:29<05:18,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  79% 2066/2600 [07:53<02:01,  4.39it/s]\u001b[A\n","Epoch 0:  98% 22961/23493 [3:48:30<05:17,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  80% 2068/2600 [07:54<02:00,  4.40it/s]\u001b[A\n","Epoch 0:  98% 22963/23493 [3:48:30<05:16,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  80% 2070/2600 [07:54<02:01,  4.37it/s]\u001b[A\n","Epoch 0:  98% 22965/23493 [3:48:31<05:15,  1.67it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  80% 2072/2600 [07:54<02:02,  4.31it/s]\u001b[A\n","Epoch 0:  98% 22967/23493 [3:48:31<05:14,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  80% 2074/2600 [07:55<01:59,  4.39it/s]\u001b[A\n","Epoch 0:  98% 22969/23493 [3:48:32<05:12,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  80% 2076/2600 [07:55<02:00,  4.33it/s]\u001b[A\n","Epoch 0:  98% 22971/23493 [3:48:32<05:11,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  80% 2078/2600 [07:56<01:58,  4.39it/s]\u001b[A\n","Epoch 0:  98% 22973/23493 [3:48:32<05:10,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  80% 2080/2600 [07:56<01:59,  4.37it/s]\u001b[A\n","Epoch 0:  98% 22975/23493 [3:48:33<05:09,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  80% 2082/2600 [07:57<02:00,  4.30it/s]\u001b[A\n","Epoch 0:  98% 22977/23493 [3:48:33<05:07,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  80% 2084/2600 [07:57<01:57,  4.39it/s]\u001b[A\n","Epoch 0:  98% 22979/23493 [3:48:34<05:06,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  80% 2086/2600 [07:58<01:57,  4.38it/s]\u001b[A\n","Epoch 0:  98% 22981/23493 [3:48:34<05:05,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  80% 2088/2600 [07:58<01:56,  4.38it/s]\u001b[A\n","Epoch 0:  98% 22983/23493 [3:48:35<05:04,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  80% 2090/2600 [07:59<01:56,  4.40it/s]\u001b[A\n","Epoch 0:  98% 22985/23493 [3:48:35<05:03,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  80% 2092/2600 [07:59<01:56,  4.37it/s]\u001b[A\n","Epoch 0:  98% 22987/23493 [3:48:36<05:01,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  81% 2094/2600 [08:00<01:55,  4.38it/s]\u001b[A\n","Epoch 0:  98% 22989/23493 [3:48:36<05:00,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  81% 2096/2600 [08:00<01:55,  4.35it/s]\u001b[A\n","Epoch 0:  98% 22991/23493 [3:48:37<04:59,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  81% 2098/2600 [08:00<01:54,  4.39it/s]\u001b[A\n","Epoch 0:  98% 22993/23493 [3:48:37<04:58,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  81% 2100/2600 [08:01<01:53,  4.41it/s]\u001b[A\n","Epoch 0:  98% 22995/23493 [3:48:37<04:57,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  81% 2102/2600 [08:01<01:53,  4.38it/s]\u001b[A\n","Epoch 0:  98% 22997/23493 [3:48:38<04:55,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  81% 2104/2600 [08:02<01:52,  4.41it/s]\u001b[A\n","Epoch 0:  98% 22999/23493 [3:48:38<04:54,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  81% 2106/2600 [08:02<01:52,  4.38it/s]\u001b[A\n","Epoch 0:  98% 23001/23493 [3:48:39<04:53,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  81% 2108/2600 [08:03<01:52,  4.39it/s]\u001b[A\n","Epoch 0:  98% 23003/23493 [3:48:39<04:52,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  81% 2110/2600 [08:03<01:50,  4.43it/s]\u001b[A\n","Epoch 0:  98% 23005/23493 [3:48:40<04:51,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  81% 2112/2600 [08:04<01:51,  4.39it/s]\u001b[A\n","Epoch 0:  98% 23007/23493 [3:48:40<04:49,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  81% 2114/2600 [08:04<01:49,  4.42it/s]\u001b[A\n","Epoch 0:  98% 23009/23493 [3:48:41<04:48,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  81% 2116/2600 [08:05<01:50,  4.37it/s]\u001b[A\n","Epoch 0:  98% 23011/23493 [3:48:41<04:47,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  81% 2118/2600 [08:05<01:49,  4.39it/s]\u001b[A\n","Epoch 0:  98% 23013/23493 [3:48:42<04:46,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  82% 2120/2600 [08:05<01:49,  4.38it/s]\u001b[A\n","Epoch 0:  98% 23015/23493 [3:48:42<04:45,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  82% 2122/2600 [08:06<01:48,  4.40it/s]\u001b[A\n","Epoch 0:  98% 23017/23493 [3:48:42<04:43,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  82% 2124/2600 [08:06<01:49,  4.35it/s]\u001b[A\n","Epoch 0:  98% 23019/23493 [3:48:43<04:42,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  82% 2126/2600 [08:07<01:49,  4.34it/s]\u001b[A\n","Epoch 0:  98% 23021/23493 [3:48:43<04:41,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  82% 2128/2600 [08:07<01:48,  4.36it/s]\u001b[A\n","Epoch 0:  98% 23023/23493 [3:48:44<04:40,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  82% 2130/2600 [08:08<01:48,  4.35it/s]\u001b[A\n","Epoch 0:  98% 23025/23493 [3:48:44<04:38,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  82% 2132/2600 [08:08<01:46,  4.39it/s]\u001b[A\n","Epoch 0:  98% 23027/23493 [3:48:45<04:37,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  82% 2134/2600 [08:09<01:46,  4.36it/s]\u001b[A\n","Epoch 0:  98% 23029/23493 [3:48:45<04:36,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  82% 2136/2600 [08:09<01:45,  4.40it/s]\u001b[A\n","Epoch 0:  98% 23031/23493 [3:48:46<04:35,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  82% 2138/2600 [08:10<01:46,  4.36it/s]\u001b[A\n","Epoch 0:  98% 23033/23493 [3:48:46<04:34,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  82% 2140/2600 [08:10<01:44,  4.38it/s]\u001b[A\n","Epoch 0:  98% 23035/23493 [3:48:47<04:32,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  82% 2142/2600 [08:10<01:44,  4.39it/s]\u001b[A\n","Epoch 0:  98% 23037/23493 [3:48:47<04:31,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  82% 2144/2600 [08:11<01:44,  4.37it/s]\u001b[A\n","Epoch 0:  98% 23039/23493 [3:48:48<04:30,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  83% 2146/2600 [08:11<01:43,  4.39it/s]\u001b[A\n","Epoch 0:  98% 23041/23493 [3:48:48<04:29,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  83% 2148/2600 [08:12<01:43,  4.38it/s]\u001b[A\n","Epoch 0:  98% 23043/23493 [3:48:48<04:28,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  83% 2150/2600 [08:12<01:43,  4.37it/s]\u001b[A\n","Epoch 0:  98% 23045/23493 [3:48:49<04:26,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  83% 2152/2600 [08:13<01:42,  4.36it/s]\u001b[A\n","Epoch 0:  98% 23047/23493 [3:48:49<04:25,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  83% 2154/2600 [08:13<01:41,  4.41it/s]\u001b[A\n","Epoch 0:  98% 23049/23493 [3:48:50<04:24,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  83% 2156/2600 [08:14<01:41,  4.37it/s]\u001b[A\n","Epoch 0:  98% 23051/23493 [3:48:50<04:23,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  83% 2158/2600 [08:14<01:41,  4.37it/s]\u001b[A\n","Epoch 0:  98% 23053/23493 [3:48:51<04:22,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  83% 2160/2600 [08:15<01:41,  4.34it/s]\u001b[A\n","Epoch 0:  98% 23055/23493 [3:48:51<04:20,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  83% 2162/2600 [08:15<01:40,  4.38it/s]\u001b[A\n","Epoch 0:  98% 23057/23493 [3:48:52<04:19,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  83% 2164/2600 [08:16<01:40,  4.34it/s]\u001b[A\n","Epoch 0:  98% 23059/23493 [3:48:52<04:18,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  83% 2166/2600 [08:16<01:39,  4.35it/s]\u001b[A\n","Epoch 0:  98% 23061/23493 [3:48:53<04:17,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  83% 2168/2600 [08:16<01:38,  4.40it/s]\u001b[A\n","Epoch 0:  98% 23063/23493 [3:48:53<04:16,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  83% 2170/2600 [08:17<01:38,  4.35it/s]\u001b[A\n","Epoch 0:  98% 23065/23493 [3:48:53<04:14,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  84% 2172/2600 [08:17<01:38,  4.33it/s]\u001b[A\n","Epoch 0:  98% 23067/23493 [3:48:54<04:13,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  84% 2174/2600 [08:18<01:37,  4.35it/s]\u001b[A\n","Epoch 0:  98% 23069/23493 [3:48:54<04:12,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  84% 2176/2600 [08:18<01:37,  4.35it/s]\u001b[A\n","Epoch 0:  98% 23071/23493 [3:48:55<04:11,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  84% 2178/2600 [08:19<01:36,  4.39it/s]\u001b[A\n","Epoch 0:  98% 23073/23493 [3:48:55<04:10,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  84% 2180/2600 [08:19<01:35,  4.38it/s]\u001b[A\n","Epoch 0:  98% 23075/23493 [3:48:56<04:08,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  84% 2182/2600 [08:20<01:35,  4.37it/s]\u001b[A\n","Epoch 0:  98% 23077/23493 [3:48:56<04:07,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  84% 2184/2600 [08:20<01:35,  4.37it/s]\u001b[A\n","Epoch 0:  98% 23079/23493 [3:48:57<04:06,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  84% 2186/2600 [08:21<01:35,  4.35it/s]\u001b[A\n","Epoch 0:  98% 23081/23493 [3:48:57<04:05,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  84% 2188/2600 [08:21<01:34,  4.34it/s]\u001b[A\n","Epoch 0:  98% 23083/23493 [3:48:58<04:04,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  84% 2190/2600 [08:21<01:34,  4.33it/s]\u001b[A\n","Epoch 0:  98% 23085/23493 [3:48:58<04:02,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  84% 2192/2600 [08:22<01:33,  4.38it/s]\u001b[A\n","Epoch 0:  98% 23087/23493 [3:48:59<04:01,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  84% 2194/2600 [08:22<01:32,  4.40it/s]\u001b[A\n","Epoch 0:  98% 23089/23493 [3:48:59<04:00,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  84% 2196/2600 [08:23<01:32,  4.38it/s]\u001b[A\n","Epoch 0:  98% 23091/23493 [3:48:59<03:59,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  85% 2198/2600 [08:23<01:31,  4.39it/s]\u001b[A\n","Epoch 0:  98% 23093/23493 [3:49:00<03:58,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  85% 2200/2600 [08:24<01:30,  4.42it/s]\u001b[A\n","Epoch 0:  98% 23095/23493 [3:49:00<03:56,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  85% 2202/2600 [08:24<01:30,  4.38it/s]\u001b[A\n","Epoch 0:  98% 23097/23493 [3:49:01<03:55,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  85% 2204/2600 [08:25<01:31,  4.35it/s]\u001b[A\n","Epoch 0:  98% 23099/23493 [3:49:01<03:54,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  85% 2206/2600 [08:25<01:30,  4.34it/s]\u001b[A\n","Epoch 0:  98% 23101/23493 [3:49:02<03:53,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  85% 2208/2600 [08:26<01:28,  4.41it/s]\u001b[A\n","Epoch 0:  98% 23103/23493 [3:49:02<03:51,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  85% 2210/2600 [08:26<01:28,  4.39it/s]\u001b[A\n","Epoch 0:  98% 23105/23493 [3:49:03<03:50,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  85% 2212/2600 [08:26<01:29,  4.36it/s]\u001b[A\n","Epoch 0:  98% 23107/23493 [3:49:03<03:49,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  85% 2214/2600 [08:27<01:28,  4.37it/s]\u001b[A\n","Epoch 0:  98% 23109/23493 [3:49:04<03:48,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  85% 2216/2600 [08:27<01:27,  4.41it/s]\u001b[A\n","Epoch 0:  98% 23111/23493 [3:49:04<03:47,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  85% 2218/2600 [08:28<01:27,  4.37it/s]\u001b[A\n","Epoch 0:  98% 23113/23493 [3:49:04<03:45,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  85% 2220/2600 [08:28<01:26,  4.42it/s]\u001b[A\n","Epoch 0:  98% 23115/23493 [3:49:05<03:44,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  85% 2222/2600 [08:29<01:26,  4.38it/s]\u001b[A\n","Epoch 0:  98% 23117/23493 [3:49:05<03:43,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  86% 2224/2600 [08:29<01:25,  4.40it/s]\u001b[A\n","Epoch 0:  98% 23119/23493 [3:49:06<03:42,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  86% 2226/2600 [08:30<01:24,  4.42it/s]\u001b[A\n","Epoch 0:  98% 23121/23493 [3:49:06<03:41,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  86% 2228/2600 [08:30<01:24,  4.38it/s]\u001b[A\n","Epoch 0:  98% 23123/23493 [3:49:07<03:39,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  86% 2230/2600 [08:31<01:24,  4.38it/s]\u001b[A\n","Epoch 0:  98% 23125/23493 [3:49:07<03:38,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  86% 2232/2600 [08:31<01:22,  4.44it/s]\u001b[A\n","Epoch 0:  98% 23127/23493 [3:49:08<03:37,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  86% 2234/2600 [08:31<01:23,  4.39it/s]\u001b[A\n","Epoch 0:  98% 23129/23493 [3:49:08<03:36,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  86% 2236/2600 [08:32<01:23,  4.39it/s]\u001b[A\n","Epoch 0:  98% 23131/23493 [3:49:09<03:35,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  86% 2238/2600 [08:32<01:22,  4.38it/s]\u001b[A\n","Epoch 0:  98% 23133/23493 [3:49:09<03:33,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  86% 2240/2600 [08:33<01:21,  4.41it/s]\u001b[A\n","Epoch 0:  98% 23135/23493 [3:49:09<03:32,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  86% 2242/2600 [08:33<01:21,  4.37it/s]\u001b[A\n","Epoch 0:  98% 23137/23493 [3:49:10<03:31,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  86% 2244/2600 [08:34<01:20,  4.40it/s]\u001b[A\n","Epoch 0:  98% 23139/23493 [3:49:10<03:30,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  86% 2246/2600 [08:34<01:20,  4.41it/s]\u001b[A\n","Epoch 0:  99% 23141/23493 [3:49:11<03:29,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  86% 2248/2600 [08:35<01:20,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23143/23493 [3:49:11<03:27,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  87% 2250/2600 [08:35<01:20,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23145/23493 [3:49:12<03:26,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  87% 2252/2600 [08:36<01:20,  4.35it/s]\u001b[A\n","Epoch 0:  99% 23147/23493 [3:49:12<03:25,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  87% 2254/2600 [08:36<01:19,  4.34it/s]\u001b[A\n","Epoch 0:  99% 23149/23493 [3:49:13<03:24,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  87% 2256/2600 [08:37<01:19,  4.34it/s]\u001b[A\n","Epoch 0:  99% 23151/23493 [3:49:13<03:23,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  87% 2258/2600 [08:37<01:18,  4.38it/s]\u001b[A\n","Epoch 0:  99% 23153/23493 [3:49:14<03:21,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  87% 2260/2600 [08:37<01:17,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23155/23493 [3:49:14<03:20,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  87% 2262/2600 [08:38<01:16,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23157/23493 [3:49:15<03:19,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  87% 2264/2600 [08:38<01:17,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23159/23493 [3:49:15<03:18,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  87% 2266/2600 [08:39<01:16,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23161/23493 [3:49:15<03:17,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  87% 2268/2600 [08:39<01:16,  4.32it/s]\u001b[A\n","Epoch 0:  99% 23163/23493 [3:49:16<03:15,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  87% 2270/2600 [08:40<01:15,  4.35it/s]\u001b[A\n","Epoch 0:  99% 23165/23493 [3:49:16<03:14,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  87% 2272/2600 [08:40<01:14,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23167/23493 [3:49:17<03:13,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  87% 2274/2600 [08:41<01:14,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23169/23493 [3:49:17<03:12,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  88% 2276/2600 [08:41<01:13,  4.38it/s]\u001b[A\n","Epoch 0:  99% 23171/23493 [3:49:18<03:11,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  88% 2278/2600 [08:42<01:14,  4.33it/s]\u001b[A\n","Epoch 0:  99% 23173/23493 [3:49:18<03:09,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  88% 2280/2600 [08:42<01:13,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23175/23493 [3:49:19<03:08,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  88% 2282/2600 [08:43<01:13,  4.31it/s]\u001b[A\n","Epoch 0:  99% 23177/23493 [3:49:19<03:07,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  88% 2284/2600 [08:43<01:12,  4.35it/s]\u001b[A\n","Epoch 0:  99% 23179/23493 [3:49:20<03:06,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  88% 2286/2600 [08:43<01:11,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23181/23493 [3:49:20<03:05,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  88% 2288/2600 [08:44<01:11,  4.34it/s]\u001b[A\n","Epoch 0:  99% 23183/23493 [3:49:20<03:04,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  88% 2290/2600 [08:44<01:10,  4.38it/s]\u001b[A\n","Epoch 0:  99% 23185/23493 [3:49:21<03:02,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  88% 2292/2600 [08:45<01:10,  4.34it/s]\u001b[A\n","Epoch 0:  99% 23187/23493 [3:49:21<03:01,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  88% 2294/2600 [08:45<01:09,  4.38it/s]\u001b[A\n","Epoch 0:  99% 23189/23493 [3:49:22<03:00,  1.68it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  88% 2296/2600 [08:46<01:09,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23191/23493 [3:49:22<02:59,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  88% 2298/2600 [08:46<01:09,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23193/23493 [3:49:23<02:58,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  88% 2300/2600 [08:47<01:08,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23195/23493 [3:49:23<02:56,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  89% 2302/2600 [08:47<01:07,  4.39it/s]\u001b[A\n","Epoch 0:  99% 23197/23493 [3:49:24<02:55,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  89% 2304/2600 [08:48<01:07,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23199/23493 [3:49:24<02:54,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  89% 2306/2600 [08:48<01:07,  4.35it/s]\u001b[A\n","Epoch 0:  99% 23201/23493 [3:49:25<02:53,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  89% 2308/2600 [08:48<01:06,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23203/23493 [3:49:25<02:52,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  89% 2310/2600 [08:49<01:06,  4.33it/s]\u001b[A\n","Epoch 0:  99% 23205/23493 [3:49:26<02:50,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  89% 2312/2600 [08:49<01:05,  4.39it/s]\u001b[A\n","Epoch 0:  99% 23207/23493 [3:49:26<02:49,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  89% 2314/2600 [08:50<01:05,  4.35it/s]\u001b[A\n","Epoch 0:  99% 23209/23493 [3:49:26<02:48,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  89% 2316/2600 [08:50<01:04,  4.38it/s]\u001b[A\n","Epoch 0:  99% 23211/23493 [3:49:27<02:47,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  89% 2318/2600 [08:51<01:04,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23213/23493 [3:49:27<02:46,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  89% 2320/2600 [08:51<01:04,  4.34it/s]\u001b[A\n","Epoch 0:  99% 23215/23493 [3:49:28<02:44,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  89% 2322/2600 [08:52<01:03,  4.39it/s]\u001b[A\n","Epoch 0:  99% 23217/23493 [3:49:28<02:43,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  89% 2324/2600 [08:52<01:02,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23219/23493 [3:49:29<02:42,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  89% 2326/2600 [08:53<01:02,  4.39it/s]\u001b[A\n","Epoch 0:  99% 23221/23493 [3:49:29<02:41,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  90% 2328/2600 [08:53<01:02,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23223/23493 [3:49:30<02:40,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  90% 2330/2600 [08:53<01:01,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23225/23493 [3:49:30<02:38,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  90% 2332/2600 [08:54<01:00,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23227/23493 [3:49:31<02:37,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  90% 2334/2600 [08:54<01:01,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23229/23493 [3:49:31<02:36,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  90% 2336/2600 [08:55<01:00,  4.39it/s]\u001b[A\n","Epoch 0:  99% 23231/23493 [3:49:31<02:35,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  90% 2338/2600 [08:55<00:59,  4.38it/s]\u001b[A\n","Epoch 0:  99% 23233/23493 [3:49:32<02:34,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  90% 2340/2600 [08:56<00:59,  4.39it/s]\u001b[A\n","Epoch 0:  99% 23235/23493 [3:49:32<02:32,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  90% 2342/2600 [08:56<00:59,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23237/23493 [3:49:33<02:31,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  90% 2344/2600 [08:57<00:58,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23239/23493 [3:49:33<02:30,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  90% 2346/2600 [08:57<00:58,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23241/23493 [3:49:34<02:29,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  90% 2348/2600 [08:58<00:57,  4.38it/s]\u001b[A\n","Epoch 0:  99% 23243/23493 [3:49:34<02:28,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  90% 2350/2600 [08:58<00:57,  4.31it/s]\u001b[A\n","Epoch 0:  99% 23245/23493 [3:49:35<02:26,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  90% 2352/2600 [08:59<00:57,  4.34it/s]\u001b[A\n","Epoch 0:  99% 23247/23493 [3:49:35<02:25,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  91% 2354/2600 [08:59<00:56,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23249/23493 [3:49:36<02:24,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  91% 2356/2600 [08:59<00:55,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23251/23493 [3:49:36<02:23,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  91% 2358/2600 [09:00<00:54,  4.41it/s]\u001b[A\n","Epoch 0:  99% 23253/23493 [3:49:36<02:22,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  91% 2360/2600 [09:00<00:54,  4.39it/s]\u001b[A\n","Epoch 0:  99% 23255/23493 [3:49:37<02:21,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  91% 2362/2600 [09:01<00:54,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23257/23493 [3:49:37<02:19,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  91% 2364/2600 [09:01<00:53,  4.43it/s]\u001b[A\n","Epoch 0:  99% 23259/23493 [3:49:38<02:18,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  91% 2366/2600 [09:02<00:52,  4.45it/s]\u001b[A\n","Epoch 0:  99% 23261/23493 [3:49:38<02:17,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  91% 2368/2600 [09:02<00:52,  4.43it/s]\u001b[A\n","Epoch 0:  99% 23263/23493 [3:49:39<02:16,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  91% 2370/2600 [09:03<00:52,  4.42it/s]\u001b[A\n","Epoch 0:  99% 23265/23493 [3:49:39<02:15,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  91% 2372/2600 [09:03<00:51,  4.41it/s]\u001b[A\n","Epoch 0:  99% 23267/23493 [3:49:40<02:13,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  91% 2374/2600 [09:04<00:51,  4.43it/s]\u001b[A\n","Epoch 0:  99% 23269/23493 [3:49:40<02:12,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  91% 2376/2600 [09:04<00:50,  4.42it/s]\u001b[A\n","Epoch 0:  99% 23271/23493 [3:49:41<02:11,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  91% 2378/2600 [09:04<00:50,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23273/23493 [3:49:41<02:10,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  92% 2380/2600 [09:05<00:49,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23275/23493 [3:49:41<02:09,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  92% 2382/2600 [09:05<00:49,  4.38it/s]\u001b[A\n","Epoch 0:  99% 23277/23493 [3:49:42<02:07,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  92% 2384/2600 [09:06<00:49,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23279/23493 [3:49:42<02:06,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  92% 2386/2600 [09:06<00:48,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23281/23493 [3:49:43<02:05,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  92% 2388/2600 [09:07<00:48,  4.39it/s]\u001b[A\n","Epoch 0:  99% 23283/23493 [3:49:43<02:04,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  92% 2390/2600 [09:07<00:47,  4.39it/s]\u001b[A\n","Epoch 0:  99% 23285/23493 [3:49:44<02:03,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  92% 2392/2600 [09:08<00:47,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23287/23493 [3:49:44<02:01,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  92% 2394/2600 [09:08<00:46,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23289/23493 [3:49:45<02:00,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  92% 2396/2600 [09:09<00:46,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23291/23493 [3:49:45<01:59,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  92% 2398/2600 [09:09<00:45,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23293/23493 [3:49:46<01:58,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  92% 2400/2600 [09:09<00:45,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23295/23493 [3:49:46<01:57,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  92% 2402/2600 [09:10<00:45,  4.38it/s]\u001b[A\n","Epoch 0:  99% 23297/23493 [3:49:47<01:55,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  92% 2404/2600 [09:10<00:44,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23299/23493 [3:49:47<01:54,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  93% 2406/2600 [09:11<00:44,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23301/23493 [3:49:47<01:53,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  93% 2408/2600 [09:11<00:44,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23303/23493 [3:49:48<01:52,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  93% 2410/2600 [09:12<00:43,  4.38it/s]\u001b[A\n","Epoch 0:  99% 23305/23493 [3:49:48<01:51,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  93% 2412/2600 [09:12<00:43,  4.35it/s]\u001b[A\n","Epoch 0:  99% 23307/23493 [3:49:49<01:50,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  93% 2414/2600 [09:13<00:42,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23309/23493 [3:49:49<01:48,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  93% 2416/2600 [09:13<00:41,  4.42it/s]\u001b[A\n","Epoch 0:  99% 23311/23493 [3:49:50<01:47,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  93% 2418/2600 [09:14<00:41,  4.38it/s]\u001b[A\n","Epoch 0:  99% 23313/23493 [3:49:50<01:46,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  93% 2420/2600 [09:14<00:40,  4.43it/s]\u001b[A\n","Epoch 0:  99% 23315/23493 [3:49:51<01:45,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  93% 2422/2600 [09:14<00:40,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23317/23493 [3:49:51<01:44,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  93% 2424/2600 [09:15<00:39,  4.41it/s]\u001b[A\n","Epoch 0:  99% 23319/23493 [3:49:52<01:42,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  93% 2426/2600 [09:15<00:39,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23321/23493 [3:49:52<01:41,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  93% 2428/2600 [09:16<00:39,  4.39it/s]\u001b[A\n","Epoch 0:  99% 23323/23493 [3:49:52<01:40,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  93% 2430/2600 [09:16<00:38,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23325/23493 [3:49:53<01:39,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  94% 2432/2600 [09:17<00:38,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23327/23493 [3:49:53<01:38,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  94% 2434/2600 [09:17<00:37,  4.42it/s]\u001b[A\n","Epoch 0:  99% 23329/23493 [3:49:54<01:36,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  94% 2436/2600 [09:18<00:37,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23331/23493 [3:49:54<01:35,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  94% 2438/2600 [09:18<00:36,  4.41it/s]\u001b[A\n","Epoch 0:  99% 23333/23493 [3:49:55<01:34,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  94% 2440/2600 [09:19<00:36,  4.35it/s]\u001b[A\n","Epoch 0:  99% 23335/23493 [3:49:55<01:33,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  94% 2442/2600 [09:19<00:36,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23337/23493 [3:49:56<01:32,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  94% 2444/2600 [09:19<00:35,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23339/23493 [3:49:56<01:31,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  94% 2446/2600 [09:20<00:34,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23341/23493 [3:49:57<01:29,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  94% 2448/2600 [09:20<00:34,  4.38it/s]\u001b[A\n","Epoch 0:  99% 23343/23493 [3:49:57<01:28,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  94% 2450/2600 [09:21<00:34,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23345/23493 [3:49:57<01:27,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  94% 2452/2600 [09:21<00:33,  4.43it/s]\u001b[A\n","Epoch 0:  99% 23347/23493 [3:49:58<01:26,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  94% 2454/2600 [09:22<00:33,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23349/23493 [3:49:58<01:25,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  94% 2456/2600 [09:22<00:32,  4.42it/s]\u001b[A\n","Epoch 0:  99% 23351/23493 [3:49:59<01:23,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  95% 2458/2600 [09:23<00:32,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23353/23493 [3:49:59<01:22,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  95% 2460/2600 [09:23<00:31,  4.38it/s]\u001b[A\n","Epoch 0:  99% 23355/23493 [3:50:00<01:21,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  95% 2462/2600 [09:24<00:31,  4.38it/s]\u001b[A\n","Epoch 0:  99% 23357/23493 [3:50:00<01:20,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  95% 2464/2600 [09:24<00:30,  4.42it/s]\u001b[A\n","Epoch 0:  99% 23359/23493 [3:50:01<01:19,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  95% 2466/2600 [09:24<00:30,  4.37it/s]\u001b[A\n","Epoch 0:  99% 23361/23493 [3:50:01<01:17,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  95% 2468/2600 [09:25<00:30,  4.39it/s]\u001b[A\n","Epoch 0:  99% 23363/23493 [3:50:02<01:16,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  95% 2470/2600 [09:25<00:29,  4.41it/s]\u001b[A\n","Epoch 0:  99% 23365/23493 [3:50:02<01:15,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  95% 2472/2600 [09:26<00:29,  4.39it/s]\u001b[A\n","Epoch 0:  99% 23367/23493 [3:50:02<01:14,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  95% 2474/2600 [09:26<00:28,  4.43it/s]\u001b[A\n","Epoch 0:  99% 23369/23493 [3:50:03<01:13,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  95% 2476/2600 [09:27<00:28,  4.36it/s]\u001b[A\n","Epoch 0:  99% 23371/23493 [3:50:03<01:12,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  95% 2478/2600 [09:27<00:27,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23373/23493 [3:50:04<01:10,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  95% 2480/2600 [09:28<00:27,  4.40it/s]\u001b[A\n","Epoch 0:  99% 23375/23493 [3:50:04<01:09,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  95% 2482/2600 [09:28<00:26,  4.37it/s]\u001b[A\n","Epoch 0: 100% 23377/23493 [3:50:05<01:08,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  96% 2484/2600 [09:29<00:26,  4.39it/s]\u001b[A\n","Epoch 0: 100% 23379/23493 [3:50:05<01:07,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  96% 2486/2600 [09:29<00:25,  4.39it/s]\u001b[A\n","Epoch 0: 100% 23381/23493 [3:50:06<01:06,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  96% 2488/2600 [09:30<00:25,  4.37it/s]\u001b[A\n","Epoch 0: 100% 23383/23493 [3:50:06<01:04,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  96% 2490/2600 [09:30<00:25,  4.39it/s]\u001b[A\n","Epoch 0: 100% 23385/23493 [3:50:07<01:03,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  96% 2492/2600 [09:30<00:24,  4.44it/s]\u001b[A\n","Epoch 0: 100% 23387/23493 [3:50:07<01:02,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  96% 2494/2600 [09:31<00:24,  4.41it/s]\u001b[A\n","Epoch 0: 100% 23389/23493 [3:50:07<01:01,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  96% 2496/2600 [09:31<00:23,  4.38it/s]\u001b[A\n","Epoch 0: 100% 23391/23493 [3:50:08<01:00,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  96% 2498/2600 [09:32<00:23,  4.36it/s]\u001b[A\n","Epoch 0: 100% 23393/23493 [3:50:08<00:59,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  96% 2500/2600 [09:32<00:22,  4.40it/s]\u001b[A\n","Epoch 0: 100% 23395/23493 [3:50:09<00:57,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  96% 2502/2600 [09:33<00:22,  4.42it/s]\u001b[A\n","Epoch 0: 100% 23397/23493 [3:50:09<00:56,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  96% 2504/2600 [09:33<00:21,  4.43it/s]\u001b[A\n","Epoch 0: 100% 23399/23493 [3:50:10<00:55,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  96% 2506/2600 [09:34<00:21,  4.41it/s]\u001b[A\n","Epoch 0: 100% 23401/23493 [3:50:10<00:54,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  96% 2508/2600 [09:34<00:20,  4.42it/s]\u001b[A\n","Epoch 0: 100% 23403/23493 [3:50:11<00:53,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  97% 2510/2600 [09:34<00:20,  4.44it/s]\u001b[A\n","Epoch 0: 100% 23405/23493 [3:50:11<00:51,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  97% 2512/2600 [09:35<00:19,  4.45it/s]\u001b[A\n","Epoch 0: 100% 23407/23493 [3:50:12<00:50,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  97% 2514/2600 [09:35<00:19,  4.42it/s]\u001b[A\n","Epoch 0: 100% 23409/23493 [3:50:12<00:49,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  97% 2516/2600 [09:36<00:19,  4.42it/s]\u001b[A\n","Epoch 0: 100% 23411/23493 [3:50:12<00:48,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  97% 2518/2600 [09:36<00:18,  4.42it/s]\u001b[A\n","Epoch 0: 100% 23413/23493 [3:50:13<00:47,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  97% 2520/2600 [09:37<00:17,  4.45it/s]\u001b[A\n","Epoch 0: 100% 23415/23493 [3:50:13<00:46,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  97% 2522/2600 [09:37<00:17,  4.45it/s]\u001b[A\n","Epoch 0: 100% 23417/23493 [3:50:14<00:44,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  97% 2524/2600 [09:38<00:17,  4.40it/s]\u001b[A\n","Epoch 0: 100% 23419/23493 [3:50:14<00:43,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  97% 2526/2600 [09:38<00:16,  4.43it/s]\u001b[A\n","Epoch 0: 100% 23421/23493 [3:50:15<00:42,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  97% 2528/2600 [09:39<00:16,  4.39it/s]\u001b[A\n","Epoch 0: 100% 23423/23493 [3:50:15<00:41,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  97% 2530/2600 [09:39<00:15,  4.38it/s]\u001b[A\n","Epoch 0: 100% 23425/23493 [3:50:16<00:40,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  97% 2532/2600 [09:39<00:15,  4.40it/s]\u001b[A\n","Epoch 0: 100% 23427/23493 [3:50:16<00:38,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  97% 2534/2600 [09:40<00:15,  4.39it/s]\u001b[A\n","Epoch 0: 100% 23429/23493 [3:50:17<00:37,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  98% 2536/2600 [09:40<00:14,  4.41it/s]\u001b[A\n","Epoch 0: 100% 23431/23493 [3:50:17<00:36,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  98% 2538/2600 [09:41<00:14,  4.38it/s]\u001b[A\n","Epoch 0: 100% 23433/23493 [3:50:17<00:35,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  98% 2540/2600 [09:41<00:13,  4.41it/s]\u001b[A\n","Epoch 0: 100% 23435/23493 [3:50:18<00:34,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  98% 2542/2600 [09:42<00:13,  4.40it/s]\u001b[A\n","Epoch 0: 100% 23437/23493 [3:50:18<00:33,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  98% 2544/2600 [09:42<00:12,  4.38it/s]\u001b[A\n","Epoch 0: 100% 23439/23493 [3:50:19<00:31,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  98% 2546/2600 [09:43<00:12,  4.40it/s]\u001b[A\n","Epoch 0: 100% 23441/23493 [3:50:19<00:30,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  98% 2548/2600 [09:43<00:11,  4.39it/s]\u001b[A\n","Epoch 0: 100% 23443/23493 [3:50:20<00:29,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  98% 2550/2600 [09:44<00:11,  4.41it/s]\u001b[A\n","Epoch 0: 100% 23445/23493 [3:50:20<00:28,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  98% 2552/2600 [09:44<00:11,  4.35it/s]\u001b[A\n","Epoch 0: 100% 23447/23493 [3:50:21<00:27,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  98% 2554/2600 [09:44<00:10,  4.41it/s]\u001b[A\n","Epoch 0: 100% 23449/23493 [3:50:21<00:25,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  98% 2556/2600 [09:45<00:09,  4.40it/s]\u001b[A\n","Epoch 0: 100% 23451/23493 [3:50:22<00:24,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  98% 2558/2600 [09:45<00:09,  4.39it/s]\u001b[A\n","Epoch 0: 100% 23453/23493 [3:50:22<00:23,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  98% 2560/2600 [09:46<00:09,  4.41it/s]\u001b[A\n","Epoch 0: 100% 23455/23493 [3:50:22<00:22,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  99% 2562/2600 [09:46<00:08,  4.46it/s]\u001b[A\n","Epoch 0: 100% 23457/23493 [3:50:23<00:21,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  99% 2564/2600 [09:47<00:08,  4.43it/s]\u001b[A\n","Epoch 0: 100% 23459/23493 [3:50:23<00:20,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  99% 2566/2600 [09:47<00:07,  4.41it/s]\u001b[A\n","Epoch 0: 100% 23461/23493 [3:50:24<00:18,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  99% 2568/2600 [09:48<00:07,  4.43it/s]\u001b[A\n","Epoch 0: 100% 23463/23493 [3:50:24<00:17,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  99% 2570/2600 [09:48<00:06,  4.39it/s]\u001b[A\n","Epoch 0: 100% 23465/23493 [3:50:25<00:16,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  99% 2572/2600 [09:49<00:06,  4.35it/s]\u001b[A\n","Epoch 0: 100% 23467/23493 [3:50:25<00:15,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  99% 2574/2600 [09:49<00:05,  4.36it/s]\u001b[A\n","Epoch 0: 100% 23469/23493 [3:50:26<00:14,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  99% 2576/2600 [09:49<00:05,  4.40it/s]\u001b[A\n","Epoch 0: 100% 23471/23493 [3:50:26<00:12,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  99% 2578/2600 [09:50<00:05,  4.38it/s]\u001b[A\n","Epoch 0: 100% 23473/23493 [3:50:27<00:11,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  99% 2580/2600 [09:50<00:04,  4.42it/s]\u001b[A\n","Epoch 0: 100% 23475/23493 [3:50:27<00:10,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  99% 2582/2600 [09:51<00:04,  4.39it/s]\u001b[A\n","Epoch 0: 100% 23477/23493 [3:50:27<00:09,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  99% 2584/2600 [09:51<00:03,  4.38it/s]\u001b[A\n","Epoch 0: 100% 23479/23493 [3:50:28<00:08,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating:  99% 2586/2600 [09:52<00:03,  4.41it/s]\u001b[A\n","Epoch 0: 100% 23481/23493 [3:50:28<00:07,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating: 100% 2588/2600 [09:52<00:02,  4.36it/s]\u001b[A\n","Epoch 0: 100% 23483/23493 [3:50:29<00:05,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating: 100% 2590/2600 [09:53<00:02,  4.43it/s]\u001b[A\n","Epoch 0: 100% 23485/23493 [3:50:29<00:04,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating: 100% 2592/2600 [09:53<00:01,  4.45it/s]\u001b[A\n","Epoch 0: 100% 23487/23493 [3:50:30<00:03,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating: 100% 2594/2600 [09:54<00:01,  4.40it/s]\u001b[A\n","Epoch 0: 100% 23489/23493 [3:50:30<00:02,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating: 100% 2596/2600 [09:54<00:00,  4.41it/s]\u001b[A\n","Epoch 0: 100% 23491/23493 [3:50:31<00:01,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Validating: 100% 2598/2600 [09:54<00:00,  4.45it/s]\u001b[A\n","Epoch 0: 100% 23493/23493 [3:50:31<00:00,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816]\n","Epoch 0: 100% 23493/23493 [3:50:31<00:00,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816, val_loss=1.220]\n","                                                   \u001b[AEpoch 0, global step 20892: val_loss reached 1.21695 (best 1.21695), saving model to \"/content/drive/MyDrive/kobart_summarization/KoBART-summarization/logs/model_chp/epoch=00-val_loss=1.217.ckpt\" as top 3\n","/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","tcmalloc: large alloc 1293287424 bytes == 0x119c50000 @  0x7faa2e9cd615 0x5d6f4c 0x51edd1 0x51ef5b 0x5aac95 0x5d8506 0x7faa1b09ff34 0x7faa1b0fd8c9 0x7faa0ba49465 0x7faa0ba459ca 0x7faa0ba4a609 0x7faa1b1013fb 0x7faa1ad8c1e6 0x5d80be 0x5d8d8c 0x4fedd4 0x4997c7 0x55d078 0x5d8941 0x4990ca 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91\n","tcmalloc: large alloc 1616609280 bytes == 0xb42bc000 @  0x7faa2e9cd615 0x5d6f4c 0x51edd1 0x51ef5b 0x5aac95 0x5d8506 0x7faa1b09ff34 0x7faa1b0fd8c9 0x7faa0ba49465 0x7faa0ba459ca 0x7faa0ba4a609 0x7faa1b1013fb 0x7faa1ad8c1e6 0x5d80be 0x5d8d8c 0x4fedd4 0x4997c7 0x55d078 0x5d8941 0x4990ca 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91\n","tcmalloc: large alloc 2020761600 bytes == 0x119c50000 @  0x7faa2e9cd615 0x5d6f4c 0x51edd1 0x51ef5b 0x5aac95 0x5d8506 0x7faa1b09ff34 0x7faa1b0fd8c9 0x7faa0ba49465 0x7faa0ba459ca 0x7faa0ba4a609 0x7faa1b1013fb 0x7faa1ad8c1e6 0x5d80be 0x5d8d8c 0x4fedd4 0x4997c7 0x55d078 0x5d8941 0x4990ca 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91\n","tcmalloc: large alloc 1616609280 bytes == 0x119c50000 @  0x7faa2e9cd615 0x5d6f4c 0x51edd1 0x51ef5b 0x5aac95 0x5d8506 0x7faa1b09ff34 0x7faa1b0fd8c9 0x7faa0ba49465 0x7faa0ba459ca 0x7faa0ba4a609 0x7faa1b1013fb 0x7faa1ad8c1e6 0x5d80be 0x5d8d8c 0x4fedd4 0x4997c7 0x55d078 0x5d8941 0x4990ca 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91\n","Epoch 0: 100% 23493/23493 [3:50:49<00:00,  1.70it/s, loss=0.924, v_num=16, train_loss=0.816, val_loss=1.220]tcmalloc: large alloc 2020761600 bytes == 0x7fa6a98da000 @  0x7faa2e9cd615 0x5d6f4c 0x51edd1 0x51ef5b 0x5aac95 0x5d8506 0x7faa1b09ff34 0x7faa1b0fd8c9 0x7faa0ba49465 0x7faa0ba459ca 0x7faa0ba4a609 0x7faa1b1013fb 0x7faa1ad8c1e6 0x5d80be 0x5d8d8c 0x4fedd4 0x4997c7 0x55d078 0x5d8941 0x4990ca 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91\n","Epoch 0: 100% 23493/23493 [3:51:11<00:00,  1.69it/s, loss=0.924, v_num=16, train_loss=0.816, val_loss=1.220]\n","Saving latest checkpoint...\n","tcmalloc: large alloc 2020761600 bytes == 0x119c50000 @  0x7faa2e9cd615 0x5d6f4c 0x51edd1 0x51ef5b 0x5aac95 0x5d8506 0x7faa1b09ff34 0x7faa1b0fd8c9 0x7faa0ba49465 0x7faa0ba459ca 0x7faa0ba4a609 0x7faa1b1013fb 0x7faa1ad8c1e6 0x5d80be 0x5d8d8c 0x4fedd4 0x4997c7 0x55d078 0x5d8941 0x4990ca 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91\n"]}]},{"cell_type":"markdown","source":["# **학습된 모델 불러오기**"],"metadata":{"id":"joCWFES3M6Ow"}},{"cell_type":"code","source":["# kobart_summary 디렉토리에 모델 저장\n","# hparams: logs 하위 디렉토리에서 사용할 모델의 버전 골라 hparams.yaml set\n","# model_binary: logs 하위 디렉토리에서 사용할 체크포인트 골라 *.ckpt set\n","! python get_model_binary.py --hparams ./logs/tb_logs/default/version_0/hparams.yaml --model_binary ./logs/model_chp/epoch=00-val_loss=1.177.ckpt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eWrzhDGHox9M","executionInfo":{"status":"ok","timestamp":1672733151688,"user_tz":-540,"elapsed":122121,"user":{"displayName":"최해민","userId":"04401546372408002204"}},"outputId":"8b133207-1cfb-4aec-c470-a8bd1ae67f17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["get_model_binary.py:13: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  hparams = yaml.load(f)\n","Downloading: 100% 4.00/4.00 [00:00<00:00, 3.48kB/s]\n","Downloading: 100% 111/111 [00:00<00:00, 97.9kB/s]\n","Downloading: 100% 682k/682k [00:01<00:00, 615kB/s]\n"]}]}]}